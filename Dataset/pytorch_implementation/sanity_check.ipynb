{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/sfs/qumulo/qhome/cjh9fw/.local/lib/python3.8/site-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZN2at4_ops10zeros_like4callERKNS_6TensorEN3c108optionalINS5_10ScalarTypeEEENS6_INS5_6LayoutEEENS6_INS5_6DeviceEEENS6_IbEENS6_INS5_12MemoryFormatEEE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mEgoExoEMSDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/sfs/weka/scratch/cjh9fw/Rivanna/2024/repos/EgoExoEMS/Dataset/pytorch_implementation/EgoExoEMSDataset.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchaudio/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     _extension,\n\u001b[1;32m      3\u001b[0m     compliance,\n\u001b[1;32m      4\u001b[0m     datasets,\n\u001b[1;32m      5\u001b[0m     functional,\n\u001b[1;32m      6\u001b[0m     io,\n\u001b[1;32m      7\u001b[0m     kaldi_io,\n\u001b[1;32m      8\u001b[0m     models,\n\u001b[1;32m      9\u001b[0m     pipelines,\n\u001b[1;32m     10\u001b[0m     sox_effects,\n\u001b[1;32m     11\u001b[0m     transforms,\n\u001b[1;32m     12\u001b[0m     utils,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_audio_backend, list_audio_backends, set_audio_backend\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchaudio/_extension/__init__.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m _IS_KALDI_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_TORCHAUDIO_EXT_AVAILABLE:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchaudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torchaudio\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     _check_cuda_version()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchaudio/_extension/utils.py:61\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m torch\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/_ops.py:1295\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1290\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1295\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /sfs/qumulo/qhome/cjh9fw/.local/lib/python3.8/site-packages/torchaudio/lib/libtorchaudio.so: undefined symbol: _ZN2at4_ops10zeros_like4callERKNS_6TensorEN3c108optionalINS5_10ScalarTypeEEENS6_INS5_6LayoutEEENS6_INS5_6DeviceEEENS6_IbEENS6_INS5_12MemoryFormatEEE"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from EgoExoEMSDataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    }
   ],
   "source": [
    "root = \"/standard/UVA-DSA/NIST EMS Project Data/CognitiveEMS_Datasets/North_Garden/May_2024/May24_updated_structure/ego_camera/ng1/1/\"  # Folder in which all videos lie in a specific structure\n",
    "annotation_file = \"../../Annotations/main_annotation.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "dataset = EgoExoEMSDataset(annotation_file=annotation_file,\n",
    "                                video_base_path='',\n",
    "                                fps=30, frames_per_clip=None, transform=transform)\n",
    "\n",
    "# Access a sample\n",
    "print(len(dataset))\n",
    "\n",
    "# create a data loader\n",
    "# batch size is 1 for simplicity and to ensure only a full clip related to a key step is given without collating.\n",
    "# if batch size is greater than 1, collate_fn will be called to collate the data.\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 129, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([3377]) tensor([3506])\n",
      "torch.Size([1, 65, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([3166]) tensor([3231])\n",
      "torch.Size([1, 86, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([2780]) tensor([2866])\n",
      "torch.Size([1, 32, 3, 224, 224]) ['no_action'] tensor([5]) tensor([1993]) tensor([2025])\n",
      "torch.Size([1, 36, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([3188]) tensor([3224])\n",
      "torch.Size([1, 237, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1174]) tensor([1411])\n",
      "torch.Size([1, 207, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([4512]) tensor([4719])\n",
      "torch.Size([1, 225, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1770]) tensor([1995])\n",
      "torch.Size([1, 209, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1910]) tensor([2119])\n",
      "torch.Size([1, 417, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([6202]) tensor([6619])\n",
      "torch.Size([1, 131, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([1046]) tensor([1177])\n",
      "torch.Size([1, 31, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([3833]) tensor([3864])\n",
      "torch.Size([1, 211, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([4567]) tensor([4778])\n",
      "torch.Size([1, 130, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([2116]) tensor([2246])\n",
      "torch.Size([1, 457, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([179]) tensor([636])\n",
      "torch.Size([1, 272, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1510]) tensor([1782])\n",
      "torch.Size([1, 259, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([2522]) tensor([2781])\n",
      "torch.Size([1, 46, 3, 224, 224]) ['no_action'] tensor([5]) tensor([1602]) tensor([1648])\n",
      "torch.Size([1, 236, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1053]) tensor([1289])\n",
      "torch.Size([1, 30, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([750]) tensor([780])\n",
      "torch.Size([1, 244, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([2529]) tensor([2773])\n",
      "torch.Size([1, 200, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1758]) tensor([1958])\n",
      "torch.Size([1, 194, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([4113]) tensor([4307])\n",
      "torch.Size([1, 95, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([4019]) tensor([4114])\n",
      "torch.Size([1, 439, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([2602]) tensor([3041])\n",
      "torch.Size([1, 44, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([3788]) tensor([3832])\n",
      "torch.Size([1, 51, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([1030]) tensor([1081])\n",
      "torch.Size([1, 228, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([2698]) tensor([2926])\n",
      "torch.Size([1, 201, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([2383]) tensor([2584])\n",
      "torch.Size([1, 813, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([5260]) tensor([6073])\n",
      "torch.Size([1, 130, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([3716]) tensor([3846])\n",
      "torch.Size([1, 125, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([2435]) tensor([2560])\n",
      "torch.Size([1, 130, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([3037]) tensor([3167])\n",
      "torch.Size([1, 101, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([1410]) tensor([1511])\n",
      "torch.Size([1, 84, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([2300]) tensor([2384])\n",
      "torch.Size([1, 250, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1030]) tensor([1280])\n",
      "torch.Size([1, 116, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([2583]) tensor([2699])\n",
      "torch.Size([1, 66, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1927]) tensor([1993])\n",
      "torch.Size([1, 107, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([3095]) tensor([3202])\n",
      "torch.Size([1, 54, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([2468]) tensor([2522])\n",
      "torch.Size([1, 57, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([2102]) tensor([2159])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@896.335] global cap_ffmpeg_impl.hpp:1541 grabFrame packet read max attempts exceeded, if your video have multiple streams (video, audio) try to increase attempt limit by setting environment variable OPENCV_FFMPEG_READ_ATTEMPTS (current value is 4096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 3, 224, 224]) ['check_responsiveness'] tensor([2]) tensor([0]) tensor([111])\n",
      "torch.Size([1, 72, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([3630]) tensor([3702])\n",
      "torch.Size([1, 424, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([3937]) tensor([4361])\n",
      "torch.Size([1, 476, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1900]) tensor([2376])\n",
      "torch.Size([1, 1378, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([225]) tensor([1603])\n",
      "torch.Size([1, 103, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([2925]) tensor([3028])\n",
      "torch.Size([1, 208, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([5005]) tensor([5213])\n",
      "torch.Size([1, 91, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([1680]) tensor([1771])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@995.312] global cap_ffmpeg_impl.hpp:453 _opencv_ffmpeg_interrupt_callback Stream timeout triggered after 30026.765449 ms\n",
      "[ WARN:0@995.312] global cap_ffmpeg_impl.hpp:453 _opencv_ffmpeg_interrupt_callback Stream timeout triggered after 30026.828598 ms\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x7e8ae80] stream 4, offset 0x315458cd: partial file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 29, 3, 224, 224]) ['check_responsiveness'] tensor([2]) tensor([1]) tensor([69])\n",
      "torch.Size([1, 44, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([2559]) tensor([2603])\n",
      "torch.Size([1, 250, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([796]) tensor([1046])\n",
      "torch.Size([1, 89, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([1279]) tensor([1368])\n",
      "torch.Size([1, 457, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([574]) tensor([1031])\n",
      "torch.Size([1, 477, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([1992]) tensor([2469])\n",
      "torch.Size([1, 91, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([1837]) tensor([1928])\n",
      "torch.Size([1, 542, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([2647]) tensor([3189])\n",
      "torch.Size([1, 93, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([637]) tensor([730])\n",
      "torch.Size([1, 201, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1367]) tensor([1568])\n",
      "torch.Size([1, 459, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([967]) tensor([1426])\n",
      "torch.Size([1, 110, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([3415]) tensor([3525])\n",
      "torch.Size([1, 127, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([1784]) tensor([1911])\n",
      "torch.Size([1, 257, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1424]) tensor([1681])\n",
      "torch.Size([1, 281, 3, 224, 224]) ['attach_defib_pads'] tensor([6]) tensor([294]) tensor([575])\n",
      "torch.Size([1, 144, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([4062]) tensor([4206])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1109.356] global cap_ffmpeg_impl.hpp:1541 grabFrame packet read max attempts exceeded, if your video have multiple streams (video, audio) try to increase attempt limit by setting environment variable OPENCV_FFMPEG_READ_ATTEMPTS (current value is 4096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 3, 224, 224]) ['approach_patient'] tensor([1]) tensor([0]) tensor([87])\n",
      "torch.Size([1, 87, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([551]) tensor([638])\n",
      "torch.Size([1, 410, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([3363]) tensor([3773])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1145.780] global cap_ffmpeg_impl.hpp:1541 grabFrame packet read max attempts exceeded, if your video have multiple streams (video, audio) try to increase attempt limit by setting environment variable OPENCV_FFMPEG_READ_ATTEMPTS (current value is 4096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([0]) tensor([64])\n",
      "torch.Size([1, 239, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1433]) tensor([1672])\n",
      "torch.Size([1, 43, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([5312]) tensor([5355])\n",
      "torch.Size([1, 88, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([1671]) tensor([1759])\n",
      "torch.Size([1, 87, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([1567]) tensor([1654])\n",
      "torch.Size([1, 133, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([6070]) tensor([6203])\n",
      "torch.Size([1, 492, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([4821]) tensor([5313])\n",
      "torch.Size([1, 213, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([3504]) tensor([3717])\n",
      "torch.Size([1, 112, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([68]) tensor([180])\n",
      "torch.Size([1, 205, 3, 224, 224]) ['attach_defib_pads'] tensor([6]) tensor([198]) tensor([403])\n",
      "torch.Size([1, 138, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([3208]) tensor([3346])\n",
      "torch.Size([1, 351, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([3027]) tensor([3378])\n",
      "torch.Size([1, 113, 3, 224, 224]) ['attach_defib_pads'] tensor([6]) tensor([331]) tensor([444])\n",
      "torch.Size([1, 79, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([3426]) tensor([3505])\n",
      "torch.Size([1, 231, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([2865]) tensor([3096])\n",
      "torch.Size([1, 94, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([1340]) tensor([1434])\n",
      "torch.Size([1, 276, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([4360]) tensor([4636])\n",
      "torch.Size([1, 84, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([109]) tensor([193])\n",
      "torch.Size([1, 191, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([2245]) tensor([2436])\n",
      "torch.Size([1, 101, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([4305]) tensor([4406])\n",
      "torch.Size([1, 117, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([3230]) tensor([3347])\n",
      "torch.Size([1, 88, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([1202]) tensor([1290])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1276.376] global cap_ffmpeg_impl.hpp:1541 grabFrame packet read max attempts exceeded, if your video have multiple streams (video, audio) try to increase attempt limit by setting environment variable OPENCV_FFMPEG_READ_ATTEMPTS (current value is 4096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 168, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([31]) tensor([199])\n",
      "torch.Size([1, 120, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([3692]) tensor([3812])\n",
      "torch.Size([1, 264, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([2382]) tensor([2646])\n",
      "torch.Size([1, 163, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([4405]) tensor([4568])\n",
      "torch.Size([1, 30, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1620]) tensor([1650])\n",
      "torch.Size([1, 252, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([2049]) tensor([2301])\n",
      "torch.Size([1, 200, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([4208]) tensor([4408])\n",
      "torch.Size([1, 127, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([2053]) tensor([2180])\n",
      "torch.Size([1, 145, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([3223]) tensor([3368])\n",
      "torch.Size([1, 104, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([4718]) tensor([4822])\n",
      "torch.Size([1, 89, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([1425]) tensor([1514])\n",
      "torch.Size([1, 239, 3, 224, 224]) ['attach_defib_pads'] tensor([6]) tensor([729]) tensor([968])\n",
      "torch.Size([1, 185, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([1653]) tensor([1838])\n",
      "torch.Size([1, 605, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([192]) tensor([797])\n",
      "torch.Size([1, 104, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([4409]) tensor([4513])\n",
      "torch.Size([1, 228, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([3845]) tensor([4073])\n",
      "torch.Size([1, 352, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([2179]) tensor([2531])\n",
      "torch.Size([1, 28, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([2525]) tensor([2553])\n",
      "torch.Size([1, 232, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([63]) tensor([295])\n",
      "torch.Size([1, 29, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([1174]) tensor([1203])\n",
      "torch.Size([1, 117, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([3504]) tensor([3621])\n",
      "torch.Size([1, 168, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([3524]) tensor([3692])\n",
      "torch.Size([1, 70, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([2024]) tensor([2094])\n",
      "torch.Size([1, 656, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([402]) tensor([1058])\n",
      "torch.Size([1, 49, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([5212]) tensor([5261])\n",
      "torch.Size([1, 109, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([3833]) tensor([3942])\n",
      "torch.Size([1, 257, 3, 224, 224]) ['attach_defib_pads'] tensor([6]) tensor([1646]) tensor([1903])\n",
      "torch.Size([1, 440, 3, 224, 224]) ['administer_shock_aed'] tensor([7]) tensor([3622]) tensor([4062])\n",
      "torch.Size([1, 226, 3, 224, 224]) ['chest_compressions'] tensor([4]) tensor([3201]) tensor([3427])\n",
      "torch.Size([1, 71, 3, 224, 224]) ['check_responsiveness'] tensor([2]) tensor([83]) tensor([154])\n",
      "torch.Size([1, 138, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([4071]) tensor([4209])\n",
      "torch.Size([1, 73, 3, 224, 224]) ['check_pulse_breathing'] tensor([3]) tensor([153]) tensor([226])\n",
      "torch.Size([1, 53, 3, 224, 224]) ['place_bvm'] tensor([8]) tensor([1288]) tensor([1341])\n",
      "torch.Size([1, 97, 3, 224, 224]) ['compress_bvm'] tensor([9]) tensor([1957]) tensor([2054])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the data loader and print the shape of the batch\n",
    "for batch in data_loader:\n",
    "    print(batch['frames'].shape, batch['audio'].shape, batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'], batch['subject_id'], batch['trial_id'])\n",
    "    # break   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
