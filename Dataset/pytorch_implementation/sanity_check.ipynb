{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "import time\n",
    "# from EgoExoEMS.EgoExoEMS import EgoExoEMSDataset, collate_fn, transform,CLIP_EgoExo_Dataset\n",
    "from EgoExoEMS.EgoExoEMS.EgoExoEMS import  EgoExoEMSDataset, collate_fn, transform,CLIP_EgoExo_Keystep_Dataset, clip_collate_fn, CLIP_EgoExo_Keystep_LIMITED_Dataset\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Accurate seek is not implemented for pyav backend\")\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class stats:  {'chest_compressions': 351, 'approach_patient': 80, 'inquire_hpi_and_pmh': 82, 'check_blood_pressure': 38, 'check_heart_rate': 29, 'check_oxygen_saturation': 45, 'explain_procedure': 25, 'place_v1_lead': 23, 'place_v2_lead': 24, 'place_v4_lead': 22, 'place_v3_lead': 22, 'place_v6_lead': 22, 'place_left_arm_lead': 25, 'place_right_leg_lead': 25, 'place_left_leg_lead': 26, 'place_right_arm_lead': 27, 'turn_on_ecg': 6, 'request_patient_to_not_move': 20, 'shave_patient': 2, 'obtain_ecg_recording': 27, 'interpret_and_report': 24, 'review_medications': 22, 'check_responsiveness': 90, 'inquire_substance_use': 8, 'check_symptom_duration': 4, 'document_lkw_time': 13, 'load_patient_to_stretcher': 38, 'secure_patient_on_stretcher': 57, 'notify_hospital_of_stroke_alert': 14, 'transport': 69, 'check_vision_deficits': 11, 'assess_neglect_signs': 2, 'inquire_medication_anticoagulants': 9, 'handoff_patient_to_hospital': 16, 'check_a&o': 8, 'face_droop_check': 19, 'ask_patient_age_sex': 27, 'check_perrl': 7, 'check_grip_strength': 21, 'evaluate_aphasia': 3, 'prepare_glucometer_and_strip': 22, 'read_and_record_glucose_level': 18, 'suction_airway': 3, 'check_pulse': 125, 'speech_abnormality_check': 9, 'arm_drift_check': 12, 'request_assistance': 27, 'check_respiratory_rate': 9, 'check_breathing': 68, 'document_hpi_and_pmh': 7, 'place_v5_lead': 22, 'assess_patient': 21, 'inset_NPA': 2, 'check_skin_condition': 2, 'turn_on_aed': 65, 'attach_defib_pads': 79, 'request_aed': 28, 'no_action': 64, 'clear_for_analysis': 72, 'clear_for_shock': 70, 'administer_shock_aed': 72, 'place_bvm': 196, 'open_airway': 13, 'ventilate_patient': 244, 'connect_leads_to_ecg': 9, 'assess_balance_and_coordination': 3}\n",
      "Train Number of classes:  66\n",
      "Number of batches in train loader: 6637\n",
      "Loading item 8, clip 9 with start_t 0 : start_frame 0 and end_t 58.400000000000006 : end_frame 1750, subject_id: P1, trial_id: s2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjh9fw/conda/envs/egoems/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "resnet_ego shape: torch.Size([1, 0])\n",
      "Label ['chest_compressions']\n",
      "Loading item 697, clip 0 with start_t 69.264 : start_frame 2075 and end_t 74.80401 : end_frame 2241, subject_id: ng1, trial_id: 1\n",
      "Batch 2:\n",
      "resnet_ego shape: torch.Size([1, 0])\n",
      "Label ['administer_shock_aed']\n",
      "Loading item 242, clip 0 with start_t 750.93 : start_frame 22505 and end_t 760.60157 : end_frame 22795, subject_id: cars_15, trial_id: 0\n",
      "Batch 3:\n",
      "resnet_ego shape: torch.Size([1, 0])\n",
      "Label ['place_v5_lead']\n",
      "Loading item 2265, clip 9 with start_t 219.104 : start_frame 6566 and end_t 332.20944 : end_frame 9956, subject_id: opvrs_4, trial_id: 2\n",
      "Batch 4:\n",
      "resnet_ego shape: torch.Size([1, 0])\n",
      "Label ['arm_drift_check']\n",
      "Loading item 1965, clip 1 with start_t 253.79 : start_frame 7606 and end_t 289.5271 : end_frame 8677, subject_id: opvrs_11, trial_id: 1\n",
      "Batch 5:\n",
      "resnet_ego shape: torch.Size([1, 0])\n",
      "Label ['prepare_glucometer_and_strip']\n",
      "Loading item 1778, clip 1 with start_t 116.012 : start_frame 3476 and end_t 133.9257 : end_frame 4013, subject_id: ng8, trial_id: 3\n",
      "Batch 6:\n",
      "resnet_ego shape: torch.Size([1, 0])\n",
      "Label ['chest_compressions']\n",
      "Loading item 726, clip 3 with start_t 121.424 : start_frame 3639 and end_t 145.88855 : end_frame 4372, subject_id: ng2, trial_id: 0\n",
      "Batch 7:\n",
      "resnet_ego shape: torch.Size([1, 0])\n",
      "Label ['chest_compressions']\n",
      "Loading item 228, clip 23 with start_t 2127.914 : start_frame 63773 and end_t 2257.0271 : end_frame 67643, subject_id: cars_14, trial_id: 0\n",
      "Batch 8:\n",
      "resnet_ego shape: torch.Size([1, 0])\n",
      "Label ['handoff_patient_to_hospital']\n",
      "Loading item 50, clip 0 with start_t 0 : start_frame 0 and end_t 125.9 : end_frame 3773, subject_id: P20, trial_id: s8\n"
     ]
    }
   ],
   "source": [
    "root = \"/standard/UVA-DSA/NIST EMS Project Data/EgoExoEMS_CVPR2025/Dataset/Final/ng9/cardiac_arrest/1/GoPro/GX010346_encoded_trimmed.mp4\"  # Folder in which all videos lie in a specific structure\n",
    "# annotation_file = \"../../Annotations/main_annotation_classification.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "annotation_file = \"../../Annotations/aaai26_main_annotation_classification.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "# annotation_file = \"../../Annotations/splits/trials/test_split_classification.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "# train_annotation_file = \"../../Annotations/splits/keysteps/train_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "# val_annotation_file = \"../../Annotations/splits/keysteps/val_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "# test_annotation_file = \"../../Annotations/splits/keysteps/test_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "\n",
    "# train_dataset = EgoExoEMSDataset(annotation_file=annotation_file,\n",
    "#                                 data_base_path='',\n",
    "#                                 fps=29.97, frames_per_clip=None, transform=transform, data_types=[ 'video', 'audio'], task=\"classification\")\n",
    "\n",
    "# # Access a sample\n",
    "# print(len(train_dataset))\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True)\n",
    "\n",
    "\n",
    "fps = 29.97\n",
    "observation_window = 150\n",
    "data_types = [ 'video']\n",
    "task = \"classification\"  # or \"segmentation\" or \"cpr_quality\"\n",
    "\n",
    "train_dataset = EgoExoEMSDataset(annotation_file=annotation_file,\n",
    "                                data_base_path='',\n",
    "                                fps=fps, frames_per_clip=observation_window, transform=transform, data_types=data_types, task=task)\n",
    "\n",
    "# val_dataset = EgoExoEMSDataset(annotation_file=args.dataloader_params[\"val_annotation_path\"],\n",
    "#                                 data_base_path='',\n",
    "#                                 fps=fps, frames_per_clip=observation_window, transform=transform, data_types=data_types, task=task)\n",
    "\n",
    "# test_dataset = EgoExoEMSDataset(annotation_file=args.dataloader_params[\"test_annotation_path\"],\n",
    "#                                 data_base_path='',\n",
    "#                                 fps=fps, frames_per_clip=observation_window, transform=transform, data_types=data_types, task=task)\n",
    "\n",
    "\n",
    "train_class_stats = train_dataset._get_class_stats()\n",
    "print(\"Train class stats: \", train_class_stats)\n",
    "# print number of keys in the dictionary\n",
    "print(\"Train Number of classes: \", len(train_class_stats.keys()))\n",
    "\n",
    "# Create DataLoaders for training and validation subsets\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "print(\"Number of batches in train loader:\", len(train_loader))\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(\"resnet_ego shape:\", batch['resnet_ego'].shape)\n",
    "\n",
    "    # save the video frames to a file for debugging\n",
    "    # Get the first video in batch\n",
    "    # video_frames = batch['frames'][0]  # shape: (num_frames, C, H, W)\n",
    "    # video_frames = video_frames.permute(0, 2, 3, 1)  # (num_frames, H, W, C)\n",
    "\n",
    "    # # Convert to uint8\n",
    "    # video_frames = (video_frames * 255).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    # # Optional: ensure only 3 channels if necessary\n",
    "    # if video_frames.shape[-1] > 3:\n",
    "    #     video_frames = video_frames[..., :3]\n",
    "\n",
    "    # # Define output path\n",
    "    # video_path = \"debug_video.mp4\"\n",
    "\n",
    "    # # Setup video writer\n",
    "    # height, width, channels = video_frames[0].shape\n",
    "    # fps = 30  # or your actual frame rate\n",
    "\n",
    "    # out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    # for frame in video_frames:\n",
    "    #     out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # out.release()\n",
    "    # print(f\"Saved debug video to {video_path}\")\n",
    "\n",
    "\n",
    "    # print(\"Audio shape:\", batch['audio'].shape)\n",
    "    print(\"Label\", batch['keystep_label'])\n",
    "    # print(\"Start Frames:\", batch['start_frames'])\n",
    "    # print(\"End Frames:\", batch['end_frames'])\n",
    "    # print(\"Trial IDs:\", batch['trial_ids'])\n",
    "    # print(\"Subject IDs:\", batch['subject_ids'])\n",
    "    # break  # Remove this line to iterate through all batches\n",
    "\n",
    "# for i, batch in enumerate(train_loader):\n",
    "#     print(f\"Batch {i+1}:\")\n",
    "#     print(\"Video shape:\", batch['video'].shape)\n",
    "#     print(\"Audio shape:\", batch['audio'].shape)\n",
    "#     print(\"Labels:\", batch['labels'])\n",
    "#     print(\"File IDs:\", batch['file_ids'])\n",
    "#     print(\"File Paths:\", batch['file_paths'])\n",
    "#     # print(\"Start Frames:\", batch['start_frames'])\n",
    "#     # print(\"End Frames:\", batch['end_frames'])\n",
    "#     # print(\"Trial IDs:\", batch['trial_ids'])\n",
    "#     # print(\"Subject IDs:\", batch['subject_ids'])\n",
    "#     break  # Remove this line to iterate through all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CLIP_EgoExo_Keystep_Dataset(annotation_file=annotation_file, fps= 29.97, data_base_path=\"\")\n",
    "# dataset = CLIP_EgoExo_Keystep_LIMITED_Dataset(annotation_file=annotation_file, fps= 29.97, data_base_path=\"\",max_neg_samples=20,max_pos_samples=20)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=clip_collate_fn, num_workers=8, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataloader))\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "    print(\"--------------------\")\n",
    "    print(data['keystep_id'])\n",
    "    print(data['pairing_info'])\n",
    "    print(\"--------------------\")\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = '/standard/UVA-DSA/NIST EMS Project Data/EgoExoEMS_CVPR2025/Dataset/Final/ng3/cardiac_arrest/9/i3d_flow/GX010374_encoded_trimmed_flow.npy'\n",
    "gopro_path = '/standard/UVA-DSA/NIST EMS Project Data/EgoExoEMS_CVPR2025/Dataset/Final/ng3/cardiac_arrest/9/GoPro/GX010374_encoded_trimmed.mp4'\n",
    "\n",
    "# load the flow\n",
    "flow = np.load(flow_path)\n",
    "print(flow.shape)\n",
    "\n",
    "import math\n",
    "\n",
    "print((2724-2626))\n",
    "print(math.ceil((2724-2626)/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# from torchvision.io import VideoReader\n",
    "# import os\n",
    "# import itertools\n",
    "\n",
    "# start_t = 71.30542\n",
    "# end_t = 72.993\n",
    "# video_path = root\n",
    "# # video_path = \"/standard/UVA-DSA/NIST EMS Project Data/CognitiveEMS_Datasets/North_Garden/Sep_2024/Raw/24-09-2024/Bhavik/cardiac_arrest/4/GoPro/GX010399.MP4\"  # Folder in which all videos lie in a specific structure\n",
    "# video_reader = VideoReader(video_path, \"video\")\n",
    "# frames = []\n",
    "\n",
    "\n",
    "# for frame in itertools.takewhile(lambda x: x['pts'] <= end_t, video_reader.seek(start_t)):\n",
    "#     if(frame['pts'] < start_t):\n",
    "#         continue\n",
    "#     print(frame['pts'])\n",
    "#     img_tensor = transform(frame['data'])\n",
    "#     frames.append(img_tensor)\n",
    "\n",
    "# frames = torch.stack(frames)\n",
    "# print(\"Seeking from \", start_t, \" to \", end_t, \"for video \", video_path)\n",
    "# print(\"Frames shape: \", frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data loader\n",
    "# batch size is 1 for simplicity and to ensure only a full clip related to a key step is given without collating.\n",
    "# if batch size is greater than 1, collate_fn will be called to collate the data.\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "print(len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the data loader and print the shape of the batch\n",
    "for batch in data_loader:\n",
    "    # print(batch['frames'].shape, batch['audio'].shape,batch['resnet'].shape,batch['resnet_exo'].shape, batch['flow'].shape, batch['rgb'].shape, batch['smartwatch'].shape, batch['depth_sensor'].shape , batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "    print(batch['resnet'].shape, batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "    print(\"*\"*4 + \"=\"*50 + \"*\"*4)\n",
    "    # break\n",
    "    \n",
    "    # audio_tensor = batch['audio'][0]\n",
    "    # #transpose\n",
    "    # audio_tensor = audio_tensor.transpose(0,1)\n",
    "    # torchaudio.save(\"./visualizations/audio.wav\", audio_tensor,48000)\n",
    "    # break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchaudio feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "\n",
    "# Example batch of audio clips (batch, samples, channels)\n",
    "audio_clips = batch['audio']  # Assume shape [batch, samples, channels]\n",
    "\n",
    "# Parameters for feature extraction\n",
    "sample_rate = 48000  # Adjust based on your dataset\n",
    "n_mels = 64  # Number of Mel filter banks\n",
    "hop_length = 512\n",
    "n_fft = 1024\n",
    "\n",
    "# MelSpectrogram transform\n",
    "mel_spectrogram = transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mels=n_mels,\n",
    "    hop_length=hop_length,\n",
    "    n_fft=n_fft\n",
    ")\n",
    "\n",
    "# Process audio for one channel (if you want stereo/multi-channel, handle each channel accordingly)\n",
    "batch_size, num_samples, num_channels = audio_clips.shape\n",
    "\n",
    "# If using mono audio (process first channel as an example)\n",
    "mel_features = mel_spectrogram(audio_clips[:, :, 0])  # Shape: [batch, n_mels, time]\n",
    "\n",
    "# To get the desired shape [batch, samples, feature_dim], we treat the time axis as the \"samples\"\n",
    "# Here, n_mels will be the feature dimension and time will be the new samples axis\n",
    "mel_features = mel_features.permute(0, 2, 1)  # Shape: [batch, time(samples), n_mels(feature_dim)]\n",
    "\n",
    "print(f\"Mel-spectrogram features shape: {mel_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual channel mel-spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example batch of audio clips (batch, samples, channels)\n",
    "audio_clips = batch['audio']  # Assume shape [batch, samples, channels]\n",
    "\n",
    "# Parameters for feature extraction\n",
    "sample_rate = 48000  # Adjust based on your dataset\n",
    "n_mels = 64  # Number of Mel filter banks\n",
    "hop_length = 512\n",
    "n_fft = 1024\n",
    "\n",
    "# MelSpectrogram transform\n",
    "mel_spectrogram = transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mels=n_mels,\n",
    "    hop_length=hop_length,\n",
    "    n_fft=n_fft\n",
    ")\n",
    "\n",
    "# Extract Mel-spectrogram for both channels\n",
    "mel_spec_left = mel_spectrogram(audio_clips[:, :, 0])  # Left channel\n",
    "mel_spec_right = mel_spectrogram(audio_clips[:, :, 1])  # Right channel\n",
    "\n",
    "mel_spec_combined = torch.cat((mel_spec_left, mel_spec_right), dim=-1)  # Shape: [n_mels, time*2]\n",
    "print(f\"Mel-spectrogram combined shape: {mel_spec_combined.shape}\")\n",
    "\n",
    "\n",
    "# Convert to log scale\n",
    "mel_spec_left_db = transforms.AmplitudeToDB()(mel_spec_left)\n",
    "mel_spec_right_db = transforms.AmplitudeToDB()(mel_spec_right)\n",
    "\n",
    "\n",
    "# Concatenate the spectrograms side by side (along the time axis)\n",
    "mel_spec_combined_db = torch.cat((mel_spec_left_db, mel_spec_right_db), dim=-1)  # Shape: [n_mels, time*2]\n",
    "print(f\"Mel-spectrogram combined shape: {mel_spec_combined_db.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example batch of audio clips (batch, samples, channels)\n",
    "audio_clips = batch['audio']  # Assume shape [batch, samples, channels]\n",
    "\n",
    "# Parameters for feature extraction\n",
    "sample_rate = 16000  # Adjust based on your dataset\n",
    "n_mels = 64  # Number of Mel filter banks\n",
    "hop_length = 512\n",
    "n_fft = 1024\n",
    "\n",
    "# MelSpectrogram transform\n",
    "mel_spectrogram = transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mels=n_mels,\n",
    "    hop_length=hop_length,\n",
    "    n_fft=n_fft\n",
    ")\n",
    "\n",
    "# Extract Mel-spectrogram for a single channel (first sample, first channel as an example)\n",
    "mel_spec = mel_spectrogram(audio_clips[0, :, 0])  # Shape: [n_mels, time]\n",
    "\n",
    "# Convert to log scale for better visualization\n",
    "mel_spec_db = transforms.AmplitudeToDB()(mel_spec)\n",
    "\n",
    "# Plotting the Mel-spectrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(mel_spec_db.numpy(), cmap='viridis', aspect='auto', origin='lower')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel-Spectrogram (Log-Scale)\")\n",
    "plt.xlabel(\"Time (frames)\")\n",
    "plt.ylabel(\"Mel Frequency Bins\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert frames tensor to video\n",
    "frames = batch['frames'][0]\n",
    "print(frames.shape) # (num_frames, 3, 224, 224)\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Example tensor (num_frames, 3, 224, 224)\n",
    "frames = batch['frames'][0]\n",
    "\n",
    "# Convert from (num_frames, 3, 224, 224) to (num_frames, 224, 224, 3)\n",
    "frames = frames.permute(0, 2, 3, 1).cpu().numpy()  # (num_frames, 224, 224, 3)\n",
    "\n",
    "# Normalize pixel values from [0, 1] or [-1, 1] if necessary\n",
    "# frames = (frames * 255).astype(np.uint8)\n",
    "\n",
    "# convert bgr to rgb\n",
    "# Define the codec and create a VideoWriter object\n",
    "output_file = './visualizations/video.mp4'\n",
    "fps = 30  # Frames per second\n",
    "height, width = frames.shape[1:3]\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "\n",
    "video_writer = cv2.VideoWriter(output_file, fourcc, fps, (width, height))\n",
    "\n",
    "# Write each frame to the video\n",
    "for frame in frames:\n",
    "    video_writer.write(frame)\n",
    "\n",
    "# Release the video writer\n",
    "video_writer.release()\n",
    "\n",
    "print(f\"Video saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_rgb_flow_imu_depth(rgb_frames, flow_frames, rgb_feature, imu_data, depth_data):\n",
    "    # Assume rgb_frames is of shape (frames, height, width, 3) and flow_frames is (frames, 1024)\n",
    "    # imu_data is of shape (frames, 3) and depth_data is (frames, 1)\n",
    "    print(rgb_frames.shape, flow_frames.shape, rgb_feature.shape, imu_data.shape, depth_data.shape)\n",
    "    num_frames = rgb_frames.shape[0]\n",
    "\n",
    "    fig, axes = plt.subplots( 5, num_frames, figsize=(num_frames * 5, 25))\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        rgb_image = rgb_frames[i].permute(1, 2, 0).cpu().numpy()\n",
    "        \n",
    "        # Plot RGB frame\n",
    "        axes[0, i].imshow(rgb_image)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(f\"RGB Frame {i+1}\")\n",
    "        \n",
    "        # Plot flow data as a heatmap for the corresponding frame\n",
    "        axes[1, i].imshow(flow_frames[i].reshape(32, 32), cmap='viridis', aspect='auto')  # assuming 1024 is reshaped to 32x32\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(f\"Flow Feature {i+1}\")\n",
    "        \n",
    "        # Plot the RGB feature as a heatmap\n",
    "        rgb_feature_image = rgb_feature[i].cpu().numpy().reshape(32, 32)  # Reshape to 32x32\n",
    "        axes[2, i].imshow(rgb_feature_image, cmap='plasma', aspect='auto')\n",
    "        axes[2, i].axis('off')\n",
    "        axes[2, i].set_title(f\"RGB Feature {i+1}\")\n",
    "        \n",
    "        # Plot smartwatch IMU data (3-axis)\n",
    "        # Plot smartwatch IMU data (3-axis: x, y, z)\n",
    "        imu_time_series = imu_data[i].cpu().numpy()  # Shape (3,)\n",
    "        axes[3, i].plot([0, 1, 2], imu_time_series, marker='o', label=['X', 'Y', 'Z'])\n",
    "        axes[3, i].set_xticks([0, 1, 2])\n",
    "        axes[3, i].set_xticklabels(['X', 'Y', 'Z'])\n",
    "        axes[3, i].set_title(f\"IMU Data {i+1}\")\n",
    "        \n",
    "        # Plot depth sensor data\n",
    "        depth_value = depth_data[i].cpu().numpy()\n",
    "        axes[4, i].bar(0, depth_value, width=0.5)\n",
    "        axes[4, i].set_ylim(0, np.max(depth_data.cpu().numpy()))  # Adjust y-axis based on max depth value\n",
    "        axes[4, i].set_title(f\"Depth Sensor {i+1}\")\n",
    "        axes[4, i].set_xticks([])  # No x-ticks since it's a single bar\n",
    "\n",
    "        # Save each figure\n",
    "        plt.savefig(f\"./visualizations/frame_{i}.png\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with your batch data\n",
    "# batch = next(iter(data_loader))\n",
    "print(batch['frames'].shape, batch['audio'].shape, batch['flow'].shape, batch['rgb'].shape, batch['smartwatch_imu'].shape, batch['depth_sensor'].shape , batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "# torch.Size([1, 82, 3, 224, 224]) torch.Size([1, 133120, 2]) torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024]) torch.Size([1, 82, 3]) torch.Size([1, 82, 1]) ['place_bvm'] tensor([13]) tensor([2416]) tensor([2498]) tensor([80.6273]) tensor([83.3773]) ['ng8'] ['7']\n",
    "\n",
    "rgb_frames= batch['frames'][0]\n",
    "flow_frames = batch['flow'][0]\n",
    "rgb_feature = batch['rgb'][0]\n",
    "imu_data = batch['smartwatch_imu'][0]\n",
    "depth_data = batch['depth_sensor'][0]\n",
    "\n",
    "plot_index = min(rgb_frames.shape[0], flow_frames.shape[0], rgb_feature.shape[0], imu_data.shape[0], depth_data.shape[0])\n",
    "\n",
    "if(plot_index > 1):\n",
    "    plot_index = 15\n",
    "    plot_rgb_flow_imu_depth(rgb_frames[0:plot_index], flow_frames[0:plot_index], rgb_feature[0:plot_index],imu_data[0:plot_index], depth_data[0:plot_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def animate_rgb_flow_imu_depth(rgb_frames, flow_frames, additional_rgb, imu_data, depth_data):\n",
    "    num_frames = rgb_frames.shape[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 1, figsize=(6, 15))  # 5 rows for RGB, Flow, Additional RGB, IMU, and Depth\n",
    "\n",
    "    # Initialize the images that will be updated\n",
    "    rgb_im = axes[0].imshow(np.zeros((rgb_frames.shape[2], rgb_frames.shape[3], 3)))\n",
    "    flow_im = axes[1].imshow(np.zeros((32, 32)), cmap='viridis')\n",
    "    add_rgb_im = axes[2].imshow(np.zeros((32, 32)), cmap='plasma')\n",
    "    \n",
    "    # For IMU and Depth, we initialize placeholders\n",
    "    imu_plot_x, = axes[3].plot([], [], 'r-', label='X')\n",
    "    imu_plot_y, = axes[3].plot([], [], 'g-', label='Y')\n",
    "    imu_plot_z, = axes[3].plot([], [], 'b-', label='Z')\n",
    "    depth_plot = axes[4].bar([0], [0], width=0.5)  # Single bar for depth value\n",
    "\n",
    "    axes[0].set_title('RGB Frame')\n",
    "    axes[1].set_title('Flow Feature')\n",
    "    axes[2].set_title('Additional RGB Feature')\n",
    "    axes[3].set_title('IMU Data (X, Y, Z)')\n",
    "    axes[4].set_title('Depth Sensor Data')\n",
    "    \n",
    "        # Set a fixed Y-axis range for IMU data\n",
    "    imu_y_range = (-10, 10)  # Adjust this range based on your IMU data values\n",
    "    axes[3].set_ylim(imu_y_range)  # Set a fixed range for the IMU Y-axis\n",
    "\n",
    "\n",
    "    # Turn off axis for cleaner visuals\n",
    "    for ax in axes:\n",
    "        ax.axis('off') if ax != axes[3] else ax.set_xticks([])  # Don't turn off axis for IMU\n",
    "\n",
    "    # IMU axis should show the three axes, so enable labels for this plot\n",
    "    axes[3].legend(loc='upper right')\n",
    "\n",
    "    def update(frame):\n",
    "        # Update RGB data\n",
    "        rgb_image = rgb_frames[frame].permute(1, 2, 0).cpu().numpy()\n",
    "        rgb_im.set_data(rgb_image)\n",
    "        \n",
    "        # Update flow data\n",
    "        flow_image = flow_frames[frame].cpu().numpy().reshape(32, 32)\n",
    "        flow_im.set_data(flow_image)\n",
    "        flow_im.set_clim(vmin=np.min(flow_image), vmax=np.max(flow_image))  # Set dynamic color range\n",
    "        \n",
    "        # Update additional RGB data\n",
    "        additional_rgb_image = additional_rgb[frame].cpu().numpy().reshape(32, 32)\n",
    "        add_rgb_im.set_data(additional_rgb_image)\n",
    "        add_rgb_im.set_clim(vmin=np.min(additional_rgb_image), vmax=np.max(additional_rgb_image))  # Set dynamic color range\n",
    "        \n",
    "        # Update IMU data\n",
    "        imu_values = imu_data[frame].cpu().numpy()  # shape (3,)\n",
    "        imu_plot_x.set_data([0, 1], [0, imu_values[0]])  # X-axis value\n",
    "        imu_plot_y.set_data([0, 1], [0, imu_values[1]])  # Y-axis value\n",
    "        imu_plot_z.set_data([0, 1], [0, imu_values[2]])  # Z-axis value\n",
    "\n",
    "        # Update depth sensor data\n",
    "        depth_value = depth_data[frame].cpu().numpy().item()  # Assuming depth_data has shape (num_frames, 1)\n",
    "        depth_plot[0].set_height(depth_value)\n",
    "        depth_plot[0].set_y(depth_value)\n",
    "\n",
    "        return [rgb_im, flow_im, add_rgb_im, imu_plot_x, imu_plot_y, imu_plot_z] + list(depth_plot)\n",
    "\n",
    "    # Create animation\n",
    "    ani = animation.FuncAnimation(fig, update, frames=num_frames, interval=200, blit=True)\n",
    "    \n",
    "    # Save as GIF using Pillow backend\n",
    "    ani.save('./visualizations/animated_video_with_imu_depth.gif', writer='pillow', fps=30)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_index = min(rgb_frames.shape[0], flow_frames.shape[0], rgb_feature.shape[0], imu_data.shape[0], depth_data.shape[0])\n",
    "\n",
    "# Example usage with your batch data\n",
    "animate_rgb_flow_imu_depth(rgb_frames[0:plot_index], flow_frames[0:plot_index], rgb_feature[0:plot_index], imu_data[0:plot_index], depth_data[0:plot_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tumbling window frame wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import math\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from torchvision import transforms\n",
    "# from collections import OrderedDict\n",
    "# import itertools\n",
    "# from torchvision.io import VideoReader\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "# ])\n",
    "\n",
    "# def collate_fn(batch, frames_per_clip=30):\n",
    "#     # Initialize empty lists for all possible modalities\n",
    "#     padded_audio_clips = []\n",
    "#     padded_flow_clips = []\n",
    "#     padded_rgb_clips = []\n",
    "#     padded_resnet_clips = []\n",
    "#     padded_resnet_exo_clips = []  # Added for resnet_exo modality\n",
    "#     padded_smartwatch_clips = []\n",
    "#     padded_depth_sensor_clips = []\n",
    "#     keystep_labels = []\n",
    "#     keystep_ids = []\n",
    "#     start_frames = []\n",
    "#     end_frames = []\n",
    "#     start_ts = []\n",
    "#     end_ts = []\n",
    "#     subject_ids = []\n",
    "#     trial_ids = []\n",
    "\n",
    "#     # Initialize max lengths for each modality, check if each is available\n",
    "#     max_flow_len = max([clip['flow'].shape[0] for clip in batch if isinstance(clip.get('flow', None), torch.Tensor)], default=0)\n",
    "#     max_audio_len = max([clip['audio'].shape[-1] for clip in batch if isinstance(clip.get('audio', None), torch.Tensor)], default=0)\n",
    "#     max_rgb_len = max([clip['rgb'].shape[0] for clip in batch if isinstance(clip.get('rgb', None), torch.Tensor)], default=0)\n",
    "#     max_resnet_len = max([clip['resnet'].shape[0] for clip in batch if isinstance(clip.get('resnet', None), torch.Tensor)], default=0)\n",
    "#     max_resnet_exo_len = max([clip['resnet_exo'].shape[0] for clip in batch if isinstance(clip.get('resnet_exo', None), torch.Tensor)], default=0)  # Added for resnet_exo modality\n",
    "#     max_smartwatch_len = max([clip['smartwatch'].shape[0] for clip in batch if isinstance(clip.get('smartwatch', None), torch.Tensor)], default=0)\n",
    "#     max_depth_sensor_len = max([clip['depth_sensor'].shape[0] for clip in batch if isinstance(clip.get('depth_sensor', None), torch.Tensor)], default=0)\n",
    "\n",
    "#     # print(\"max_flow_len:\",max_flow_len)\n",
    "#     # print(\"max_audio_len:\",max_audio_len)\n",
    "#     # print(\"max_rgb_len:\",max_rgb_len)\n",
    "#     # print(\"max_resnet_len:\",max_resnet_len)\n",
    "#     # print(\"max_resnet_exo_len:\",max_resnet_exo_len)\n",
    "#     # print(\"max_smartwatch_len:\",max_smartwatch_len)\n",
    "#     # print(\"max_depth_sensor_len:\",max_depth_sensor_len)\n",
    "    \n",
    "#     pad_length = 0\n",
    "#     for b in batch:\n",
    "#         # print(\"b:\",b)\n",
    "#         # Pad audio if available\n",
    "#         if 'audio' in b and isinstance(b['audio'], torch.Tensor):\n",
    "#             audio_clip = b['audio']\n",
    "#             audio_pad_size = max_audio_len - audio_clip.shape[0]\n",
    "#             if audio_pad_size > 0:\n",
    "#                 audio_pad = torch.zeros((audio_pad_size, *audio_clip.shape[1:]))\n",
    "#                 audio_clip = torch.cat([audio_clip, audio_pad], dim=0)\n",
    "#             padded_audio_clips.append(audio_clip)\n",
    "\n",
    "\n",
    "#         # Pad flow data if available\n",
    "#         if 'flow' in b and isinstance(b['flow'], torch.Tensor):\n",
    "#             flow_clip = b['flow']\n",
    "#             flow_pad_size = max_flow_len - flow_clip.shape[0]\n",
    "#             if flow_pad_size > 0:\n",
    "#                 flow_pad = torch.zeros((flow_pad_size, *flow_clip.shape[1:]))\n",
    "#                 flow_clip = torch.cat([flow_clip, flow_pad], dim=0)\n",
    "#             padded_flow_clips.append(flow_clip)\n",
    "\n",
    "#         # Pad rgb data if available\n",
    "#         if 'rgb' in b and isinstance(b['rgb'], torch.Tensor):\n",
    "#             rgb_clip = b['rgb']\n",
    "#             rgb_pad_size = max_rgb_len - rgb_clip.shape[0]\n",
    "#             if rgb_pad_size > 0:\n",
    "#                 rgb_pad = torch.zeros((rgb_pad_size, *rgb_clip.shape[1:]))\n",
    "#                 rgb_clip = torch.cat([rgb_clip, rgb_pad], dim=0)\n",
    "#             padded_rgb_clips.append(rgb_clip)\n",
    "\n",
    "#         # Pad resnet data if available\n",
    "#         if 'resnet' in b and isinstance(b['resnet'], torch.Tensor):\n",
    "#             resnet_clip = b['resnet']\n",
    "#             resnet_pad_size = max_resnet_len - resnet_clip.shape[0]\n",
    "#             if resnet_pad_size > 0:\n",
    "#                 resnet_pad = torch.zeros((resnet_pad_size, *resnet_clip.shape[1:]))\n",
    "#                 resnet_clip = torch.cat([resnet_clip, resnet_pad], dim=0)\n",
    "#             padded_resnet_clips.append(resnet_clip)\n",
    "\n",
    "#         # Pad resnet_exo data if available\n",
    "#         if 'resnet_exo' in b and isinstance(b['resnet_exo'], torch.Tensor):\n",
    "#             resnet_exo_clip = b['resnet_exo']\n",
    "#             resnet_exo_pad_size = max_resnet_exo_len - resnet_exo_clip.shape[0]\n",
    "#             if resnet_exo_pad_size > 0:\n",
    "#                 resnet_exo_pad = torch.zeros((resnet_exo_pad_size, *resnet_exo_clip.shape[1:]))\n",
    "#                 resnet_exo_clip = torch.cat([resnet_exo_clip, resnet_exo_pad], dim=0)\n",
    "#             padded_resnet_exo_clips.append(resnet_exo_clip)\n",
    "\n",
    "#         # Pad smartwatch data if available\n",
    "#         if 'smartwatch' in b and isinstance(b['smartwatch'], torch.Tensor):\n",
    "#             smartwatch_clip = b['smartwatch']\n",
    "#             if max_smartwatch_len > 0:\n",
    "#                 smartwatch_pad_size = max_smartwatch_len - smartwatch_clip.shape[0]\n",
    "#                 if smartwatch_pad_size > 0:\n",
    "#                     smartwatch_pad = torch.zeros((smartwatch_clip.shape[0], smartwatch_pad_size))\n",
    "#                     smartwatch_clip = torch.cat([smartwatch_clip, smartwatch_pad], dim=0)\n",
    "#             padded_smartwatch_clips.append(smartwatch_clip)\n",
    "\n",
    "#         # Pad depth_sensor data if available\n",
    "#         if 'depth_sensor' in b and isinstance(b['depth_sensor'], torch.Tensor):\n",
    "#             depth_sensor_clip = b['depth_sensor']\n",
    "#             if max_depth_sensor_len > 0:\n",
    "#                 depth_sensor_pad_size = max_depth_sensor_len - depth_sensor_clip.shape[0]\n",
    "#                 if depth_sensor_pad_size > 0:\n",
    "#                     depth_sensor_pad = torch.zeros((depth_sensor_clip.shape[0], depth_sensor_pad_size))\n",
    "#                     depth_sensor_clip = torch.cat([depth_sensor_clip, depth_sensor_pad], dim=0)\n",
    "#             padded_depth_sensor_clips.append(depth_sensor_clip)\n",
    "\n",
    "\n",
    "#         pad_length = frames_per_clip - len(b['keystep_id']) \n",
    "        \n",
    "#         if pad_length > 0:\n",
    "#             # repeat last element of keystep_id and keystep_label\n",
    "#             b['keystep_id'] = b['keystep_id'] + [b['keystep_id'][-1]]*pad_length\n",
    "#             b['start_frame'] = b['start_frame'] + [b['start_frame'][-1]]*pad_length\n",
    "#             b['end_frame'] = b['end_frame'] + [b['end_frame'][-1]]*pad_length\n",
    "#             b['start_t'] = b['start_t'] + [b['start_t'][-1]]*pad_length\n",
    "#             b['end_t'] = b['end_t'] + [b['end_t'][-1]]*pad_length\n",
    "        \n",
    "#         # Collect other fields\n",
    "#         keystep_labels.append(b['keystep_label'])\n",
    "#         keystep_ids.append(b['keystep_id'])\n",
    "#         start_frames.append(b['start_frame'])\n",
    "#         end_frames.append(b['end_frame'])\n",
    "#         start_ts.append(b['start_t'])\n",
    "#         end_ts.append(b['end_t'])\n",
    "#         subject_ids.append(b['subject_id'])\n",
    "#         trial_ids.append(b['trial_id'])\n",
    "\n",
    "#     output = {\n",
    "#         'keystep_label': keystep_labels,\n",
    "#         'keystep_id': torch.tensor(keystep_ids),\n",
    "#         'start_frame': torch.tensor(start_frames),\n",
    "#         'end_frame': torch.tensor(end_frames),\n",
    "#         'start_t': torch.tensor(start_ts),\n",
    "#         'end_t': torch.tensor(end_ts),\n",
    "#         'subject_id': subject_ids,\n",
    "#         'trial_id': trial_ids\n",
    "#     }\n",
    "\n",
    "#     # Only include modality data if it exists\n",
    "#     output['audio'] = torch.stack(padded_audio_clips) if padded_audio_clips else torch.zeros(0)\n",
    "#     output['flow'] = torch.stack(padded_flow_clips) if padded_flow_clips else torch.zeros(0)\n",
    "#     output['rgb'] = torch.stack(padded_rgb_clips) if padded_rgb_clips else torch.zeros(0)\n",
    "#     output['resnet'] = torch.stack(padded_resnet_clips) if padded_resnet_clips else torch.zeros(0)\n",
    "#     output['resnet_exo'] = torch.stack(padded_resnet_exo_clips) if padded_resnet_exo_clips else torch.zeros(0)  # Added for resnet_exo\n",
    "#     output['smartwatch'] = torch.stack(padded_smartwatch_clips) if padded_smartwatch_clips else torch.zeros(0)\n",
    "#     output['depth_sensor'] = torch.stack(padded_depth_sensor_clips) if padded_depth_sensor_clips else torch.zeros(0)\n",
    "\n",
    "#     return output\n",
    "\n",
    "\n",
    "# class WindowEgoExoEMSDataset(Dataset):\n",
    "#     def __init__(self, annotation_file, data_base_path, fps, \n",
    "#                 frames_per_clip=30, transform=None,\n",
    "#                 data_types=['resnet']):\n",
    "        \n",
    "#         self.annotation_file = annotation_file\n",
    "#         self.data_base_path = data_base_path\n",
    "#         self.fps = fps\n",
    "#         self.frames_per_clip = frames_per_clip  # Store frames_per_clip\n",
    "#         self.transform = transform\n",
    "#         self.data = []\n",
    "#         self.clip_indices = []  # This will store (item_idx, clip_idx) tuples\n",
    "#         self.data_types = data_types\n",
    "        \n",
    "#         self.data_dict = None\n",
    "#         self._load_annotations()\n",
    "#         self._split_windows()\n",
    "\n",
    "#     def _load_annotations(self):\n",
    "#         with open(self.annotation_file, 'r') as f:\n",
    "#             annotations = json.load(f)\n",
    "        \n",
    "#         subject_dict = {}\n",
    "#         for subject in annotations['subjects']:\n",
    "#             trial_dict ={}\n",
    "#             for trial in subject['trials']:\n",
    "#                 avail_streams = trial['streams']\n",
    "                \n",
    "#                 # Initialize paths to None by default\n",
    "#                 audio_path = None\n",
    "#                 flow_path = None\n",
    "#                 rgb_path = None\n",
    "#                 resnet_path = None\n",
    "#                 resnet_exo_path = None  # Added for resnet_exo modality\n",
    "#                 smartwatch_path = None\n",
    "#                 depth_sensor_path = None\n",
    "\n",
    "#                 # Check for each data type and retrieve the corresponding file path\n",
    "#                 if 'audio' in self.data_types:\n",
    "#                     audio_path = avail_streams.get('egocam_rgb_audio', {}).get('file_path', None)\n",
    "#                 if 'flow' in self.data_types:\n",
    "#                     flow_path = avail_streams.get('i3d_flow', {}).get('file_path', None)\n",
    "#                 if 'rgb' in self.data_types:\n",
    "#                     rgb_path = avail_streams.get('i3d_rgb', {}).get('file_path', None)\n",
    "#                 if 'resnet' in self.data_types:\n",
    "#                     resnet_path = avail_streams.get('resnet50', {}).get('file_path', None)\n",
    "#                 if 'resnet_exo' in self.data_types:\n",
    "#                     resnet_exo_path = avail_streams.get('resnet50-exo', {}).get('file_path', None)  # Adjust key as needed\n",
    "#                 if 'smartwatch' in self.data_types:\n",
    "#                     smartwatch_path = avail_streams.get('smartwatch_imu', {}).get('file_path', None)\n",
    "#                 if 'depth_sensor' in self.data_types:\n",
    "#                     depth_sensor_path = avail_streams.get('vl6180_ToF_depth', {}).get('file_path', None)\n",
    "\n",
    "#                 # Skip the trial if any required data type is not available\n",
    "#                 if ('flow' in self.data_types and not flow_path) or \\\n",
    "#                 ('audio' in self.data_types and not audio_path) or \\\n",
    "#                 ('rgb' in self.data_types and not rgb_path) or \\\n",
    "#                 ('resnet' in self.data_types and not resnet_path) or \\\n",
    "#                 ('resnet_exo' in self.data_types and not resnet_exo_path) or \\\n",
    "#                 ('smartwatch' in self.data_types and not smartwatch_path) or \\\n",
    "#                 ('depth_sensor' in self.data_types and not depth_sensor_path):\n",
    "#                     print(f\"[Warning] Skipping trial {trial['trial_id']} for subject {subject['subject_id']} due to missing data\")\n",
    "#                     continue\n",
    "                \n",
    "#                 if audio_path or flow_path or rgb_path or resnet_path or resnet_exo_path or smartwatch_path or depth_sensor_path:\n",
    "#                     keysteps = trial['keysteps']\n",
    "#                     keysteps_dict = []\n",
    "#                     for step in keysteps:\n",
    "#                         start_frame = math.floor(step['start_t'] * self.fps)\n",
    "#                         end_frame = math.floor(step['end_t'] * self.fps)\n",
    "#                         label = step['label']\n",
    "#                         keystep_id = step['class_id']\n",
    "\n",
    "#                         data_dict = {}\n",
    "#                         if 'audio' in self.data_types:\n",
    "#                             data_dict['audio_path'] = os.path.join(self.data_base_path, audio_path)\n",
    "#                         if 'flow' in self.data_types:\n",
    "#                             data_dict['flow_path'] = os.path.join(self.data_base_path, flow_path)\n",
    "#                         if 'rgb' in self.data_types:\n",
    "#                             data_dict['rgb_path'] = os.path.join(self.data_base_path, rgb_path)\n",
    "#                         if 'resnet' in self.data_types:\n",
    "#                             data_dict['resnet_path'] = os.path.join(self.data_base_path, resnet_path)\n",
    "#                         if 'resnet_exo' in self.data_types:\n",
    "#                             data_dict['resnet_exo_path'] = os.path.join(self.data_base_path, resnet_exo_path)\n",
    "#                         if 'smartwatch' in self.data_types:\n",
    "#                             data_dict['smartwatch_path'] = os.path.join(self.data_base_path, smartwatch_path)\n",
    "#                         if 'depth_sensor' in self.data_types:\n",
    "#                             data_dict['depth_sensor_path'] = os.path.join(self.data_base_path, depth_sensor_path)\n",
    "#                         data_dict['start_frame'] = start_frame\n",
    "#                         data_dict['end_frame'] = end_frame\n",
    "#                         data_dict['start_t'] = step['start_t']\n",
    "#                         data_dict['end_t'] = step['end_t']\n",
    "#                         data_dict['keystep_label'] = label\n",
    "#                         data_dict['keystep_id'] = keystep_id\n",
    "#                         data_dict['subject'] = subject['subject_id']\n",
    "#                         data_dict['trial'] = trial['trial_id']\n",
    "\n",
    "#                         keysteps_dict.append(data_dict)\n",
    "#                         self.data.append(data_dict)\n",
    "\n",
    "#                         trial_dict[trial['trial_id']] = keysteps_dict\n",
    "#                 subject_dict[subject['subject_id']] = trial_dict\n",
    "\n",
    "#         self.data_dict = subject_dict\n",
    "\n",
    "#     def _split_windows(self):\n",
    "#         print(\"Splitting data to windows\")\n",
    "#         windowed_clips = []\n",
    "#         current_window = []  # Initialize an empty current window\n",
    "#         accumulated_frames = 0  # Track how many frames have been accumulated in the current window\n",
    "\n",
    "#         current_subject = None\n",
    "#         current_trial = None\n",
    "\n",
    "\n",
    "#         for i, item in enumerate(self.data):\n",
    "#             start_frame = item['start_frame']\n",
    "#             end_frame = item['end_frame']\n",
    "#             keystep_id = item['keystep_id']\n",
    "#             keystep_label = item['keystep_label']\n",
    "#             subject_id = item['subject']\n",
    "#             trial_id = item['trial']\n",
    "#             num_frames_total = end_frame - start_frame\n",
    "\n",
    "                \n",
    "#         # If the subject or trial changes, store the current window and reset it\n",
    "#             if subject_id != current_subject or trial_id != current_trial:\n",
    "#                 if len(current_window) > 0:\n",
    "#                     windowed_clips.append(current_window)\n",
    "#                 current_window = []  # Reset the current window\n",
    "#                 accumulated_frames = 0  # Reset the frame counter\n",
    "#                 current_subject = subject_id  # Update current subject\n",
    "#                 current_trial = trial_id  # Update current trial\n",
    "\n",
    "#             # Loop through frames from the start to the end of the current keystep\n",
    "#             for j in range(start_frame, end_frame):\n",
    "#                 frame_data = {}\n",
    "#                 frame_data['frame'] = j\n",
    "\n",
    "#                 # Copy the relevant data for the current frame\n",
    "#                 for key, value in item.items():\n",
    "#                     if isinstance(value, (int, float, str)):\n",
    "#                         frame_data[key] = value\n",
    "#                     elif isinstance(value, (list, np.ndarray)):\n",
    "#                         if len(value) == num_frames_total:\n",
    "#                             frame_data[key] = value[j - start_frame]\n",
    "#                     else:\n",
    "#                         frame_data[key] = value\n",
    "\n",
    "#                 # Append the current frame data to the window\n",
    "#                 current_window.append(frame_data)\n",
    "#                 accumulated_frames += 1\n",
    "\n",
    "\n",
    "#                 # Once we reach the window size, store the window and reset\n",
    "#                 if accumulated_frames == self.frames_per_clip:\n",
    "#                     windowed_clips.append(current_window)\n",
    "#                     current_window = []  # Reset the current window\n",
    "#                     accumulated_frames = 0  # Reset the frame counter\n",
    "\n",
    "#         # # dump the data to a file\n",
    "#         # with open('data.json', 'w') as f:\n",
    "#         #     json.dump(windowed_clips, f)\n",
    "            \n",
    "#         self.data = windowed_clips\n",
    "#         print(f\"Total windowed clips: {len(windowed_clips)}\")\n",
    "\n",
    "            \n",
    "\n",
    "#     def __len__(self):\n",
    "#         # The length should now be based on the number of clips, not items\n",
    "#         # total_clips = len(self.data) // self.frames_per_clip if self.frames_per_clip else len(self.data)\n",
    "#         # dump the data to a file\n",
    "#         # with open('data.json', 'w') as f:\n",
    "#         #     json.dump(self.data, f)\n",
    "#         total_clips = len(self.data)\n",
    "#         return total_clips\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         window = self.data[idx]\n",
    "#         # print(\"window\", window)\n",
    "\n",
    "#         first_frame_of_clip = window[0]['frame']\n",
    "#         last_frame_of_clip = window[-1]['frame']+1\n",
    "        \n",
    "#         # print(f\"getting clip {idx} with frame {first_frame_of_clip} to {last_frame_of_clip} of length ({last_frame_of_clip - first_frame_of_clip})\")\n",
    "\n",
    "#         # Initialize dictionaries to hold accumulated frames for each modality\n",
    "#         batch_audio = []\n",
    "#         batch_flow = []\n",
    "#         batch_rgb = []\n",
    "#         batch_resnet = []\n",
    "#         batch_resnet_exo = []\n",
    "#         batch_sw_acc = []\n",
    "#         batch_depth_sensor = []\n",
    "\n",
    "#         # Initialize lists for metadata\n",
    "#         keystep_labels = []\n",
    "#         keystep_ids = []\n",
    "#         start_frames = []\n",
    "#         end_frames = []\n",
    "#         start_ts = []\n",
    "#         end_ts = []\n",
    "#         subject_ids = []\n",
    "#         trial_ids = []\n",
    "\n",
    "\n",
    "#         # Initialize variables\n",
    "#         flow = torch.zeros(0)\n",
    "#         rgb = torch.zeros(0)\n",
    "#         resnet = torch.zeros(0)\n",
    "#         resnet_exo = torch.zeros(0)  # Added for resnet_exo modality\n",
    "#         sw_acc = torch.zeros(0)\n",
    "#         depth_sensor_readings = torch.zeros(0)\n",
    "\n",
    "#         # Load flow if available\n",
    "#         if 'audio' in self.data_types:\n",
    "#             audio_path = window[0]['audio_path']\n",
    "#             clip_start_t = first_frame_of_clip / self.fps\n",
    "#             clip_end_t = last_frame_of_clip / self.fps\n",
    "\n",
    "#             # print(f\"loading audio from {clip_start_t} to {clip_end_t} from file {audio_path}\")\n",
    "\n",
    "#             audio_reader = VideoReader(audio_path, \"audio\")\n",
    "#             audio_clips = []\n",
    "#             for audio_frame in itertools.takewhile(lambda x: x['pts'] <= clip_end_t, audio_reader.seek(clip_start_t)):\n",
    "#                 audio_clips.append(audio_frame['data'])\n",
    "#             audio_clips = torch.cat(audio_clips, dim=0) if audio_clips else torch.zeros(1, 0)\n",
    "\n",
    "#             # pad the flow tensor to the i\n",
    "#             batch_audio.append(audio_clips)\n",
    "\n",
    "\n",
    "#         if 'flow' in self.data_types:\n",
    "#             flow_path = window[0]['flow_path']\n",
    "#             flow_npy = np.load(flow_path)\n",
    "#             flow_length = len(flow_npy)\n",
    "#             flow = torch.from_numpy(np.load(flow_path))[first_frame_of_clip:last_frame_of_clip]\n",
    "#                 # pad the flow tensor to the i\n",
    "#             batch_flow.append(flow)\n",
    "\n",
    "#         # Load rgb if available\n",
    "#         if 'rgb' in self.data_types:\n",
    "#             rgb_path =  window[0]['rgb_path']\n",
    "#             rgb_npy = np.load(rgb_path)\n",
    "#             rgb_length = len(rgb_npy)\n",
    "#             rgb = torch.from_numpy(np.load(rgb_path))[first_frame_of_clip:last_frame_of_clip]\n",
    "#                 # pad the rgb tensor to the i\n",
    "#             batch_rgb.append(rgb)\n",
    "\n",
    "#         # Load resnet if available\n",
    "#         if 'resnet' in self.data_types:\n",
    "#             resnet_path =  window[0]['resnet_path']\n",
    "#             resnet_npy = np.load(resnet_path)\n",
    "#             resnet_length = len(resnet_npy)\n",
    "#             resnet = torch.from_numpy(np.load(resnet_path))[first_frame_of_clip:last_frame_of_clip]\n",
    "#             batch_resnet.append(resnet)\n",
    "\n",
    "#         # Load resnet_exo if available\n",
    "#         if 'resnet_exo' in self.data_types:\n",
    "#             resnet_exo_path =  window[0]['resnet_exo_path']\n",
    "#             resnet_exo = torch.from_numpy(np.load(resnet_exo_path))[first_frame_of_clip:last_frame_of_clip]\n",
    "#             batch_resnet_exo.append(resnet_exo)\n",
    "\n",
    "#         # Load smartwatch data if available\n",
    "#         if 'smartwatch' in self.data_types:\n",
    "#             smartwatch_path =  window[0]['smartwatch_path']\n",
    "#             with open(smartwatch_path, 'r') as f:\n",
    "#                 lines = f.readlines()[1:]\n",
    "#             sw_acc = [line.strip() for line in lines][first_frame_of_clip:last_frame_of_clip]\n",
    "#             acc_x = [float(l.split(',')[0]) for l in sw_acc]\n",
    "#             acc_y = [float(l.split(',')[1]) for l in sw_acc]\n",
    "#             acc_z = [float(l.split(',')[2]) for l in sw_acc]\n",
    "#             sw_acc = torch.from_numpy(np.array([acc_x, acc_y, acc_z])).float()\n",
    "#             sw_acc = sw_acc.permute(1, 0)  # (frames, channels)\n",
    "#             batch_sw_acc.append(sw_acc)\n",
    "\n",
    "#         # Load depth_sensor data if available\n",
    "#         if 'depth_sensor' in self.data_types:\n",
    "#             depth_sensor_path =  window[0]['depth_sensor_path']\n",
    "#             with open(depth_sensor_path, 'r') as f:\n",
    "#                 lines = f.readlines()[1:]\n",
    "#             depth_sensor_readings = [line.strip() for line in lines][first_frame_of_clip:last_frame_of_clip]\n",
    "#             depth_reading = [float(l.split(',')[0]) for l in depth_sensor_readings]\n",
    "#             depth_sensor_readings = torch.from_numpy(np.array([depth_reading])).float()\n",
    "#             depth_sensor_readings = depth_sensor_readings.permute(1, 0)\n",
    "#             batch_depth_sensor.append(depth_sensor_readings)\n",
    "\n",
    "#         for frame in window:\n",
    "#             # Accumulate metadata for each frame\n",
    "#             keystep_labels.append( frame['keystep_label'])\n",
    "#             keystep_ids.append( frame['keystep_id'])\n",
    "#             start_frames.append( frame['start_frame'])\n",
    "#             end_frames.append( frame['end_frame'])\n",
    "#             start_ts.append( frame['start_t'])\n",
    "#             end_ts.append( frame['end_t'])\n",
    "#             subject_ids.append( frame['subject'])\n",
    "#             trial_ids.append( frame['trial'])\n",
    "\n",
    "#         # print(\"batch_resnet:\",batch_resnet[0].shape)\n",
    "#         # Stack frames to form a batch of frames_per_clip\n",
    "#         output = {\n",
    "#             'audio': (batch_audio[0]) if batch_audio else torch.zeros(1,0),\n",
    "#             'flow': (batch_flow[0]) if batch_flow else torch.zeros(0),\n",
    "#             'rgb': (batch_rgb[0]) if batch_rgb else torch.zeros(0),\n",
    "#             'resnet': (batch_resnet[0]) if batch_resnet else torch.zeros(0),\n",
    "#             'resnet_exo': (batch_resnet_exo[0]) if batch_resnet_exo else torch.zeros(0),\n",
    "#             'smartwatch': (batch_sw_acc[0]) if batch_sw_acc else torch.zeros(0),\n",
    "#             'depth_sensor': (batch_depth_sensor[0]) if batch_depth_sensor else torch.zeros(0),\n",
    "\n",
    "#             # Metadata (individual per frame)\n",
    "#             'keystep_label': keystep_labels,\n",
    "#             'keystep_id': keystep_ids,\n",
    "#             'start_frame': start_frames,\n",
    "#             'end_frame': end_frames,\n",
    "#             'start_t': start_ts,\n",
    "#             'end_t': end_ts,\n",
    "#             'subject_id': subject_ids,\n",
    "#             'trial_id': trial_ids\n",
    "#         }\n",
    "\n",
    "#         return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from EgoExoEMS.EgoExoEMS import *\n",
    "\n",
    "annotation_file = \"../../Annotations/splits/trials/train_split_segmentation.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "# train_annotation_file = \"../../Annotations/splits/keysteps/train_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "# val_annotation_file = \"../../Annotations/splits/keysteps/val_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "# test_annotation_file = \"../../Annotations/splits/keysteps/test_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "annotation_file = \"../../Annotations/main_annotation_segmentation.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "\n",
    "train_dataset = WindowEgoExoEMSDataset(annotation_file=annotation_file,\n",
    "                                data_base_path='',\n",
    "                                fps=30, frames_per_clip=120, transform=transform, data_types=[ 'resnet'])\n",
    "\n",
    "\n",
    "\n",
    "# Assuming your dataset has a 'targets' attribute with class labels\n",
    "\n",
    "\n",
    "print(\"Size of dataset:\",len(train_dataset))\n",
    "\n",
    "# Use a partial function or lambda to pass the frames_per_clip argument\n",
    "collate_fn_with_args = partial(window_collate_fn, frames_per_clip=120)\n",
    "\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_with_args)\n",
    "\n",
    "print(\"Size of data loader:\",len(data_loader))\n",
    "\n",
    "# # Access a sample\n",
    "# # batch = next(iter(data_loader))\n",
    "# # print(batch['resnet'].shape, batch['resnet_exo'].shape, batch['flow'].shape, batch['rgb'].shape, batch['smartwatch'].shape, batch['depth_sensor'].shape , batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "\n",
    "for idx,batch in enumerate(data_loader):\n",
    "    print(\"batch\", batch['resnet'].shape, batch['keystep_id'].shape, len(batch['keystep_label'][0]), batch['window_start_frame'], batch['window_end_frame'])\n",
    "    # print(\"batch\", batch['video'].shape, batch['keystep_id'].shape,batch['window_start_frame'], batch['window_end_frame'])\n",
    "    # print(batch['resnet'].shape, batch['audio'].shape, batch['resnet_exo'].shape, batch['flow'].shape, batch['rgb'].shape, batch['smartwatch'].shape, batch['depth_sensor'].shape , batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "    # print(batch['resnet'].shape)\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egoems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
