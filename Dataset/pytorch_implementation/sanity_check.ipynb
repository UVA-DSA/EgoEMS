{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "import time\n",
    "# from EgoExoEMS.EgoExoEMS import EgoExoEMSDataset, collate_fn, transform,CLIP_EgoExo_Dataset\n",
    "from EgoExoEMS.EgoExoEMS.EgoExoEMS import  EgoExoEMSDataset, collate_fn, transform,CLIP_EgoExo_Keystep_Dataset, clip_collate_fn, CLIP_EgoExo_Keystep_LIMITED_Dataset\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Accurate seek is not implemented for pyav backend\")\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Skipping trial s11 (missing data)\n",
      "[Warning] Skipping trial s2 (missing data)\n",
      "[Warning] Skipping trial s3 (missing data)\n",
      "[Warning] Skipping trial s4 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s6 (missing data)\n",
      "[Warning] Skipping trial s8 (missing data)\n",
      "[Warning] Skipping trial s9 (missing data)\n",
      "[Warning] Skipping trial s2 (missing data)\n",
      "[Warning] Skipping trial s3 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s6 (missing data)\n",
      "[Warning] Skipping trial s7 (missing data)\n",
      "[Warning] Skipping trial s6 (missing data)\n",
      "[Warning] Skipping trial s7 (missing data)\n",
      "[Warning] Skipping trial s10 (missing data)\n",
      "[Warning] Skipping trial s2 (missing data)\n",
      "[Warning] Skipping trial s3 (missing data)\n",
      "[Warning] Skipping trial s4 (missing data)\n",
      "[Warning] Skipping trial s6 (missing data)\n",
      "[Warning] Skipping trial s7 (missing data)\n",
      "[Warning] Skipping trial s8 (missing data)\n",
      "[Warning] Skipping trial s9 (missing data)\n",
      "[Warning] Skipping trial s1 (missing data)\n",
      "[Warning] Skipping trial s2 (missing data)\n",
      "[Warning] Skipping trial s4 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s6 (missing data)\n",
      "[Warning] Skipping trial s8 (missing data)\n",
      "[Warning] Skipping trial s3 (missing data)\n",
      "[Warning] Skipping trial s4 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s7 (missing data)\n",
      "[Warning] Skipping trial s10 (missing data)\n",
      "[Warning] Skipping trial s4 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s6 (missing data)\n",
      "[Warning] Skipping trial s7 (missing data)\n",
      "[Warning] Skipping trial s8 (missing data)\n",
      "[Warning] Skipping trial s9 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s7 (missing data)\n",
      "[Warning] Skipping trial s8 (missing data)\n",
      "[Warning] Skipping trial s3 (missing data)\n",
      "[Warning] Skipping trial s4 (missing data)\n",
      "[Warning] Skipping trial s8 (missing data)\n",
      "[Warning] Skipping trial s9 (missing data)\n",
      "[Warning] Skipping trial s2 (missing data)\n",
      "[Warning] Skipping trial s4 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s8 (missing data)\n",
      "[Warning] Skipping trial s9 (missing data)\n",
      "[Warning] Skipping trial s1 (missing data)\n",
      "[Warning] Skipping trial s2 (missing data)\n",
      "[Warning] Skipping trial s3 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s6 (missing data)\n",
      "[Warning] Skipping trial s7 (missing data)\n",
      "[Warning] Skipping trial s8 (missing data)\n",
      "[Warning] Skipping trial s9 (missing data)\n",
      "[Warning] Skipping trial s1 (missing data)\n",
      "[Warning] Skipping trial s2 (missing data)\n",
      "[Warning] Skipping trial s3 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s6 (missing data)\n",
      "[Warning] Skipping trial s1 (missing data)\n",
      "[Warning] Skipping trial s3 (missing data)\n",
      "[Warning] Skipping trial s4 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s6 (missing data)\n",
      "[Warning] Skipping trial s7 (missing data)\n",
      "[Warning] Skipping trial s2 (missing data)\n",
      "[Warning] Skipping trial s3 (missing data)\n",
      "[Warning] Skipping trial s4 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s7 (missing data)\n",
      "[Warning] Skipping trial s8 (missing data)\n",
      "[Warning] Skipping trial s2 (missing data)\n",
      "[Warning] Skipping trial s4 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s6 (missing data)\n",
      "[Warning] Skipping trial s7 (missing data)\n",
      "[Warning] Skipping trial s8 (missing data)\n",
      "[Warning] Skipping trial s9 (missing data)\n",
      "[Warning] Skipping trial s1 (missing data)\n",
      "[Warning] Skipping trial s2 (missing data)\n",
      "[Warning] Skipping trial s3 (missing data)\n",
      "[Warning] Skipping trial s4 (missing data)\n",
      "[Warning] Skipping trial s5 (missing data)\n",
      "[Warning] Skipping trial s6 (missing data)\n",
      "[Warning] Skipping trial s7 (missing data)\n",
      "[Warning] Skipping trial s8 (missing data)\n",
      "[Warning] Skipping trial s9 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 6 (missing data)\n",
      "[Warning] Skipping trial 7 (missing data)\n",
      "[Warning] Skipping trial 8 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 1 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 1 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 1 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 1 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 1 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 1 (missing data)\n",
      "[Warning] Skipping trial 2 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 1 (missing data)\n",
      "[Warning] Skipping trial 2 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 1 (missing data)\n",
      "[Warning] Skipping trial 2 (missing data)\n",
      "[Warning] Skipping trial 3 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "[Warning] Skipping trial 1 (missing data)\n",
      "[Warning] Skipping trial 0 (missing data)\n",
      "Train class stats:  {'approach_patient': 33, 'check_responsiveness': 66, 'check_breathing': 63, 'check_pulse': 100, 'request_assistance': 24, 'turn_on_aed': 65, 'attach_defib_pads': 79, 'request_aed': 28, 'chest_compressions': 258, 'no_action': 61, 'clear_for_analysis': 72, 'clear_for_shock': 70, 'administer_shock_aed': 72, 'place_bvm': 196, 'open_airway': 13, 'ventilate_patient': 244}\n",
      "Train Number of classes:  16\n",
      "Number of batches in train loader: 2244\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 1:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_shock']\n",
      "Start Frames: tensor([1988])\n",
      "End Frames: tensor([2153])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng6']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 2:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2521])\n",
      "End Frames: tensor([3103])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng6']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 3:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 63936, 2])\n",
      "Label ['clear_for_shock']\n",
      "Start Frames: tensor([2043])\n",
      "End Frames: tensor([2162])\n",
      "Trial IDs: ['9']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 4:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 35712, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([1712])\n",
      "End Frames: tensor([1779])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 5:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 59328, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([2770])\n",
      "End Frames: tensor([2880])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ng6']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 6:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['attach_defib_pads']\n",
      "Start Frames: tensor([813])\n",
      "End Frames: tensor([2030])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ms2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 7:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 72576, 2])\n",
      "Label ['no_action']\n",
      "Start Frames: tensor([3583])\n",
      "End Frames: tensor([3718])\n",
      "Trial IDs: ['6']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 8:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([327])\n",
      "End Frames: tensor([816])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 9:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([3019])\n",
      "End Frames: tensor([3594])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 10:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['no_action']\n",
      "Start Frames: tensor([0])\n",
      "End Frames: tensor([2501])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 11:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([473])\n",
      "End Frames: tensor([975])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 12:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 55296, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([3762])\n",
      "End Frames: tensor([3864])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['wa1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 13:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([356])\n",
      "End Frames: tensor([1624])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 14:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([2496])\n",
      "End Frames: tensor([3353])\n",
      "Trial IDs: ['18']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 15:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 77760, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([4282])\n",
      "End Frames: tensor([4427])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 16:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2334])\n",
      "End Frames: tensor([2802])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 17:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 33408, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([3111])\n",
      "End Frames: tensor([3173])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng6']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 18:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([3143])\n",
      "End Frames: tensor([3568])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 19:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2498])\n",
      "End Frames: tensor([3025])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 20:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_analysis']\n",
      "Start Frames: tensor([2483])\n",
      "End Frames: tensor([2914])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 21:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 27648, 2])\n",
      "Label ['check_pulse']\n",
      "Start Frames: tensor([318])\n",
      "End Frames: tensor([369])\n",
      "Trial IDs: ['10']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 22:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([590])\n",
      "End Frames: tensor([1054])\n",
      "Trial IDs: ['9']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 23:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2330])\n",
      "End Frames: tensor([2924])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng6']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 24:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 51264, 2])\n",
      "Label ['clear_for_shock']\n",
      "Start Frames: tensor([1914])\n",
      "End Frames: tensor([2008])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 25:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['attach_defib_pads']\n",
      "Start Frames: tensor([1309])\n",
      "End Frames: tensor([1922])\n",
      "Trial IDs: ['16']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 26:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['administer_shock_aed']\n",
      "Start Frames: tensor([2169])\n",
      "End Frames: tensor([2323])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng6']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 27:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([286])\n",
      "End Frames: tensor([2127])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng9']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 28:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2619])\n",
      "End Frames: tensor([3062])\n",
      "Trial IDs: ['11']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 29:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 39744, 2])\n",
      "Label ['check_pulse']\n",
      "Start Frames: tensor([2341])\n",
      "End Frames: tensor([2415])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 30:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 25344, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([816])\n",
      "End Frames: tensor([862])\n",
      "Trial IDs: ['11']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 31:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2954])\n",
      "End Frames: tensor([3559])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 32:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([626])\n",
      "End Frames: tensor([792])\n",
      "Trial IDs: ['17']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 33:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['attach_defib_pads']\n",
      "Start Frames: tensor([1015])\n",
      "End Frames: tensor([1322])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 34:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([307])\n",
      "End Frames: tensor([786])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['wa1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 35:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['attach_defib_pads']\n",
      "Start Frames: tensor([1309])\n",
      "End Frames: tensor([1922])\n",
      "Trial IDs: ['16']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 36:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 38639, 2])\n",
      "Label ['approach_patient']\n",
      "Start Frames: tensor([0])\n",
      "End Frames: tensor([74])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ms2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 37:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 28800, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([951])\n",
      "End Frames: tensor([1003])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 38:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 65664, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([4028])\n",
      "End Frames: tensor([4151])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ms2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 39:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2588])\n",
      "End Frames: tensor([3113])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['wa1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 40:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 33984, 2])\n",
      "Label ['open_airway']\n",
      "Start Frames: tensor([1008])\n",
      "End Frames: tensor([1070])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 41:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 21312, 2])\n",
      "Label ['turn_on_aed']\n",
      "Start Frames: tensor([1305])\n",
      "End Frames: tensor([1343])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 42:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_analysis']\n",
      "Start Frames: tensor([1586])\n",
      "End Frames: tensor([1832])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 43:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 27072, 2])\n",
      "Label ['turn_on_aed']\n",
      "Start Frames: tensor([2839])\n",
      "End Frames: tensor([2889])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 44:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([332])\n",
      "End Frames: tensor([819])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 45:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_shock']\n",
      "Start Frames: tensor([1423])\n",
      "End Frames: tensor([1595])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 46:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 46080, 2])\n",
      "Label ['check_responsiveness']\n",
      "Start Frames: tensor([72])\n",
      "End Frames: tensor([158])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 47:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_analysis']\n",
      "Start Frames: tensor([2054])\n",
      "End Frames: tensor([2340])\n",
      "Trial IDs: ['11']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 48:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['attach_defib_pads']\n",
      "Start Frames: tensor([815])\n",
      "End Frames: tensor([1206])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ms2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 49:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([792])\n",
      "End Frames: tensor([1998])\n",
      "Trial IDs: ['17']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 50:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([3867])\n",
      "End Frames: tensor([4762])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 51:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_shock']\n",
      "Start Frames: tensor([1624])\n",
      "End Frames: tensor([1867])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 52:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2265])\n",
      "End Frames: tensor([2762])\n",
      "Trial IDs: ['8']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 53:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 75456, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([1483])\n",
      "End Frames: tensor([1623])\n",
      "Trial IDs: ['9']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 54:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 76608, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([1151])\n",
      "End Frames: tensor([1294])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 55:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([338])\n",
      "End Frames: tensor([1344])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ms2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 56:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 39744, 2])\n",
      "Label ['check_breathing']\n",
      "Start Frames: tensor([152])\n",
      "End Frames: tensor([226])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 57:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['attach_defib_pads']\n",
      "Start Frames: tensor([1015])\n",
      "End Frames: tensor([1322])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 58:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 55872, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([3655])\n",
      "End Frames: tensor([3759])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 59:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([309])\n",
      "End Frames: tensor([705])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 60:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 62784, 2])\n",
      "Label ['check_pulse']\n",
      "Start Frames: tensor([2253])\n",
      "End Frames: tensor([2369])\n",
      "Trial IDs: ['18']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 61:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['request_aed']\n",
      "Start Frames: tensor([362])\n",
      "End Frames: tensor([533])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 62:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2736])\n",
      "End Frames: tensor([4252])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 63:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([3507])\n",
      "End Frames: tensor([3948])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 64:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([407])\n",
      "End Frames: tensor([844])\n",
      "Trial IDs: ['13']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 65:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([3679])\n",
      "End Frames: tensor([3836])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng7']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 66:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 48960, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([1739])\n",
      "End Frames: tensor([1830])\n",
      "Trial IDs: ['8']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 67:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([1095])\n",
      "End Frames: tensor([1557])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng9']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 68:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_analysis']\n",
      "Start Frames: tensor([1502])\n",
      "End Frames: tensor([1933])\n",
      "Trial IDs: ['20']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 69:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 43776, 2])\n",
      "Label ['check_responsiveness']\n",
      "Start Frames: tensor([133])\n",
      "End Frames: tensor([214])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 70:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([1760])\n",
      "End Frames: tensor([2392])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 71:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 52416, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([3766])\n",
      "End Frames: tensor([3864])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 72:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2966])\n",
      "End Frames: tensor([3416])\n",
      "Trial IDs: ['10']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 73:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 23616, 2])\n",
      "Label ['request_aed']\n",
      "Start Frames: tensor([426])\n",
      "End Frames: tensor([470])\n",
      "Trial IDs: ['8']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 74:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['turn_on_aed']\n",
      "Start Frames: tensor([3662])\n",
      "End Frames: tensor([3939])\n",
      "Trial IDs: ['7']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 75:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([3572])\n",
      "End Frames: tensor([4124])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 76:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 77760, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([960])\n",
      "End Frames: tensor([1105])\n",
      "Trial IDs: ['16']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 77:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_analysis']\n",
      "Start Frames: tensor([3231])\n",
      "End Frames: tensor([3469])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 78:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([524])\n",
      "End Frames: tensor([721])\n",
      "Trial IDs: ['20']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 79:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_shock']\n",
      "Start Frames: tensor([2572])\n",
      "End Frames: tensor([2747])\n",
      "Trial IDs: ['13']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 80:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 74880, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([3263])\n",
      "End Frames: tensor([3402])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 81:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['attach_defib_pads']\n",
      "Start Frames: tensor([998])\n",
      "End Frames: tensor([1333])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 82:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([270])\n",
      "End Frames: tensor([2120])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 83:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 16704, 2])\n",
      "Label ['turn_on_aed']\n",
      "Start Frames: tensor([1398])\n",
      "End Frames: tensor([1428])\n",
      "Trial IDs: ['7']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 84:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['turn_on_aed']\n",
      "Start Frames: tensor([2005])\n",
      "End Frames: tensor([3776])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 85:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['administer_shock_aed']\n",
      "Start Frames: tensor([2366])\n",
      "End Frames: tensor([2556])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 86:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2033])\n",
      "End Frames: tensor([2589])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 87:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 61056, 2])\n",
      "Label ['check_breathing']\n",
      "Start Frames: tensor([270])\n",
      "End Frames: tensor([383])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 88:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 65088, 2])\n",
      "Label ['administer_shock_aed']\n",
      "Start Frames: tensor([2299])\n",
      "End Frames: tensor([2420])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 89:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_analysis']\n",
      "Start Frames: tensor([1928])\n",
      "End Frames: tensor([2349])\n",
      "Trial IDs: ['17']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 90:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 53568, 2])\n",
      "Label ['administer_shock_aed']\n",
      "Start Frames: tensor([1851])\n",
      "End Frames: tensor([1951])\n",
      "Trial IDs: ['12']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 91:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 48384, 2])\n",
      "Label ['check_pulse']\n",
      "Start Frames: tensor([264])\n",
      "End Frames: tensor([354])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 92:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['attach_defib_pads']\n",
      "Start Frames: tensor([1034])\n",
      "End Frames: tensor([1618])\n",
      "Trial IDs: ['18']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 93:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['attach_defib_pads']\n",
      "Start Frames: tensor([1570])\n",
      "End Frames: tensor([2328])\n",
      "Trial IDs: ['8']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 94:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['attach_defib_pads']\n",
      "Start Frames: tensor([1384])\n",
      "End Frames: tensor([1805])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng6']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 95:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([3238])\n",
      "End Frames: tensor([3713])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 96:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([472])\n",
      "End Frames: tensor([2508])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 97:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 47808, 2])\n",
      "Label ['check_breathing']\n",
      "Start Frames: tensor([323])\n",
      "End Frames: tensor([412])\n",
      "Trial IDs: ['19']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 98:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2789])\n",
      "End Frames: tensor([3233])\n",
      "Trial IDs: ['9']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 99:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['ventilate_patient']\n",
      "Start Frames: tensor([3826])\n",
      "End Frames: tensor([3991])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 100:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([3174])\n",
      "End Frames: tensor([3709])\n",
      "Trial IDs: ['8']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 101:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2834])\n",
      "End Frames: tensor([3251])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 102:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 38016, 2])\n",
      "Label ['administer_shock_aed']\n",
      "Start Frames: tensor([1819])\n",
      "End Frames: tensor([1890])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ms2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 103:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 61632, 2])\n",
      "Label ['administer_shock_aed']\n",
      "Start Frames: tensor([2012])\n",
      "End Frames: tensor([2126])\n",
      "Trial IDs: ['9']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 104:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2736])\n",
      "End Frames: tensor([4252])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 105:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['check_pulse']\n",
      "Start Frames: tensor([158])\n",
      "End Frames: tensor([375])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 106:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([4558])\n",
      "End Frames: tensor([5156])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['wa1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 107:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 21888, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([994])\n",
      "End Frames: tensor([1034])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng6']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 108:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['attach_defib_pads']\n",
      "Start Frames: tensor([1052])\n",
      "End Frames: tensor([1683])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 109:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_analysis']\n",
      "Start Frames: tensor([3306])\n",
      "End Frames: tensor([3799])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 110:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['administer_shock_aed']\n",
      "Start Frames: tensor([4682])\n",
      "End Frames: tensor([4838])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 111:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 77760, 2])\n",
      "Label ['administer_shock_aed']\n",
      "Start Frames: tensor([3842])\n",
      "End Frames: tensor([3986])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 112:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2868])\n",
      "End Frames: tensor([3345])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 113:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2113])\n",
      "End Frames: tensor([2702])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ms2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 114:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2517])\n",
      "End Frames: tensor([3814])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 115:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 46080, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([2461])\n",
      "End Frames: tensor([2546])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ms2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 116:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([286])\n",
      "End Frames: tensor([2127])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng9']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 117:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([3199])\n",
      "End Frames: tensor([3709])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 118:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 43200, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([2784])\n",
      "End Frames: tensor([2864])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ms2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 119:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 29952, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([235])\n",
      "End Frames: tensor([289])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 120:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2832])\n",
      "End Frames: tensor([3270])\n",
      "Trial IDs: ['20']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 121:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 29376, 2])\n",
      "Label ['approach_patient']\n",
      "Start Frames: tensor([64])\n",
      "End Frames: tensor([118])\n",
      "Trial IDs: ['6']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 122:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 44928, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([4061])\n",
      "End Frames: tensor([4144])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 123:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 31680, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([4224])\n",
      "End Frames: tensor([4282])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 124:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['clear_for_shock']\n",
      "Start Frames: tensor([3799])\n",
      "End Frames: tensor([3953])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 125:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([286])\n",
      "End Frames: tensor([2127])\n",
      "Trial IDs: ['1']\n",
      "Subject IDs: ['ng9']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 126:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2362])\n",
      "End Frames: tensor([4158])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng7']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 127:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 13248, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([271])\n",
      "End Frames: tensor([295])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 128:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 12096, 2])\n",
      "Label ['request_aed']\n",
      "Start Frames: tensor([411])\n",
      "End Frames: tensor([433])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 129:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([1167])\n",
      "End Frames: tensor([1624])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 130:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 72000, 2])\n",
      "Label ['no_action']\n",
      "Start Frames: tensor([2508])\n",
      "End Frames: tensor([2642])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 131:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([5284])\n",
      "End Frames: tensor([5972])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['wa1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 132:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([1265])\n",
      "End Frames: tensor([1739])\n",
      "Trial IDs: ['8']\n",
      "Subject IDs: ['ng3']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 133:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([3036])\n",
      "End Frames: tensor([3569])\n",
      "Trial IDs: ['5']\n",
      "Subject IDs: ['ng5']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 134:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2437])\n",
      "End Frames: tensor([2912])\n",
      "Trial IDs: ['4']\n",
      "Subject IDs: ['ng4']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 135:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 36864, 2])\n",
      "Label ['place_bvm']\n",
      "Start Frames: tensor([4013])\n",
      "End Frames: tensor([4081])\n",
      "Trial IDs: ['3']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 136:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([3386])\n",
      "End Frames: tensor([3821])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ng8']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 137:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([472])\n",
      "End Frames: tensor([2508])\n",
      "Trial IDs: ['0']\n",
      "Subject IDs: ['ms1']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 138:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([277])\n",
      "End Frames: tensor([2149])\n",
      "Trial IDs: ['2']\n",
      "Subject IDs: ['ms2']\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Batch 139:\n",
      "clip_ego shape: torch.Size([1, 0])\n",
      "smartwatch shape: torch.Size([1, 150, 3])\n",
      "depth_sensor shape: torch.Size([1, 150, 1])\n",
      "audio shape: torch.Size([1, 80080, 2])\n",
      "Label ['chest_compressions']\n",
      "Start Frames: tensor([2379])\n",
      "End Frames: tensor([3258])\n",
      "Trial IDs: ['18']\n",
      "Subject IDs: ['ng5']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of batches in train loader:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-*\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/conda/envs/egoems/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/conda/envs/egoems/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/conda/envs/egoems/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/conda/envs/egoems/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/sfs/gpfs/tardis/home/cjh9fw/Desktop/2024/repos/EgoExoEMS/Dataset/pytorch_implementation/EgoExoEMS/EgoExoEMS/EgoExoEMS.py:367\u001b[0m, in \u001b[0;36mEgoExoEMSDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    365\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    366\u001b[0m seg \u001b[38;5;241m=\u001b[39m lines[item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_frame\u001b[39m\u001b[38;5;124m'\u001b[39m]:item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_frame\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m--> 367\u001b[0m xs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(l\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m seg]\n\u001b[1;32m    368\u001b[0m ys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(l\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m seg]\n\u001b[1;32m    369\u001b[0m zs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(l\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m seg]\n",
      "File \u001b[0;32m/sfs/gpfs/tardis/home/cjh9fw/Desktop/2024/repos/EgoExoEMS/Dataset/pytorch_implementation/EgoExoEMS/EgoExoEMS/EgoExoEMS.py:367\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    365\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    366\u001b[0m seg \u001b[38;5;241m=\u001b[39m lines[item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_frame\u001b[39m\u001b[38;5;124m'\u001b[39m]:item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_frame\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m--> 367\u001b[0m xs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m seg]\n\u001b[1;32m    368\u001b[0m ys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(l\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m seg]\n\u001b[1;32m    369\u001b[0m zs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(l\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m seg]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "root = \"/standard/UVA-DSA/NIST EMS Project Data/EgoExoEMS_CVPR2025/Dataset/Final/ng9/cardiac_arrest/1/GoPro/GX010346_encoded_trimmed.mp4\"  # Folder in which all videos lie in a specific structure\n",
    "# annotation_file = \"../../Annotations/main_annotation_classification.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "annotation_file = \"../../Annotations/aaai26_main_annotation_classification.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "# annotation_file = \"../../Annotations/splits/trials/test_split_classification.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "# train_annotation_file = \"../../Annotations/splits/keysteps/train_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "# val_annotation_file = \"../../Annotations/splits/keysteps/val_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "# test_annotation_file = \"../../Annotations/splits/keysteps/test_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "\n",
    "# train_dataset = EgoExoEMSDataset(annotation_file=annotation_file,\n",
    "#                                 data_base_path='',\n",
    "#                                 fps=29.97, frames_per_clip=None, transform=transform, data_types=[ 'video', 'audio'], task=\"classification\")\n",
    "\n",
    "# # Access a sample\n",
    "# print(len(train_dataset))\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True)\n",
    "\n",
    "\n",
    "fps = 29.97\n",
    "observation_window = 150\n",
    "data_types = [  'audio','smartwatch', 'depth_sensor']\n",
    "task = \"classification\"  # or \"segmentation\" or \"cpr_quality\"\n",
    "\n",
    "train_dataset = EgoExoEMSDataset(annotation_file=annotation_file,\n",
    "                                data_base_path='',\n",
    "                                fps=fps, frames_per_clip=observation_window, transform=transform, data_types=data_types, task=task)\n",
    "\n",
    "# val_dataset = EgoExoEMSDataset(annotation_file=args.dataloader_params[\"val_annotation_path\"],\n",
    "#                                 data_base_path='',\n",
    "#                                 fps=fps, frames_per_clip=observation_window, transform=transform, data_types=data_types, task=task)\n",
    "\n",
    "# test_dataset = EgoExoEMSDataset(annotation_file=args.dataloader_params[\"test_annotation_path\"],\n",
    "#                                 data_base_path='',\n",
    "#                                 fps=fps, frames_per_clip=observation_window, transform=transform, data_types=data_types, task=task)\n",
    "\n",
    "\n",
    "train_class_stats = train_dataset._get_class_stats()\n",
    "print(\"Train class stats: \", train_class_stats)\n",
    "# print number of keys in the dictionary\n",
    "print(\"Train Number of classes: \", len(train_class_stats.keys()))\n",
    "\n",
    "# Create DataLoaders for training and validation subsets\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "print(\"Number of batches in train loader:\", len(train_loader))\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(\"-*\"*30)\n",
    "    print(f\"Batch {i+1}:\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"clip_ego shape:\", batch['clip_ego'].shape)\n",
    "    print(\"smartwatch shape:\", batch['smartwatch'].shape)\n",
    "    print(\"depth_sensor shape:\", batch['depth_sensor'].shape)\n",
    "    print(\"audio shape:\", batch['audio'].shape)\n",
    "\n",
    "\n",
    "    # save the video frames to a file for debugging\n",
    "    # Get the first video in batch\n",
    "    # video_frames = batch['frames'][0]  # shape: (num_frames, C, H, W)\n",
    "    # video_frames = video_frames.permute(0, 2, 3, 1)  # (num_frames, H, W, C)\n",
    "\n",
    "    # # Convert to uint8\n",
    "    # video_frames = (video_frames * 255).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    # # Optional: ensure only 3 channels if necessary\n",
    "    # if video_frames.shape[-1] > 3:\n",
    "    #     video_frames = video_frames[..., :3]\n",
    "\n",
    "    # # Define output path\n",
    "    # video_path = \"debug_video.mp4\"\n",
    "\n",
    "    # # Setup video writer\n",
    "    # height, width, channels = video_frames[0].shape\n",
    "    # fps = 30  # or your actual frame rate\n",
    "\n",
    "    # out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    # for frame in video_frames:\n",
    "    #     out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # out.release()\n",
    "    # print(f\"Saved debug video to {video_path}\")\n",
    "\n",
    "                            # d['keystep_label'] = lbl\n",
    "                            # d['keystep_id'] = cid\n",
    "                            # d['subject'] = subject['subject_id']\n",
    "                            # d['trial'] = trial['trial_id']\n",
    "\n",
    "    print(\"Label\", batch['keystep_label'])\n",
    "    print(\"Start Frames:\", batch['start_frame'])\n",
    "    print(\"End Frames:\", batch['end_frame'])\n",
    "    print(\"Trial IDs:\", batch['trial_id'])\n",
    "    print(\"Subject IDs:\", batch['subject_id'])\n",
    "    # break  # Remove this line to iterate through all batches\n",
    "\n",
    "# for i, batch in enumerate(train_loader):\n",
    "#     print(f\"Batch {i+1}:\")\n",
    "#     print(\"Video shape:\", batch['video'].shape)\n",
    "#     print(\"Audio shape:\", batch['audio'].shape)\n",
    "#     print(\"Labels:\", batch['labels'])\n",
    "#     print(\"File IDs:\", batch['file_ids'])\n",
    "#     print(\"File Paths:\", batch['file_paths'])\n",
    "\n",
    "#     break  # Remove this line to iterate through all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CLIP_EgoExo_Keystep_Dataset(annotation_file=annotation_file, fps= 29.97, data_base_path=\"\")\n",
    "# dataset = CLIP_EgoExo_Keystep_LIMITED_Dataset(annotation_file=annotation_file, fps= 29.97, data_base_path=\"\",max_neg_samples=20,max_pos_samples=20)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=clip_collate_fn, num_workers=8, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataloader))\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "    print(\"--------------------\")\n",
    "    print(data['keystep_id'])\n",
    "    print(data['pairing_info'])\n",
    "    print(\"--------------------\")\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = '/standard/UVA-DSA/NIST EMS Project Data/EgoExoEMS_CVPR2025/Dataset/Final/ng3/cardiac_arrest/9/i3d_flow/GX010374_encoded_trimmed_flow.npy'\n",
    "gopro_path = '/standard/UVA-DSA/NIST EMS Project Data/EgoExoEMS_CVPR2025/Dataset/Final/ng3/cardiac_arrest/9/GoPro/GX010374_encoded_trimmed.mp4'\n",
    "\n",
    "# load the flow\n",
    "flow = np.load(flow_path)\n",
    "print(flow.shape)\n",
    "\n",
    "import math\n",
    "\n",
    "print((2724-2626))\n",
    "print(math.ceil((2724-2626)/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# from torchvision.io import VideoReader\n",
    "# import os\n",
    "# import itertools\n",
    "\n",
    "# start_t = 71.30542\n",
    "# end_t = 72.993\n",
    "# video_path = root\n",
    "# # video_path = \"/standard/UVA-DSA/NIST EMS Project Data/CognitiveEMS_Datasets/North_Garden/Sep_2024/Raw/24-09-2024/Bhavik/cardiac_arrest/4/GoPro/GX010399.MP4\"  # Folder in which all videos lie in a specific structure\n",
    "# video_reader = VideoReader(video_path, \"video\")\n",
    "# frames = []\n",
    "\n",
    "\n",
    "# for frame in itertools.takewhile(lambda x: x['pts'] <= end_t, video_reader.seek(start_t)):\n",
    "#     if(frame['pts'] < start_t):\n",
    "#         continue\n",
    "#     print(frame['pts'])\n",
    "#     img_tensor = transform(frame['data'])\n",
    "#     frames.append(img_tensor)\n",
    "\n",
    "# frames = torch.stack(frames)\n",
    "# print(\"Seeking from \", start_t, \" to \", end_t, \"for video \", video_path)\n",
    "# print(\"Frames shape: \", frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data loader\n",
    "# batch size is 1 for simplicity and to ensure only a full clip related to a key step is given without collating.\n",
    "# if batch size is greater than 1, collate_fn will be called to collate the data.\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "print(len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the data loader and print the shape of the batch\n",
    "for batch in data_loader:\n",
    "    # print(batch['frames'].shape, batch['audio'].shape,batch['resnet'].shape,batch['resnet_exo'].shape, batch['flow'].shape, batch['rgb'].shape, batch['smartwatch'].shape, batch['depth_sensor'].shape , batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "    print(batch['resnet'].shape, batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "    print(\"*\"*4 + \"=\"*50 + \"*\"*4)\n",
    "    # break\n",
    "    \n",
    "    # audio_tensor = batch['audio'][0]\n",
    "    # #transpose\n",
    "    # audio_tensor = audio_tensor.transpose(0,1)\n",
    "    # torchaudio.save(\"./visualizations/audio.wav\", audio_tensor,48000)\n",
    "    # break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchaudio feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "\n",
    "# Example batch of audio clips (batch, samples, channels)\n",
    "audio_clips = batch['audio']  # Assume shape [batch, samples, channels]\n",
    "\n",
    "# Parameters for feature extraction\n",
    "sample_rate = 48000  # Adjust based on your dataset\n",
    "n_mels = 64  # Number of Mel filter banks\n",
    "hop_length = 512\n",
    "n_fft = 1024\n",
    "\n",
    "# MelSpectrogram transform\n",
    "mel_spectrogram = transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mels=n_mels,\n",
    "    hop_length=hop_length,\n",
    "    n_fft=n_fft\n",
    ")\n",
    "\n",
    "# Process audio for one channel (if you want stereo/multi-channel, handle each channel accordingly)\n",
    "batch_size, num_samples, num_channels = audio_clips.shape\n",
    "\n",
    "# If using mono audio (process first channel as an example)\n",
    "mel_features = mel_spectrogram(audio_clips[:, :, 0])  # Shape: [batch, n_mels, time]\n",
    "\n",
    "# To get the desired shape [batch, samples, feature_dim], we treat the time axis as the \"samples\"\n",
    "# Here, n_mels will be the feature dimension and time will be the new samples axis\n",
    "mel_features = mel_features.permute(0, 2, 1)  # Shape: [batch, time(samples), n_mels(feature_dim)]\n",
    "\n",
    "print(f\"Mel-spectrogram features shape: {mel_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual channel mel-spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example batch of audio clips (batch, samples, channels)\n",
    "audio_clips = batch['audio']  # Assume shape [batch, samples, channels]\n",
    "\n",
    "# Parameters for feature extraction\n",
    "sample_rate = 48000  # Adjust based on your dataset\n",
    "n_mels = 64  # Number of Mel filter banks\n",
    "hop_length = 512\n",
    "n_fft = 1024\n",
    "\n",
    "# MelSpectrogram transform\n",
    "mel_spectrogram = transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mels=n_mels,\n",
    "    hop_length=hop_length,\n",
    "    n_fft=n_fft\n",
    ")\n",
    "\n",
    "# Extract Mel-spectrogram for both channels\n",
    "mel_spec_left = mel_spectrogram(audio_clips[:, :, 0])  # Left channel\n",
    "mel_spec_right = mel_spectrogram(audio_clips[:, :, 1])  # Right channel\n",
    "\n",
    "mel_spec_combined = torch.cat((mel_spec_left, mel_spec_right), dim=-1)  # Shape: [n_mels, time*2]\n",
    "print(f\"Mel-spectrogram combined shape: {mel_spec_combined.shape}\")\n",
    "\n",
    "\n",
    "# Convert to log scale\n",
    "mel_spec_left_db = transforms.AmplitudeToDB()(mel_spec_left)\n",
    "mel_spec_right_db = transforms.AmplitudeToDB()(mel_spec_right)\n",
    "\n",
    "\n",
    "# Concatenate the spectrograms side by side (along the time axis)\n",
    "mel_spec_combined_db = torch.cat((mel_spec_left_db, mel_spec_right_db), dim=-1)  # Shape: [n_mels, time*2]\n",
    "print(f\"Mel-spectrogram combined shape: {mel_spec_combined_db.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example batch of audio clips (batch, samples, channels)\n",
    "audio_clips = batch['audio']  # Assume shape [batch, samples, channels]\n",
    "\n",
    "# Parameters for feature extraction\n",
    "sample_rate = 16000  # Adjust based on your dataset\n",
    "n_mels = 64  # Number of Mel filter banks\n",
    "hop_length = 512\n",
    "n_fft = 1024\n",
    "\n",
    "# MelSpectrogram transform\n",
    "mel_spectrogram = transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mels=n_mels,\n",
    "    hop_length=hop_length,\n",
    "    n_fft=n_fft\n",
    ")\n",
    "\n",
    "# Extract Mel-spectrogram for a single channel (first sample, first channel as an example)\n",
    "mel_spec = mel_spectrogram(audio_clips[0, :, 0])  # Shape: [n_mels, time]\n",
    "\n",
    "# Convert to log scale for better visualization\n",
    "mel_spec_db = transforms.AmplitudeToDB()(mel_spec)\n",
    "\n",
    "# Plotting the Mel-spectrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(mel_spec_db.numpy(), cmap='viridis', aspect='auto', origin='lower')\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Mel-Spectrogram (Log-Scale)\")\n",
    "plt.xlabel(\"Time (frames)\")\n",
    "plt.ylabel(\"Mel Frequency Bins\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert frames tensor to video\n",
    "frames = batch['frames'][0]\n",
    "print(frames.shape) # (num_frames, 3, 224, 224)\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Example tensor (num_frames, 3, 224, 224)\n",
    "frames = batch['frames'][0]\n",
    "\n",
    "# Convert from (num_frames, 3, 224, 224) to (num_frames, 224, 224, 3)\n",
    "frames = frames.permute(0, 2, 3, 1).cpu().numpy()  # (num_frames, 224, 224, 3)\n",
    "\n",
    "# Normalize pixel values from [0, 1] or [-1, 1] if necessary\n",
    "# frames = (frames * 255).astype(np.uint8)\n",
    "\n",
    "# convert bgr to rgb\n",
    "# Define the codec and create a VideoWriter object\n",
    "output_file = './visualizations/video.mp4'\n",
    "fps = 30  # Frames per second\n",
    "height, width = frames.shape[1:3]\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "\n",
    "video_writer = cv2.VideoWriter(output_file, fourcc, fps, (width, height))\n",
    "\n",
    "# Write each frame to the video\n",
    "for frame in frames:\n",
    "    video_writer.write(frame)\n",
    "\n",
    "# Release the video writer\n",
    "video_writer.release()\n",
    "\n",
    "print(f\"Video saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_rgb_flow_imu_depth(rgb_frames, flow_frames, rgb_feature, imu_data, depth_data):\n",
    "    # Assume rgb_frames is of shape (frames, height, width, 3) and flow_frames is (frames, 1024)\n",
    "    # imu_data is of shape (frames, 3) and depth_data is (frames, 1)\n",
    "    print(rgb_frames.shape, flow_frames.shape, rgb_feature.shape, imu_data.shape, depth_data.shape)\n",
    "    num_frames = rgb_frames.shape[0]\n",
    "\n",
    "    fig, axes = plt.subplots( 5, num_frames, figsize=(num_frames * 5, 25))\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        rgb_image = rgb_frames[i].permute(1, 2, 0).cpu().numpy()\n",
    "        \n",
    "        # Plot RGB frame\n",
    "        axes[0, i].imshow(rgb_image)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(f\"RGB Frame {i+1}\")\n",
    "        \n",
    "        # Plot flow data as a heatmap for the corresponding frame\n",
    "        axes[1, i].imshow(flow_frames[i].reshape(32, 32), cmap='viridis', aspect='auto')  # assuming 1024 is reshaped to 32x32\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(f\"Flow Feature {i+1}\")\n",
    "        \n",
    "        # Plot the RGB feature as a heatmap\n",
    "        rgb_feature_image = rgb_feature[i].cpu().numpy().reshape(32, 32)  # Reshape to 32x32\n",
    "        axes[2, i].imshow(rgb_feature_image, cmap='plasma', aspect='auto')\n",
    "        axes[2, i].axis('off')\n",
    "        axes[2, i].set_title(f\"RGB Feature {i+1}\")\n",
    "        \n",
    "        # Plot smartwatch IMU data (3-axis)\n",
    "        # Plot smartwatch IMU data (3-axis: x, y, z)\n",
    "        imu_time_series = imu_data[i].cpu().numpy()  # Shape (3,)\n",
    "        axes[3, i].plot([0, 1, 2], imu_time_series, marker='o', label=['X', 'Y', 'Z'])\n",
    "        axes[3, i].set_xticks([0, 1, 2])\n",
    "        axes[3, i].set_xticklabels(['X', 'Y', 'Z'])\n",
    "        axes[3, i].set_title(f\"IMU Data {i+1}\")\n",
    "        \n",
    "        # Plot depth sensor data\n",
    "        depth_value = depth_data[i].cpu().numpy()\n",
    "        axes[4, i].bar(0, depth_value, width=0.5)\n",
    "        axes[4, i].set_ylim(0, np.max(depth_data.cpu().numpy()))  # Adjust y-axis based on max depth value\n",
    "        axes[4, i].set_title(f\"Depth Sensor {i+1}\")\n",
    "        axes[4, i].set_xticks([])  # No x-ticks since it's a single bar\n",
    "\n",
    "        # Save each figure\n",
    "        plt.savefig(f\"./visualizations/frame_{i}.png\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with your batch data\n",
    "# batch = next(iter(data_loader))\n",
    "print(batch['frames'].shape, batch['audio'].shape, batch['flow'].shape, batch['rgb'].shape, batch['smartwatch_imu'].shape, batch['depth_sensor'].shape , batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "# torch.Size([1, 82, 3, 224, 224]) torch.Size([1, 133120, 2]) torch.Size([1, 1, 1024]) torch.Size([1, 1, 1024]) torch.Size([1, 82, 3]) torch.Size([1, 82, 1]) ['place_bvm'] tensor([13]) tensor([2416]) tensor([2498]) tensor([80.6273]) tensor([83.3773]) ['ng8'] ['7']\n",
    "\n",
    "rgb_frames= batch['frames'][0]\n",
    "flow_frames = batch['flow'][0]\n",
    "rgb_feature = batch['rgb'][0]\n",
    "imu_data = batch['smartwatch_imu'][0]\n",
    "depth_data = batch['depth_sensor'][0]\n",
    "\n",
    "plot_index = min(rgb_frames.shape[0], flow_frames.shape[0], rgb_feature.shape[0], imu_data.shape[0], depth_data.shape[0])\n",
    "\n",
    "if(plot_index > 1):\n",
    "    plot_index = 15\n",
    "    plot_rgb_flow_imu_depth(rgb_frames[0:plot_index], flow_frames[0:plot_index], rgb_feature[0:plot_index],imu_data[0:plot_index], depth_data[0:plot_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def animate_rgb_flow_imu_depth(rgb_frames, flow_frames, additional_rgb, imu_data, depth_data):\n",
    "    num_frames = rgb_frames.shape[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 1, figsize=(6, 15))  # 5 rows for RGB, Flow, Additional RGB, IMU, and Depth\n",
    "\n",
    "    # Initialize the images that will be updated\n",
    "    rgb_im = axes[0].imshow(np.zeros((rgb_frames.shape[2], rgb_frames.shape[3], 3)))\n",
    "    flow_im = axes[1].imshow(np.zeros((32, 32)), cmap='viridis')\n",
    "    add_rgb_im = axes[2].imshow(np.zeros((32, 32)), cmap='plasma')\n",
    "    \n",
    "    # For IMU and Depth, we initialize placeholders\n",
    "    imu_plot_x, = axes[3].plot([], [], 'r-', label='X')\n",
    "    imu_plot_y, = axes[3].plot([], [], 'g-', label='Y')\n",
    "    imu_plot_z, = axes[3].plot([], [], 'b-', label='Z')\n",
    "    depth_plot = axes[4].bar([0], [0], width=0.5)  # Single bar for depth value\n",
    "\n",
    "    axes[0].set_title('RGB Frame')\n",
    "    axes[1].set_title('Flow Feature')\n",
    "    axes[2].set_title('Additional RGB Feature')\n",
    "    axes[3].set_title('IMU Data (X, Y, Z)')\n",
    "    axes[4].set_title('Depth Sensor Data')\n",
    "    \n",
    "        # Set a fixed Y-axis range for IMU data\n",
    "    imu_y_range = (-10, 10)  # Adjust this range based on your IMU data values\n",
    "    axes[3].set_ylim(imu_y_range)  # Set a fixed range for the IMU Y-axis\n",
    "\n",
    "\n",
    "    # Turn off axis for cleaner visuals\n",
    "    for ax in axes:\n",
    "        ax.axis('off') if ax != axes[3] else ax.set_xticks([])  # Don't turn off axis for IMU\n",
    "\n",
    "    # IMU axis should show the three axes, so enable labels for this plot\n",
    "    axes[3].legend(loc='upper right')\n",
    "\n",
    "    def update(frame):\n",
    "        # Update RGB data\n",
    "        rgb_image = rgb_frames[frame].permute(1, 2, 0).cpu().numpy()\n",
    "        rgb_im.set_data(rgb_image)\n",
    "        \n",
    "        # Update flow data\n",
    "        flow_image = flow_frames[frame].cpu().numpy().reshape(32, 32)\n",
    "        flow_im.set_data(flow_image)\n",
    "        flow_im.set_clim(vmin=np.min(flow_image), vmax=np.max(flow_image))  # Set dynamic color range\n",
    "        \n",
    "        # Update additional RGB data\n",
    "        additional_rgb_image = additional_rgb[frame].cpu().numpy().reshape(32, 32)\n",
    "        add_rgb_im.set_data(additional_rgb_image)\n",
    "        add_rgb_im.set_clim(vmin=np.min(additional_rgb_image), vmax=np.max(additional_rgb_image))  # Set dynamic color range\n",
    "        \n",
    "        # Update IMU data\n",
    "        imu_values = imu_data[frame].cpu().numpy()  # shape (3,)\n",
    "        imu_plot_x.set_data([0, 1], [0, imu_values[0]])  # X-axis value\n",
    "        imu_plot_y.set_data([0, 1], [0, imu_values[1]])  # Y-axis value\n",
    "        imu_plot_z.set_data([0, 1], [0, imu_values[2]])  # Z-axis value\n",
    "\n",
    "        # Update depth sensor data\n",
    "        depth_value = depth_data[frame].cpu().numpy().item()  # Assuming depth_data has shape (num_frames, 1)\n",
    "        depth_plot[0].set_height(depth_value)\n",
    "        depth_plot[0].set_y(depth_value)\n",
    "\n",
    "        return [rgb_im, flow_im, add_rgb_im, imu_plot_x, imu_plot_y, imu_plot_z] + list(depth_plot)\n",
    "\n",
    "    # Create animation\n",
    "    ani = animation.FuncAnimation(fig, update, frames=num_frames, interval=200, blit=True)\n",
    "    \n",
    "    # Save as GIF using Pillow backend\n",
    "    ani.save('./visualizations/animated_video_with_imu_depth.gif', writer='pillow', fps=30)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_index = min(rgb_frames.shape[0], flow_frames.shape[0], rgb_feature.shape[0], imu_data.shape[0], depth_data.shape[0])\n",
    "\n",
    "# Example usage with your batch data\n",
    "animate_rgb_flow_imu_depth(rgb_frames[0:plot_index], flow_frames[0:plot_index], rgb_feature[0:plot_index], imu_data[0:plot_index], depth_data[0:plot_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tumbling window frame wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import math\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from torchvision import transforms\n",
    "# from collections import OrderedDict\n",
    "# import itertools\n",
    "# from torchvision.io import VideoReader\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "# ])\n",
    "\n",
    "# def collate_fn(batch, frames_per_clip=30):\n",
    "#     # Initialize empty lists for all possible modalities\n",
    "#     padded_audio_clips = []\n",
    "#     padded_flow_clips = []\n",
    "#     padded_rgb_clips = []\n",
    "#     padded_resnet_clips = []\n",
    "#     padded_resnet_exo_clips = []  # Added for resnet_exo modality\n",
    "#     padded_smartwatch_clips = []\n",
    "#     padded_depth_sensor_clips = []\n",
    "#     keystep_labels = []\n",
    "#     keystep_ids = []\n",
    "#     start_frames = []\n",
    "#     end_frames = []\n",
    "#     start_ts = []\n",
    "#     end_ts = []\n",
    "#     subject_ids = []\n",
    "#     trial_ids = []\n",
    "\n",
    "#     # Initialize max lengths for each modality, check if each is available\n",
    "#     max_flow_len = max([clip['flow'].shape[0] for clip in batch if isinstance(clip.get('flow', None), torch.Tensor)], default=0)\n",
    "#     max_audio_len = max([clip['audio'].shape[-1] for clip in batch if isinstance(clip.get('audio', None), torch.Tensor)], default=0)\n",
    "#     max_rgb_len = max([clip['rgb'].shape[0] for clip in batch if isinstance(clip.get('rgb', None), torch.Tensor)], default=0)\n",
    "#     max_resnet_len = max([clip['resnet'].shape[0] for clip in batch if isinstance(clip.get('resnet', None), torch.Tensor)], default=0)\n",
    "#     max_resnet_exo_len = max([clip['resnet_exo'].shape[0] for clip in batch if isinstance(clip.get('resnet_exo', None), torch.Tensor)], default=0)  # Added for resnet_exo modality\n",
    "#     max_smartwatch_len = max([clip['smartwatch'].shape[0] for clip in batch if isinstance(clip.get('smartwatch', None), torch.Tensor)], default=0)\n",
    "#     max_depth_sensor_len = max([clip['depth_sensor'].shape[0] for clip in batch if isinstance(clip.get('depth_sensor', None), torch.Tensor)], default=0)\n",
    "\n",
    "#     # print(\"max_flow_len:\",max_flow_len)\n",
    "#     # print(\"max_audio_len:\",max_audio_len)\n",
    "#     # print(\"max_rgb_len:\",max_rgb_len)\n",
    "#     # print(\"max_resnet_len:\",max_resnet_len)\n",
    "#     # print(\"max_resnet_exo_len:\",max_resnet_exo_len)\n",
    "#     # print(\"max_smartwatch_len:\",max_smartwatch_len)\n",
    "#     # print(\"max_depth_sensor_len:\",max_depth_sensor_len)\n",
    "    \n",
    "#     pad_length = 0\n",
    "#     for b in batch:\n",
    "#         # print(\"b:\",b)\n",
    "#         # Pad audio if available\n",
    "#         if 'audio' in b and isinstance(b['audio'], torch.Tensor):\n",
    "#             audio_clip = b['audio']\n",
    "#             audio_pad_size = max_audio_len - audio_clip.shape[0]\n",
    "#             if audio_pad_size > 0:\n",
    "#                 audio_pad = torch.zeros((audio_pad_size, *audio_clip.shape[1:]))\n",
    "#                 audio_clip = torch.cat([audio_clip, audio_pad], dim=0)\n",
    "#             padded_audio_clips.append(audio_clip)\n",
    "\n",
    "\n",
    "#         # Pad flow data if available\n",
    "#         if 'flow' in b and isinstance(b['flow'], torch.Tensor):\n",
    "#             flow_clip = b['flow']\n",
    "#             flow_pad_size = max_flow_len - flow_clip.shape[0]\n",
    "#             if flow_pad_size > 0:\n",
    "#                 flow_pad = torch.zeros((flow_pad_size, *flow_clip.shape[1:]))\n",
    "#                 flow_clip = torch.cat([flow_clip, flow_pad], dim=0)\n",
    "#             padded_flow_clips.append(flow_clip)\n",
    "\n",
    "#         # Pad rgb data if available\n",
    "#         if 'rgb' in b and isinstance(b['rgb'], torch.Tensor):\n",
    "#             rgb_clip = b['rgb']\n",
    "#             rgb_pad_size = max_rgb_len - rgb_clip.shape[0]\n",
    "#             if rgb_pad_size > 0:\n",
    "#                 rgb_pad = torch.zeros((rgb_pad_size, *rgb_clip.shape[1:]))\n",
    "#                 rgb_clip = torch.cat([rgb_clip, rgb_pad], dim=0)\n",
    "#             padded_rgb_clips.append(rgb_clip)\n",
    "\n",
    "#         # Pad resnet data if available\n",
    "#         if 'resnet' in b and isinstance(b['resnet'], torch.Tensor):\n",
    "#             resnet_clip = b['resnet']\n",
    "#             resnet_pad_size = max_resnet_len - resnet_clip.shape[0]\n",
    "#             if resnet_pad_size > 0:\n",
    "#                 resnet_pad = torch.zeros((resnet_pad_size, *resnet_clip.shape[1:]))\n",
    "#                 resnet_clip = torch.cat([resnet_clip, resnet_pad], dim=0)\n",
    "#             padded_resnet_clips.append(resnet_clip)\n",
    "\n",
    "#         # Pad resnet_exo data if available\n",
    "#         if 'resnet_exo' in b and isinstance(b['resnet_exo'], torch.Tensor):\n",
    "#             resnet_exo_clip = b['resnet_exo']\n",
    "#             resnet_exo_pad_size = max_resnet_exo_len - resnet_exo_clip.shape[0]\n",
    "#             if resnet_exo_pad_size > 0:\n",
    "#                 resnet_exo_pad = torch.zeros((resnet_exo_pad_size, *resnet_exo_clip.shape[1:]))\n",
    "#                 resnet_exo_clip = torch.cat([resnet_exo_clip, resnet_exo_pad], dim=0)\n",
    "#             padded_resnet_exo_clips.append(resnet_exo_clip)\n",
    "\n",
    "#         # Pad smartwatch data if available\n",
    "#         if 'smartwatch' in b and isinstance(b['smartwatch'], torch.Tensor):\n",
    "#             smartwatch_clip = b['smartwatch']\n",
    "#             if max_smartwatch_len > 0:\n",
    "#                 smartwatch_pad_size = max_smartwatch_len - smartwatch_clip.shape[0]\n",
    "#                 if smartwatch_pad_size > 0:\n",
    "#                     smartwatch_pad = torch.zeros((smartwatch_clip.shape[0], smartwatch_pad_size))\n",
    "#                     smartwatch_clip = torch.cat([smartwatch_clip, smartwatch_pad], dim=0)\n",
    "#             padded_smartwatch_clips.append(smartwatch_clip)\n",
    "\n",
    "#         # Pad depth_sensor data if available\n",
    "#         if 'depth_sensor' in b and isinstance(b['depth_sensor'], torch.Tensor):\n",
    "#             depth_sensor_clip = b['depth_sensor']\n",
    "#             if max_depth_sensor_len > 0:\n",
    "#                 depth_sensor_pad_size = max_depth_sensor_len - depth_sensor_clip.shape[0]\n",
    "#                 if depth_sensor_pad_size > 0:\n",
    "#                     depth_sensor_pad = torch.zeros((depth_sensor_clip.shape[0], depth_sensor_pad_size))\n",
    "#                     depth_sensor_clip = torch.cat([depth_sensor_clip, depth_sensor_pad], dim=0)\n",
    "#             padded_depth_sensor_clips.append(depth_sensor_clip)\n",
    "\n",
    "\n",
    "#         pad_length = frames_per_clip - len(b['keystep_id']) \n",
    "        \n",
    "#         if pad_length > 0:\n",
    "#             # repeat last element of keystep_id and keystep_label\n",
    "#             b['keystep_id'] = b['keystep_id'] + [b['keystep_id'][-1]]*pad_length\n",
    "#             b['start_frame'] = b['start_frame'] + [b['start_frame'][-1]]*pad_length\n",
    "#             b['end_frame'] = b['end_frame'] + [b['end_frame'][-1]]*pad_length\n",
    "#             b['start_t'] = b['start_t'] + [b['start_t'][-1]]*pad_length\n",
    "#             b['end_t'] = b['end_t'] + [b['end_t'][-1]]*pad_length\n",
    "        \n",
    "#         # Collect other fields\n",
    "#         keystep_labels.append(b['keystep_label'])\n",
    "#         keystep_ids.append(b['keystep_id'])\n",
    "#         start_frames.append(b['start_frame'])\n",
    "#         end_frames.append(b['end_frame'])\n",
    "#         start_ts.append(b['start_t'])\n",
    "#         end_ts.append(b['end_t'])\n",
    "#         subject_ids.append(b['subject_id'])\n",
    "#         trial_ids.append(b['trial_id'])\n",
    "\n",
    "#     output = {\n",
    "#         'keystep_label': keystep_labels,\n",
    "#         'keystep_id': torch.tensor(keystep_ids),\n",
    "#         'start_frame': torch.tensor(start_frames),\n",
    "#         'end_frame': torch.tensor(end_frames),\n",
    "#         'start_t': torch.tensor(start_ts),\n",
    "#         'end_t': torch.tensor(end_ts),\n",
    "#         'subject_id': subject_ids,\n",
    "#         'trial_id': trial_ids\n",
    "#     }\n",
    "\n",
    "#     # Only include modality data if it exists\n",
    "#     output['audio'] = torch.stack(padded_audio_clips) if padded_audio_clips else torch.zeros(0)\n",
    "#     output['flow'] = torch.stack(padded_flow_clips) if padded_flow_clips else torch.zeros(0)\n",
    "#     output['rgb'] = torch.stack(padded_rgb_clips) if padded_rgb_clips else torch.zeros(0)\n",
    "#     output['resnet'] = torch.stack(padded_resnet_clips) if padded_resnet_clips else torch.zeros(0)\n",
    "#     output['resnet_exo'] = torch.stack(padded_resnet_exo_clips) if padded_resnet_exo_clips else torch.zeros(0)  # Added for resnet_exo\n",
    "#     output['smartwatch'] = torch.stack(padded_smartwatch_clips) if padded_smartwatch_clips else torch.zeros(0)\n",
    "#     output['depth_sensor'] = torch.stack(padded_depth_sensor_clips) if padded_depth_sensor_clips else torch.zeros(0)\n",
    "\n",
    "#     return output\n",
    "\n",
    "\n",
    "# class WindowEgoExoEMSDataset(Dataset):\n",
    "#     def __init__(self, annotation_file, data_base_path, fps, \n",
    "#                 frames_per_clip=30, transform=None,\n",
    "#                 data_types=['resnet']):\n",
    "        \n",
    "#         self.annotation_file = annotation_file\n",
    "#         self.data_base_path = data_base_path\n",
    "#         self.fps = fps\n",
    "#         self.frames_per_clip = frames_per_clip  # Store frames_per_clip\n",
    "#         self.transform = transform\n",
    "#         self.data = []\n",
    "#         self.clip_indices = []  # This will store (item_idx, clip_idx) tuples\n",
    "#         self.data_types = data_types\n",
    "        \n",
    "#         self.data_dict = None\n",
    "#         self._load_annotations()\n",
    "#         self._split_windows()\n",
    "\n",
    "#     def _load_annotations(self):\n",
    "#         with open(self.annotation_file, 'r') as f:\n",
    "#             annotations = json.load(f)\n",
    "        \n",
    "#         subject_dict = {}\n",
    "#         for subject in annotations['subjects']:\n",
    "#             trial_dict ={}\n",
    "#             for trial in subject['trials']:\n",
    "#                 avail_streams = trial['streams']\n",
    "                \n",
    "#                 # Initialize paths to None by default\n",
    "#                 audio_path = None\n",
    "#                 flow_path = None\n",
    "#                 rgb_path = None\n",
    "#                 resnet_path = None\n",
    "#                 resnet_exo_path = None  # Added for resnet_exo modality\n",
    "#                 smartwatch_path = None\n",
    "#                 depth_sensor_path = None\n",
    "\n",
    "#                 # Check for each data type and retrieve the corresponding file path\n",
    "#                 if 'audio' in self.data_types:\n",
    "#                     audio_path = avail_streams.get('egocam_rgb_audio', {}).get('file_path', None)\n",
    "#                 if 'flow' in self.data_types:\n",
    "#                     flow_path = avail_streams.get('i3d_flow', {}).get('file_path', None)\n",
    "#                 if 'rgb' in self.data_types:\n",
    "#                     rgb_path = avail_streams.get('i3d_rgb', {}).get('file_path', None)\n",
    "#                 if 'resnet' in self.data_types:\n",
    "#                     resnet_path = avail_streams.get('resnet50', {}).get('file_path', None)\n",
    "#                 if 'resnet_exo' in self.data_types:\n",
    "#                     resnet_exo_path = avail_streams.get('resnet50-exo', {}).get('file_path', None)  # Adjust key as needed\n",
    "#                 if 'smartwatch' in self.data_types:\n",
    "#                     smartwatch_path = avail_streams.get('smartwatch_imu', {}).get('file_path', None)\n",
    "#                 if 'depth_sensor' in self.data_types:\n",
    "#                     depth_sensor_path = avail_streams.get('vl6180_ToF_depth', {}).get('file_path', None)\n",
    "\n",
    "#                 # Skip the trial if any required data type is not available\n",
    "#                 if ('flow' in self.data_types and not flow_path) or \\\n",
    "#                 ('audio' in self.data_types and not audio_path) or \\\n",
    "#                 ('rgb' in self.data_types and not rgb_path) or \\\n",
    "#                 ('resnet' in self.data_types and not resnet_path) or \\\n",
    "#                 ('resnet_exo' in self.data_types and not resnet_exo_path) or \\\n",
    "#                 ('smartwatch' in self.data_types and not smartwatch_path) or \\\n",
    "#                 ('depth_sensor' in self.data_types and not depth_sensor_path):\n",
    "#                     print(f\"[Warning] Skipping trial {trial['trial_id']} for subject {subject['subject_id']} due to missing data\")\n",
    "#                     continue\n",
    "                \n",
    "#                 if audio_path or flow_path or rgb_path or resnet_path or resnet_exo_path or smartwatch_path or depth_sensor_path:\n",
    "#                     keysteps = trial['keysteps']\n",
    "#                     keysteps_dict = []\n",
    "#                     for step in keysteps:\n",
    "#                         start_frame = math.floor(step['start_t'] * self.fps)\n",
    "#                         end_frame = math.floor(step['end_t'] * self.fps)\n",
    "#                         label = step['label']\n",
    "#                         keystep_id = step['class_id']\n",
    "\n",
    "#                         data_dict = {}\n",
    "#                         if 'audio' in self.data_types:\n",
    "#                             data_dict['audio_path'] = os.path.join(self.data_base_path, audio_path)\n",
    "#                         if 'flow' in self.data_types:\n",
    "#                             data_dict['flow_path'] = os.path.join(self.data_base_path, flow_path)\n",
    "#                         if 'rgb' in self.data_types:\n",
    "#                             data_dict['rgb_path'] = os.path.join(self.data_base_path, rgb_path)\n",
    "#                         if 'resnet' in self.data_types:\n",
    "#                             data_dict['resnet_path'] = os.path.join(self.data_base_path, resnet_path)\n",
    "#                         if 'resnet_exo' in self.data_types:\n",
    "#                             data_dict['resnet_exo_path'] = os.path.join(self.data_base_path, resnet_exo_path)\n",
    "#                         if 'smartwatch' in self.data_types:\n",
    "#                             data_dict['smartwatch_path'] = os.path.join(self.data_base_path, smartwatch_path)\n",
    "#                         if 'depth_sensor' in self.data_types:\n",
    "#                             data_dict['depth_sensor_path'] = os.path.join(self.data_base_path, depth_sensor_path)\n",
    "#                         data_dict['start_frame'] = start_frame\n",
    "#                         data_dict['end_frame'] = end_frame\n",
    "#                         data_dict['start_t'] = step['start_t']\n",
    "#                         data_dict['end_t'] = step['end_t']\n",
    "#                         data_dict['keystep_label'] = label\n",
    "#                         data_dict['keystep_id'] = keystep_id\n",
    "#                         data_dict['subject'] = subject['subject_id']\n",
    "#                         data_dict['trial'] = trial['trial_id']\n",
    "\n",
    "#                         keysteps_dict.append(data_dict)\n",
    "#                         self.data.append(data_dict)\n",
    "\n",
    "#                         trial_dict[trial['trial_id']] = keysteps_dict\n",
    "#                 subject_dict[subject['subject_id']] = trial_dict\n",
    "\n",
    "#         self.data_dict = subject_dict\n",
    "\n",
    "#     def _split_windows(self):\n",
    "#         print(\"Splitting data to windows\")\n",
    "#         windowed_clips = []\n",
    "#         current_window = []  # Initialize an empty current window\n",
    "#         accumulated_frames = 0  # Track how many frames have been accumulated in the current window\n",
    "\n",
    "#         current_subject = None\n",
    "#         current_trial = None\n",
    "\n",
    "\n",
    "#         for i, item in enumerate(self.data):\n",
    "#             start_frame = item['start_frame']\n",
    "#             end_frame = item['end_frame']\n",
    "#             keystep_id = item['keystep_id']\n",
    "#             keystep_label = item['keystep_label']\n",
    "#             subject_id = item['subject']\n",
    "#             trial_id = item['trial']\n",
    "#             num_frames_total = end_frame - start_frame\n",
    "\n",
    "                \n",
    "#         # If the subject or trial changes, store the current window and reset it\n",
    "#             if subject_id != current_subject or trial_id != current_trial:\n",
    "#                 if len(current_window) > 0:\n",
    "#                     windowed_clips.append(current_window)\n",
    "#                 current_window = []  # Reset the current window\n",
    "#                 accumulated_frames = 0  # Reset the frame counter\n",
    "#                 current_subject = subject_id  # Update current subject\n",
    "#                 current_trial = trial_id  # Update current trial\n",
    "\n",
    "#             # Loop through frames from the start to the end of the current keystep\n",
    "#             for j in range(start_frame, end_frame):\n",
    "#                 frame_data = {}\n",
    "#                 frame_data['frame'] = j\n",
    "\n",
    "#                 # Copy the relevant data for the current frame\n",
    "#                 for key, value in item.items():\n",
    "#                     if isinstance(value, (int, float, str)):\n",
    "#                         frame_data[key] = value\n",
    "#                     elif isinstance(value, (list, np.ndarray)):\n",
    "#                         if len(value) == num_frames_total:\n",
    "#                             frame_data[key] = value[j - start_frame]\n",
    "#                     else:\n",
    "#                         frame_data[key] = value\n",
    "\n",
    "#                 # Append the current frame data to the window\n",
    "#                 current_window.append(frame_data)\n",
    "#                 accumulated_frames += 1\n",
    "\n",
    "\n",
    "#                 # Once we reach the window size, store the window and reset\n",
    "#                 if accumulated_frames == self.frames_per_clip:\n",
    "#                     windowed_clips.append(current_window)\n",
    "#                     current_window = []  # Reset the current window\n",
    "#                     accumulated_frames = 0  # Reset the frame counter\n",
    "\n",
    "#         # # dump the data to a file\n",
    "#         # with open('data.json', 'w') as f:\n",
    "#         #     json.dump(windowed_clips, f)\n",
    "            \n",
    "#         self.data = windowed_clips\n",
    "#         print(f\"Total windowed clips: {len(windowed_clips)}\")\n",
    "\n",
    "            \n",
    "\n",
    "#     def __len__(self):\n",
    "#         # The length should now be based on the number of clips, not items\n",
    "#         # total_clips = len(self.data) // self.frames_per_clip if self.frames_per_clip else len(self.data)\n",
    "#         # dump the data to a file\n",
    "#         # with open('data.json', 'w') as f:\n",
    "#         #     json.dump(self.data, f)\n",
    "#         total_clips = len(self.data)\n",
    "#         return total_clips\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         window = self.data[idx]\n",
    "#         # print(\"window\", window)\n",
    "\n",
    "#         first_frame_of_clip = window[0]['frame']\n",
    "#         last_frame_of_clip = window[-1]['frame']+1\n",
    "        \n",
    "#         # print(f\"getting clip {idx} with frame {first_frame_of_clip} to {last_frame_of_clip} of length ({last_frame_of_clip - first_frame_of_clip})\")\n",
    "\n",
    "#         # Initialize dictionaries to hold accumulated frames for each modality\n",
    "#         batch_audio = []\n",
    "#         batch_flow = []\n",
    "#         batch_rgb = []\n",
    "#         batch_resnet = []\n",
    "#         batch_resnet_exo = []\n",
    "#         batch_sw_acc = []\n",
    "#         batch_depth_sensor = []\n",
    "\n",
    "#         # Initialize lists for metadata\n",
    "#         keystep_labels = []\n",
    "#         keystep_ids = []\n",
    "#         start_frames = []\n",
    "#         end_frames = []\n",
    "#         start_ts = []\n",
    "#         end_ts = []\n",
    "#         subject_ids = []\n",
    "#         trial_ids = []\n",
    "\n",
    "\n",
    "#         # Initialize variables\n",
    "#         flow = torch.zeros(0)\n",
    "#         rgb = torch.zeros(0)\n",
    "#         resnet = torch.zeros(0)\n",
    "#         resnet_exo = torch.zeros(0)  # Added for resnet_exo modality\n",
    "#         sw_acc = torch.zeros(0)\n",
    "#         depth_sensor_readings = torch.zeros(0)\n",
    "\n",
    "#         # Load flow if available\n",
    "#         if 'audio' in self.data_types:\n",
    "#             audio_path = window[0]['audio_path']\n",
    "#             clip_start_t = first_frame_of_clip / self.fps\n",
    "#             clip_end_t = last_frame_of_clip / self.fps\n",
    "\n",
    "#             # print(f\"loading audio from {clip_start_t} to {clip_end_t} from file {audio_path}\")\n",
    "\n",
    "#             audio_reader = VideoReader(audio_path, \"audio\")\n",
    "#             audio_clips = []\n",
    "#             for audio_frame in itertools.takewhile(lambda x: x['pts'] <= clip_end_t, audio_reader.seek(clip_start_t)):\n",
    "#                 audio_clips.append(audio_frame['data'])\n",
    "#             audio_clips = torch.cat(audio_clips, dim=0) if audio_clips else torch.zeros(1, 0)\n",
    "\n",
    "#             # pad the flow tensor to the i\n",
    "#             batch_audio.append(audio_clips)\n",
    "\n",
    "\n",
    "#         if 'flow' in self.data_types:\n",
    "#             flow_path = window[0]['flow_path']\n",
    "#             flow_npy = np.load(flow_path)\n",
    "#             flow_length = len(flow_npy)\n",
    "#             flow = torch.from_numpy(np.load(flow_path))[first_frame_of_clip:last_frame_of_clip]\n",
    "#                 # pad the flow tensor to the i\n",
    "#             batch_flow.append(flow)\n",
    "\n",
    "#         # Load rgb if available\n",
    "#         if 'rgb' in self.data_types:\n",
    "#             rgb_path =  window[0]['rgb_path']\n",
    "#             rgb_npy = np.load(rgb_path)\n",
    "#             rgb_length = len(rgb_npy)\n",
    "#             rgb = torch.from_numpy(np.load(rgb_path))[first_frame_of_clip:last_frame_of_clip]\n",
    "#                 # pad the rgb tensor to the i\n",
    "#             batch_rgb.append(rgb)\n",
    "\n",
    "#         # Load resnet if available\n",
    "#         if 'resnet' in self.data_types:\n",
    "#             resnet_path =  window[0]['resnet_path']\n",
    "#             resnet_npy = np.load(resnet_path)\n",
    "#             resnet_length = len(resnet_npy)\n",
    "#             resnet = torch.from_numpy(np.load(resnet_path))[first_frame_of_clip:last_frame_of_clip]\n",
    "#             batch_resnet.append(resnet)\n",
    "\n",
    "#         # Load resnet_exo if available\n",
    "#         if 'resnet_exo' in self.data_types:\n",
    "#             resnet_exo_path =  window[0]['resnet_exo_path']\n",
    "#             resnet_exo = torch.from_numpy(np.load(resnet_exo_path))[first_frame_of_clip:last_frame_of_clip]\n",
    "#             batch_resnet_exo.append(resnet_exo)\n",
    "\n",
    "#         # Load smartwatch data if available\n",
    "#         if 'smartwatch' in self.data_types:\n",
    "#             smartwatch_path =  window[0]['smartwatch_path']\n",
    "#             with open(smartwatch_path, 'r') as f:\n",
    "#                 lines = f.readlines()[1:]\n",
    "#             sw_acc = [line.strip() for line in lines][first_frame_of_clip:last_frame_of_clip]\n",
    "#             acc_x = [float(l.split(',')[0]) for l in sw_acc]\n",
    "#             acc_y = [float(l.split(',')[1]) for l in sw_acc]\n",
    "#             acc_z = [float(l.split(',')[2]) for l in sw_acc]\n",
    "#             sw_acc = torch.from_numpy(np.array([acc_x, acc_y, acc_z])).float()\n",
    "#             sw_acc = sw_acc.permute(1, 0)  # (frames, channels)\n",
    "#             batch_sw_acc.append(sw_acc)\n",
    "\n",
    "#         # Load depth_sensor data if available\n",
    "#         if 'depth_sensor' in self.data_types:\n",
    "#             depth_sensor_path =  window[0]['depth_sensor_path']\n",
    "#             with open(depth_sensor_path, 'r') as f:\n",
    "#                 lines = f.readlines()[1:]\n",
    "#             depth_sensor_readings = [line.strip() for line in lines][first_frame_of_clip:last_frame_of_clip]\n",
    "#             depth_reading = [float(l.split(',')[0]) for l in depth_sensor_readings]\n",
    "#             depth_sensor_readings = torch.from_numpy(np.array([depth_reading])).float()\n",
    "#             depth_sensor_readings = depth_sensor_readings.permute(1, 0)\n",
    "#             batch_depth_sensor.append(depth_sensor_readings)\n",
    "\n",
    "#         for frame in window:\n",
    "#             # Accumulate metadata for each frame\n",
    "#             keystep_labels.append( frame['keystep_label'])\n",
    "#             keystep_ids.append( frame['keystep_id'])\n",
    "#             start_frames.append( frame['start_frame'])\n",
    "#             end_frames.append( frame['end_frame'])\n",
    "#             start_ts.append( frame['start_t'])\n",
    "#             end_ts.append( frame['end_t'])\n",
    "#             subject_ids.append( frame['subject'])\n",
    "#             trial_ids.append( frame['trial'])\n",
    "\n",
    "#         # print(\"batch_resnet:\",batch_resnet[0].shape)\n",
    "#         # Stack frames to form a batch of frames_per_clip\n",
    "#         output = {\n",
    "#             'audio': (batch_audio[0]) if batch_audio else torch.zeros(1,0),\n",
    "#             'flow': (batch_flow[0]) if batch_flow else torch.zeros(0),\n",
    "#             'rgb': (batch_rgb[0]) if batch_rgb else torch.zeros(0),\n",
    "#             'resnet': (batch_resnet[0]) if batch_resnet else torch.zeros(0),\n",
    "#             'resnet_exo': (batch_resnet_exo[0]) if batch_resnet_exo else torch.zeros(0),\n",
    "#             'smartwatch': (batch_sw_acc[0]) if batch_sw_acc else torch.zeros(0),\n",
    "#             'depth_sensor': (batch_depth_sensor[0]) if batch_depth_sensor else torch.zeros(0),\n",
    "\n",
    "#             # Metadata (individual per frame)\n",
    "#             'keystep_label': keystep_labels,\n",
    "#             'keystep_id': keystep_ids,\n",
    "#             'start_frame': start_frames,\n",
    "#             'end_frame': end_frames,\n",
    "#             'start_t': start_ts,\n",
    "#             'end_t': end_ts,\n",
    "#             'subject_id': subject_ids,\n",
    "#             'trial_id': trial_ids\n",
    "#         }\n",
    "\n",
    "#         return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from EgoExoEMS.EgoExoEMS import *\n",
    "\n",
    "annotation_file = \"../../Annotations/splits/trials/train_split_segmentation.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "# train_annotation_file = \"../../Annotations/splits/keysteps/train_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "# val_annotation_file = \"../../Annotations/splits/keysteps/val_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "# test_annotation_file = \"../../Annotations/splits/keysteps/test_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "annotation_file = \"../../Annotations/main_annotation_segmentation.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "\n",
    "train_dataset = WindowEgoExoEMSDataset(annotation_file=annotation_file,\n",
    "                                data_base_path='',\n",
    "                                fps=30, frames_per_clip=120, transform=transform, data_types=[ 'resnet'])\n",
    "\n",
    "\n",
    "\n",
    "# Assuming your dataset has a 'targets' attribute with class labels\n",
    "\n",
    "\n",
    "print(\"Size of dataset:\",len(train_dataset))\n",
    "\n",
    "# Use a partial function or lambda to pass the frames_per_clip argument\n",
    "collate_fn_with_args = partial(window_collate_fn, frames_per_clip=120)\n",
    "\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_with_args)\n",
    "\n",
    "print(\"Size of data loader:\",len(data_loader))\n",
    "\n",
    "# # Access a sample\n",
    "# # batch = next(iter(data_loader))\n",
    "# # print(batch['resnet'].shape, batch['resnet_exo'].shape, batch['flow'].shape, batch['rgb'].shape, batch['smartwatch'].shape, batch['depth_sensor'].shape , batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "\n",
    "for idx,batch in enumerate(data_loader):\n",
    "    print(\"batch\", batch['resnet'].shape, batch['keystep_id'].shape, len(batch['keystep_label'][0]), batch['window_start_frame'], batch['window_end_frame'])\n",
    "    # print(\"batch\", batch['video'].shape, batch['keystep_id'].shape,batch['window_start_frame'], batch['window_end_frame'])\n",
    "    # print(batch['resnet'].shape, batch['audio'].shape, batch['resnet_exo'].shape, batch['flow'].shape, batch['rgb'].shape, batch['smartwatch'].shape, batch['depth_sensor'].shape , batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "    # print(batch['resnet'].shape)\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egoems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
