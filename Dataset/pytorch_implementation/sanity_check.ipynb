{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjh9fw/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/sfs/qumulo/qhome/cjh9fw/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from EgoExoEMSDataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "root = \"/standard/UVA-DSA/NIST EMS Project Data/CognitiveEMS_Datasets/North_Garden/May_2024/May24_updated_structure/ego_camera/ng1/1/\"  # Folder in which all videos lie in a specific structure\n",
    "annotation_file = \"../../Annotations/main_annotation.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "train_annotation_file = \"../../Annotations/splits/train_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "val_annotation_file = \"../../Annotations/splits/val_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "test_annotation_file = \"../../Annotations/splits/test_split.json\"  # A row for each video sample as: (VIDEO_PATH START_FRAME END_FRAME CLASS_ID)\n",
    "\n",
    "train_dataset = EgoExoEMSDataset(annotation_file=train_annotation_file,\n",
    "                                data_base_path='',\n",
    "                                fps=30, frames_per_clip=None, transform=transform)\n",
    "\n",
    "# Access a sample\n",
    "print(len(train_dataset))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data loader\n",
    "# batch size is 1 for simplicity and to ensure only a full clip related to a key step is given without collating.\n",
    "# if batch size is greater than 1, collate_fn will be called to collate the data.\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjh9fw/.local/lib/python3.8/site-packages/torchvision/io/video_reader.py:245: UserWarning: Accurate seek is not implemented for pyav backend\n",
      "  warnings.warn(\"Accurate seek is not implemented for pyav backend\")\n",
      "/home/cjh9fw/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 87, 3, 224, 224]) torch.Size([1, 139264, 2]) torch.Size([1, 87, 1024]) torch.Size([1, 87, 1024]) ['approach_patient'] tensor([1]) tensor([0]) tensor([87]) tensor([0.0080]) tensor([2.8828]) ['ng1'] ['1']\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the data loader and print the shape of the batch\n",
    "for batch in data_loader:\n",
    "    print(batch['frames'].shape, batch['audio'].shape, batch['flow'].shape, batch['rgb'].shape, batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "    # audio_tensor = batch['audio'][0]\n",
    "    # #transpose\n",
    "    # audio_tensor = audio_tensor.transpose(0,1)\n",
    "    # torchaudio.save(\"audio.wav\", audio_tensor,48000)\n",
    "    break   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
