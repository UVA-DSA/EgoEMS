{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../../Annotations/splits/cpr_quality/split_1.json\n",
      "Saved ../../Annotations/splits/cpr_quality/split_2.json\n",
      "Saved ../../Annotations/splits/cpr_quality/split_3.json\n",
      "Saved ../../Annotations/splits/cpr_quality/split_4.json\n",
      "Success: All subjects are covered in the validation sets across all splits.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "\n",
    "task = \"cpr_quality\" # segmentation or classification\n",
    "\n",
    "# Load the original annotation file\n",
    "input_json_path = f'../../Annotations/main_annotation_{task}.json'  # Replace with your input JSON file path\n",
    "output_dir = f'../../Annotations/splits/{task}'  # Output directory for split files\n",
    "\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load the dataset\n",
    "with open(input_json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Define train-test ratio\n",
    "train_ratio = 0.7\n",
    "test_ratio = 1 - train_ratio  # 30%\n",
    "\n",
    "# Shuffle the subjects to randomize order\n",
    "subjects = data['subjects']\n",
    "num_subjects = len(subjects)\n",
    "num_test_subjects = int(test_ratio * num_subjects)\n",
    "\n",
    "# Divide subjects into non-overlapping test sets for each split\n",
    "def create_non_overlapping_splits(subjects, num_test_subjects):\n",
    "    random.shuffle(subjects)\n",
    "    splits = []\n",
    "    \n",
    "    # Calculate number of splits needed to cover all subjects in the test set at least once\n",
    "    num_splits = ceil(num_subjects / num_test_subjects)\n",
    "    \n",
    "    # Generate non-overlapping splits\n",
    "    for i in range(num_splits):\n",
    "        # Select a unique subset of subjects for the validation set\n",
    "        start_index = i * num_test_subjects\n",
    "        end_index = start_index + num_test_subjects\n",
    "        test_subjects = subjects[start_index:end_index]\n",
    "        \n",
    "        # Remaining subjects for training\n",
    "        train_subjects = [subj for subj in subjects if subj not in test_subjects]\n",
    "        \n",
    "        # Prepare split data in the required format\n",
    "        train_split = {\"train\": \",\".join([subj['subject_id'] for subj in train_subjects])}\n",
    "        test_split = {\"validation\": \",\".join([subj['subject_id'] for subj in test_subjects])}\n",
    "        \n",
    "        split_data = {**train_split, **test_split}\n",
    "        splits.append(split_data)\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Generate non-overlapping splits\n",
    "splits = create_non_overlapping_splits(subjects, num_test_subjects)\n",
    "\n",
    "# Save each split as a JSON file\n",
    "for i, split_data in enumerate(splits, start=1):\n",
    "    split_file_path = os.path.join(output_dir, f'split_{i}.json')\n",
    "    with open(split_file_path, 'w') as f:\n",
    "        json.dump(split_data, f, indent=4)\n",
    "    print(f\"Saved {split_file_path}\")\n",
    "\n",
    "\n",
    "# Verification step to ensure all subjects are covered in validation sets across splits\n",
    "all_validation_subjects = set()\n",
    "for split in splits:\n",
    "    validation_subjects = set(split[\"validation\"].split(\",\"))\n",
    "    all_validation_subjects.update(validation_subjects)\n",
    "\n",
    "# Collect all unique subject IDs from the dataset\n",
    "all_subject_ids = {subj['subject_id'] for subj in subjects}\n",
    "\n",
    "# Check if all subjects are covered\n",
    "if all_subject_ids == all_validation_subjects:\n",
    "    print(\"Success: All subjects are covered in the validation sets across all splits.\")\n",
    "else:\n",
    "    missing_subjects = all_subject_ids - all_validation_subjects\n",
    "    print(f\"Error: The following subjects are missing in the validation sets: {missing_subjects}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
