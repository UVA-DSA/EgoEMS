{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy \n",
    "\n",
    "def group_overlapping(keysteps):\n",
    "    \"\"\"\n",
    "    Given a list of keystep dicts (each with 'start_t' and 'end_t'),\n",
    "    group overlapping keysteps into clusters. For each cluster with more than one\n",
    "    keystep, keep the keystep with the longest duration as primary and return the rest as secondary.\n",
    "    Keysteps that do not overlap with any others are returned as primary.\n",
    "    \"\"\"\n",
    "    if len(keysteps) <= 1:\n",
    "        return keysteps, []\n",
    "\n",
    "    # Sort keysteps by start time\n",
    "    sorted_keysteps = sorted(keysteps, key=lambda k: k['start_t'])\n",
    "    clusters = []\n",
    "    current_cluster = [sorted_keysteps[0]]\n",
    "    current_cluster_end = sorted_keysteps[0]['end_t']\n",
    "\n",
    "    for step in sorted_keysteps[1:]:\n",
    "        # Check if current keystep overlaps with current cluster.\n",
    "        # Overlap if the start time is less than current cluster's max end time.\n",
    "        if step['start_t'] < current_cluster_end:\n",
    "            current_cluster.append(step)\n",
    "            current_cluster_end = max(current_cluster_end, step['end_t'])\n",
    "        else:\n",
    "            clusters.append(current_cluster)\n",
    "            current_cluster = [step]\n",
    "            current_cluster_end = step['end_t']\n",
    "    if current_cluster:\n",
    "        clusters.append(current_cluster)\n",
    "\n",
    "    primary = []\n",
    "    secondary = []\n",
    "    for cluster in clusters:\n",
    "        if len(cluster) == 1:\n",
    "            primary.extend(cluster)\n",
    "        else:\n",
    "            # In a cluster of overlapping keysteps, choose the one with maximum duration as primary.\n",
    "            durations = [k['end_t'] - k['start_t'] for k in cluster]\n",
    "            max_index = durations.index(max(durations))\n",
    "            for i, k in enumerate(cluster):\n",
    "                if i == max_index:\n",
    "                    primary.append(k)\n",
    "                else:\n",
    "                    secondary.append(k)\n",
    "    return primary, secondary\n",
    "\n",
    "def process_annotation(input_file, output_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Dictionary to hold new secondary subjects (keyed by new subject id)\n",
    "    secondary_subjects = {}\n",
    "\n",
    "    # Process each subject in the annotation\n",
    "    for subject in data.get(\"subjects\", []):\n",
    "        subject_id = subject[\"subject_id\"]\n",
    "        # Iterate over a copy of trials list so we can modify in place\n",
    "        for trial in subject.get(\"trials\", []):\n",
    "            keysteps = trial.get(\"keysteps\", [])\n",
    "            # Only process trials with more than one keystep (possible overlap)\n",
    "            if len(keysteps) > 1:\n",
    "                primary_keysteps, secondary_keysteps = group_overlapping(keysteps)\n",
    "                # Update the current trial with primary keysteps only\n",
    "                trial[\"keysteps\"] = primary_keysteps\n",
    "\n",
    "                if secondary_keysteps:\n",
    "                    # Create or update the secondary subject for this original subject\n",
    "                    secondary_subject_id = f\"{subject_id}-secondary\"\n",
    "                    if secondary_subject_id not in secondary_subjects:\n",
    "                        secondary_subjects[secondary_subject_id] = {\n",
    "                            \"subject_id\": secondary_subject_id,\n",
    "                            \"trials\": []\n",
    "                        }\n",
    "                    # Make a deep copy of the trial for the secondary subject\n",
    "                    trial_secondary = copy.deepcopy(trial)\n",
    "                    # Replace keysteps with the secondary ones only\n",
    "                    trial_secondary[\"keysteps\"] = secondary_keysteps\n",
    "                    secondary_subjects[secondary_subject_id][\"trials\"].append(trial_secondary)\n",
    "\n",
    "    # Append the secondary subjects (if any) to the subjects list\n",
    "    for sec_sub in secondary_subjects.values():\n",
    "        data[\"subjects\"].append(sec_sub)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"Modified annotation saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "input_filename = \"../../Annotations/main_annotation_classification.json\"\n",
    "output_filename = \"multiperson_annotation.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_annotation(input_filename, output_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify annotation multiperson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def load_annotations(annotation_file, primary_subject_id, trial_id):\n",
    "    \"\"\"\n",
    "    Loads the keystep annotations for a given trial.\n",
    "    Returns a tuple (primary_annotations, secondary_annotations) where:\n",
    "      - primary_annotations come from the subject with subject_id == primary_subject_id\n",
    "      - secondary_annotations come from the subject with subject_id == f\"{primary_subject_id}-secondary\"\n",
    "    \"\"\"\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    primary_annotations = []\n",
    "    secondary_annotations = []\n",
    "    \n",
    "    for subject in data.get(\"subjects\", []):\n",
    "        sid = subject.get(\"subject_id\", \"\")\n",
    "        for trial in subject.get(\"trials\", []):\n",
    "            if trial.get(\"trial_id\") == trial_id:\n",
    "                if sid == primary_subject_id:\n",
    "                    primary_annotations = trial.get(\"keysteps\", [])\n",
    "                elif sid == f\"{primary_subject_id}-secondary\":\n",
    "                    secondary_annotations = trial.get(\"keysteps\", [])\n",
    "    return primary_annotations, secondary_annotations\n",
    "\n",
    "def find_trial(data, subject_id, trial_id):\n",
    "    \"\"\"\n",
    "    Returns the trial dict for a given subject_id and trial_id.\n",
    "    \"\"\"\n",
    "    for subject in data.get(\"subjects\", []):\n",
    "        if subject.get(\"subject_id\") == subject_id:\n",
    "            for trial in subject.get(\"trials\", []):\n",
    "                if trial.get(\"trial_id\") == trial_id:\n",
    "                    return trial\n",
    "    return None\n",
    "\n",
    "def get_video_file_from_trial(trial, preferred_extensions=['.mp4']):\n",
    "    \"\"\"\n",
    "    Iterates over the streams in a trial and returns the file_path of the first one\n",
    "    whose filename ends with one of the preferred extensions.\n",
    "    \"\"\"\n",
    "    clip_ego_stream = trial.get(\"streams\", {}).get(\"egocam_rgb_audio\", {})\n",
    "    file_path = clip_ego_stream.get(\"file_path\", \"\")\n",
    "    if any(file_path.lower().endswith(ext) for ext in preferred_extensions):\n",
    "        return file_path\n",
    "    \n",
    "    return None\n",
    "\n",
    "def visualize_annotations(video_path, primary_annotations, secondary_annotations, output_video_path=\"annotated_video.mp4\"):\n",
    "    \"\"\"\n",
    "    Opens the video file, reads it frame by frame, overlays text annotations based on keystep intervals,\n",
    "    and saves the annotated frames into an output video file.\n",
    "    Primary annotations are drawn in green and secondary annotations in red.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file:\", video_path)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Calculate current time (in seconds) based on frame index and FPS\n",
    "        current_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        current_time = current_frame / fps\n",
    "\n",
    "        # Build overlay texts for active keysteps at current time\n",
    "        primary_text = \"\"\n",
    "        for ann in primary_annotations:\n",
    "            if ann[\"start_t\"] <= current_time <= ann[\"end_t\"]:\n",
    "                primary_text += f\"{ann['label']} \"\n",
    "\n",
    "        secondary_text = \"\"\n",
    "        for ann in secondary_annotations:\n",
    "            if ann[\"start_t\"] <= current_time <= ann[\"end_t\"]:\n",
    "                secondary_text += f\"{ann['label']} \"\n",
    "\n",
    "        # Overlay texts on the frame:\n",
    "        if primary_text:\n",
    "            cv2.putText(frame, \"Primary: \" + primary_text, (50, 50), font, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        if secondary_text:\n",
    "            cv2.putText(frame, \"Secondary: \" + secondary_text, (50, 100), font, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Annotated video saved to {output_video_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary Annotations: [{'keystep_id': '5_FuqYvMnm', 'start_t': 0, 'end_t': 6.60863, 'label': 'approach_patient', 'class_id': 0}, {'keystep_id': '5_1JZ1Tkf5', 'start_t': 6.609, 'end_t': 11.41892, 'label': 'check_responsiveness', 'class_id': 1}, {'keystep_id': '5_2FerCyqc', 'start_t': 15.773, 'end_t': 83.70723, 'label': 'chest_compressions', 'class_id': 4}]\n",
      "Secondary Annotations: [{'keystep_id': '5_u2JcKI3V', 'start_t': 11.419, 'end_t': 15.77309, 'label': 'check_pulse', 'class_id': 2}, {'keystep_id': '5_fRl1lC6p', 'start_t': 13.395, 'end_t': 15.77309, 'label': 'check_breathing', 'class_id': 3}, {'keystep_id': '5_dfb1x5md', 'start_t': 19.50903, 'end_t': 22.50903, 'label': 'request_assistance', 'class_id': 6}, {'keystep_id': '5_wli4oXCE', 'start_t': 22.509, 'end_t': 25.11386, 'label': 'request_aed', 'class_id': 5}, {'keystep_id': '5_e4hWg5c0', 'start_t': 30.50903, 'end_t': 33.59237, 'label': 'turn_on_aed', 'class_id': 7}, {'keystep_id': '5_xXQP77f7', 'start_t': 33.884, 'end_t': 44.13068, 'label': 'attach_defib_pads', 'class_id': 8}, {'keystep_id': '5_5hQ8bC2y', 'start_t': 66.6757, 'end_t': 72.67494, 'label': 'attach_defib_pads', 'class_id': 8}, {'keystep_id': '5_5gbXcS1v', 'start_t': 83.707, 'end_t': 88.1881, 'label': 'no_action', 'class_id': 15}]\n",
      "Using video file: /standard/UVA-DSA/NIST EMS Project Data/EgoExoEMS_CVPR2025/Dataset/Final/ms1/cardiac_arrest/0/GoPro/GX010391_encoded_trimmed.mp4\n",
      "Annotated video saved to annotated_video.mp4\n"
     ]
    }
   ],
   "source": [
    "# Specify your annotation file and trial details here.\n",
    "annotation_file = output_filename  # modified annotation file from previous processing\n",
    "primary_subject_id = \"ms1\"  # e.g., original subject id\n",
    "trial_id = \"0\"            # trial you wish to verify\n",
    "\n",
    "# Load the full annotation JSON\n",
    "with open(annotation_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Get the primary trial details to obtain the video file path.\n",
    "primary_trial = find_trial(data, primary_subject_id, trial_id)\n",
    "if primary_trial is None:\n",
    "    print(f\"Trial {trial_id} not found for subject {primary_subject_id}.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "video_path = get_video_file_from_trial(primary_trial, preferred_extensions=[\".mp4\"])\n",
    "if video_path is None:\n",
    "    print(\"No video file (with .mp4 extension) found in the trial streams.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Load keystep annotations from both primary and secondary subjects.\n",
    "primary_annotations, secondary_annotations = load_annotations(annotation_file, primary_subject_id, trial_id)\n",
    "\n",
    "print(\"Primary Annotations:\", primary_annotations)\n",
    "print(\"Secondary Annotations:\", secondary_annotations)\n",
    "print(\"Using video file:\", video_path)\n",
    "\n",
    "visualize_annotations(video_path, primary_annotations, secondary_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import json\n",
    "\n",
    "class AnnotationEditor:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        self.master.title(\"Annotation Editor\")\n",
    "        \n",
    "        # Load annotation file\n",
    "        self.annotation_file = \"./multiperson_annotation.json\"\n",
    "        try:\n",
    "            with open(self.annotation_file, 'r') as f:\n",
    "                self.annotation_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to load annotation file: {e}\")\n",
    "            master.destroy()\n",
    "            return\n",
    "        \n",
    "        # Build a list of trial options that have a matching secondary subject.\n",
    "        self.trial_options = self.get_trial_options()\n",
    "        if not self.trial_options:\n",
    "            messagebox.showerror(\"Error\", \"No trials with secondary subjects found.\")\n",
    "            master.destroy()\n",
    "            return\n",
    "        \n",
    "        # Create UI elements\n",
    "        top_frame = tk.Frame(master)\n",
    "        top_frame.pack(pady=5)\n",
    "        \n",
    "        tk.Label(top_frame, text=\"Select Trial (format: PrimarySubjectID:TrialID)\").pack(side=tk.LEFT)\n",
    "        self.selected_trial_var = tk.StringVar(value=self.trial_options[0])\n",
    "        self.trial_dropdown = ttk.Combobox(top_frame, textvariable=self.selected_trial_var, values=self.trial_options, state=\"readonly\")\n",
    "        self.trial_dropdown.pack(side=tk.LEFT, padx=5)\n",
    "        self.trial_dropdown.bind(\"<<ComboboxSelected>>\", self.on_trial_selected)\n",
    "        \n",
    "        # Video display area\n",
    "        self.video_label = tk.Label(master)\n",
    "        self.video_label.pack()\n",
    "        \n",
    "        # Active annotations listbox\n",
    "        listbox_frame = tk.Frame(master)\n",
    "        listbox_frame.pack(pady=5)\n",
    "        tk.Label(listbox_frame, text=\"Active Annotations (click to select then switch):\").pack()\n",
    "        self.annotation_listbox = tk.Listbox(listbox_frame, width=60, height=6)\n",
    "        self.annotation_listbox.pack()\n",
    "        \n",
    "        # Control buttons\n",
    "        control_frame = tk.Frame(master)\n",
    "        control_frame.pack(pady=5)\n",
    "        self.switch_button = tk.Button(control_frame, text=\"Switch Annotation\", command=self.switch_annotation)\n",
    "        self.switch_button.pack(side=tk.LEFT, padx=5)\n",
    "        self.save_button = tk.Button(control_frame, text=\"Save Annotations\", command=self.save_annotations)\n",
    "        self.save_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Video playback variables\n",
    "        self.cap = None\n",
    "        self.fps = 0\n",
    "        self.video_running = False\n",
    "        \n",
    "        # Load the first trial by default\n",
    "        self.load_selected_trial()\n",
    "    \n",
    "    def get_trial_options(self):\n",
    "        \"\"\"\n",
    "        Returns a list of strings \"PrimarySubjectID:TrialID\" for which there exists\n",
    "        both a primary subject (without \"-secondary\") and its corresponding secondary subject.\n",
    "        \"\"\"\n",
    "        options = []\n",
    "        primary_ids = {subj[\"subject_id\"] for subj in self.annotation_data.get(\"subjects\", []) if not subj[\"subject_id\"].endswith(\"-secondary\")}\n",
    "        for pid in primary_ids:\n",
    "            secondary_id = pid + \"-secondary\"\n",
    "            primary_trials = {trial[\"trial_id\"] for trial in self.get_trials_for_subject(pid)}\n",
    "            secondary_trials = {trial[\"trial_id\"] for trial in self.get_trials_for_subject(secondary_id)}\n",
    "            common = primary_trials.intersection(secondary_trials)\n",
    "            for trial_id in common:\n",
    "                options.append(f\"{pid}:{trial_id}\")\n",
    "        return options\n",
    "    \n",
    "    def get_trials_for_subject(self, subject_id):\n",
    "        for subj in self.annotation_data.get(\"subjects\", []):\n",
    "            if subj.get(\"subject_id\") == subject_id:\n",
    "                return subj.get(\"trials\", [])\n",
    "        return []\n",
    "    \n",
    "    def on_trial_selected(self, event=None):\n",
    "        self.load_selected_trial()\n",
    "    \n",
    "    def load_selected_trial(self):\n",
    "        # Stop any running video playback.\n",
    "        self.video_running = False\n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "            self.cap = None\n",
    "        \n",
    "        selection = self.selected_trial_var.get()\n",
    "        if not selection:\n",
    "            return\n",
    "        # Expect selection in format \"PrimarySubjectID:TrialID\"\n",
    "        try:\n",
    "            primary_id, trial_id = selection.split(\":\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Invalid trial selection format: {e}\")\n",
    "            return\n",
    "        \n",
    "        self.primary_subject_id = primary_id\n",
    "        self.trial_id = trial_id\n",
    "        \n",
    "        # Retrieve primary and secondary trial objects.\n",
    "        self.primary_trial = self.find_trial(self.primary_subject_id, self.trial_id)\n",
    "        self.secondary_trial = self.find_trial(self.primary_subject_id + \"-secondary\", self.trial_id)\n",
    "        if self.primary_trial is None or self.secondary_trial is None:\n",
    "            messagebox.showerror(\"Error\", \"Selected trial not found in both primary and secondary subjects.\")\n",
    "            return\n",
    "        \n",
    "        # Get video file path from primary trial's 'clip_ego' stream.\n",
    "        self.video_path = self.get_video_file_from_trial(self.primary_trial)\n",
    "        if not self.video_path:\n",
    "            messagebox.showerror(\"Error\", \"No video file found for the selected trial.\")\n",
    "            return\n",
    "        \n",
    "        # Open the video capture\n",
    "        self.cap = cv2.VideoCapture(self.video_path)\n",
    "        if not self.cap.isOpened():\n",
    "            messagebox.showerror(\"Error\", f\"Cannot open video file: {self.video_path}\")\n",
    "            return\n",
    "        self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
    "        self.video_running = True\n",
    "        self.update_frame()\n",
    "    \n",
    "    def find_trial(self, subject_id, trial_id):\n",
    "        for subj in self.annotation_data.get(\"subjects\", []):\n",
    "            if subj.get(\"subject_id\") == subject_id:\n",
    "                for trial in subj.get(\"trials\", []):\n",
    "                    if trial.get(\"trial_id\") == trial_id:\n",
    "                        return trial\n",
    "        return None\n",
    "    \n",
    "    def get_video_file_from_trial(self, trial, preferred_extensions=['.mp4']):\n",
    "        \"\"\"\n",
    "        Returns the file_path from the 'egocam_rgb_audio' stream if it matches one of the preferred extensions.\n",
    "        \"\"\"\n",
    "        clip_ego = trial.get(\"streams\", {}).get(\"egocam_rgb_audio\", {})\n",
    "        file_path = clip_ego.get(\"file_path\", \"\")\n",
    "        if any(file_path.lower().endswith(ext) for ext in preferred_extensions):\n",
    "            return file_path\n",
    "        return None\n",
    "    \n",
    "    def update_frame(self):\n",
    "        if not self.video_running or self.cap is None:\n",
    "            return\n",
    "        \n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            # Restart video at end.\n",
    "            self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            ret, frame = self.cap.read()\n",
    "        current_frame_num = self.cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        current_time = current_frame_num / self.fps\n",
    "        \n",
    "        # Optionally overlay active annotations on the frame.\n",
    "        primary_text = \"\"\n",
    "        for ann in self.primary_trial.get(\"keysteps\", []):\n",
    "            if ann[\"start_t\"] <= current_time <= ann[\"end_t\"]:\n",
    "                primary_text += f\"{ann['label']} \"\n",
    "        secondary_text = \"\"\n",
    "        for ann in self.secondary_trial.get(\"keysteps\", []):\n",
    "            if ann[\"start_t\"] <= current_time <= ann[\"end_t\"]:\n",
    "                secondary_text += f\"{ann['label']} \"\n",
    "        cv2.putText(frame, \"Primary: \" + primary_text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.putText(frame, \"Secondary: \" + secondary_text, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "        \n",
    "        # Convert frame to Tkinter-compatible image.\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(frame_rgb)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        self.video_label.imgtk = imgtk\n",
    "        self.video_label.configure(image=imgtk)\n",
    "        \n",
    "        # Update the active annotations listbox.\n",
    "        self.update_active_annotations(current_time)\n",
    "        \n",
    "        # Schedule the next frame update.\n",
    "        self.master.after(int(1000/self.fps), self.update_frame)\n",
    "    \n",
    "    def update_active_annotations(self, current_time):\n",
    "        # Clear the listbox.\n",
    "        self.annotation_listbox.delete(0, tk.END)\n",
    "        # Add primary annotations active at current time.\n",
    "        for idx, ann in enumerate(self.primary_trial.get(\"keysteps\", [])):\n",
    "            if ann[\"start_t\"] <= current_time <= ann[\"end_t\"]:\n",
    "                text = f\"Primary - {idx}: {ann['label']} [{ann['start_t']} - {ann['end_t']}]\"\n",
    "                self.annotation_listbox.insert(tk.END, text)\n",
    "        # Add secondary annotations active at current time.\n",
    "        for idx, ann in enumerate(self.secondary_trial.get(\"keysteps\", [])):\n",
    "            if ann[\"start_t\"] <= current_time <= ann[\"end_t\"]:\n",
    "                text = f\"Secondary - {idx}: {ann['label']} [{ann['start_t']} - {ann['end_t']}]\"\n",
    "                self.annotation_listbox.insert(tk.END, text)\n",
    "    \n",
    "    def switch_annotation(self):\n",
    "        # Get the selected annotation from the listbox.\n",
    "        sel = self.annotation_listbox.curselection()\n",
    "        if not sel:\n",
    "            messagebox.showinfo(\"Info\", \"Please select an annotation to switch.\")\n",
    "            return\n",
    "        index = sel[0]\n",
    "        item_text = self.annotation_listbox.get(index)\n",
    "        if item_text.startswith(\"Primary\"):\n",
    "            try:\n",
    "                # Extract index from text: \"Primary - {idx}: ...\"\n",
    "                ann_idx = int(item_text.split(\"-\")[1].split(\":\")[0].strip())\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", f\"Failed to parse annotation index: {e}\")\n",
    "                return\n",
    "            if ann_idx < len(self.primary_trial.get(\"keysteps\", [])):\n",
    "                ann = self.primary_trial[\"keysteps\"].pop(ann_idx)\n",
    "                self.secondary_trial.setdefault(\"keysteps\", []).append(ann)\n",
    "                messagebox.showinfo(\"Info\", \"Annotation switched from primary to secondary.\")\n",
    "        elif item_text.startswith(\"Secondary\"):\n",
    "            try:\n",
    "                ann_idx = int(item_text.split(\"-\")[1].split(\":\")[0].strip())\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", f\"Failed to parse annotation index: {e}\")\n",
    "                return\n",
    "            if ann_idx < len(self.secondary_trial.get(\"keysteps\", [])):\n",
    "                ann = self.secondary_trial[\"keysteps\"].pop(ann_idx)\n",
    "                self.primary_trial.setdefault(\"keysteps\", []).append(ann)\n",
    "                messagebox.showinfo(\"Info\", \"Annotation switched from secondary to primary.\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Invalid annotation selection.\")\n",
    "        # Refresh the listbox.\n",
    "        current_frame = self.cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        self.update_active_annotations(current_frame / self.fps)\n",
    "    \n",
    "    def save_annotations(self):\n",
    "        output_file = \"modified_annotation_updated.json\"\n",
    "        try:\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(self.annotation_data, f, indent=4)\n",
    "            messagebox.showinfo(\"Info\", f\"Annotations saved to {output_file}\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to save annotations: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = AnnotationEditor(root)\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
