{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Generation Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import fnmatch\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../../Annotations'\n",
    "temp_output_dir = './temp'\n",
    "\n",
    "\n",
    "\n",
    "task = \"classification\" # \"classification\" or \"segmentation\" or \"cpr_quality\"\n",
    "task = \"segmentation\" # \"classification\" or \"segmentation\" or \"cpr_quality\"\n",
    "task = \"cpr_quality\" # \"classification\" or \"segmentation\" or \"cpr_quality\"\n",
    "\n",
    "# root_dir = '/standard/UVA-DSA/NIST EMS Project Data/EgoExoEMS_CVPR2025/Dataset/Final'  # Replace with your directory path\n",
    "# root_dir = '/standard/UVA-DSA/NIST EMS Project Data/DataCollection_Spring_2025/CARS/organized'  # Replace with your directory path\n",
    "root_dir = '/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/'  # Replace with your directory path\n",
    "\n",
    "# output_file = f'{temp_output_dir}/main_annotation.json' # temp\n",
    "output_file = f'{output_dir}/aaai26_main_annotation_{task}.json'\n",
    "# output_file = f'{output_dir}/main_annotation_{task}.json'\n",
    "\n",
    "# remove existing output file if it exists\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Ready Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# production ready\n",
    "is_production = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "from collections import OrderedDict\n",
    "\n",
    "accepted_stream_types = [\n",
    "    'ego', 'clip_ego', 'resnet_ego',\n",
    "    'distance_sensor_data',\n",
    "    'smartwatch_data', 'audio', 'i3d_flow', 'i3d_rgb', 'resnet_ego',\n",
    "    'BBOX_MASKS'\n",
    "]\n",
    "\n",
    "# --- UPDATED: no remapping; directory name is the stream type ---\n",
    "def get_stream_type(directory_name: str) -> str:\n",
    "    \"\"\"Return the canonical stream type from the directory name (no aliases).\"\"\"\n",
    "    return directory_name\n",
    "\n",
    "# --- UPDATED: parsing rules aligned to accepted_stream_types ---\n",
    "def parse_file(file_path: str, stream_type: str):\n",
    "    \"\"\"\n",
    "    Decide whether to keep this file for the given stream_type.\n",
    "    NOTE: For 'audio', JSONs are handled outside this function and placed\n",
    "    into 'audio_transcript' (virtual stream key).\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_id, ext = os.path.splitext(file_name)\n",
    "    ext = ext.lower()\n",
    "\n",
    "    if stream_type == 'ego':\n",
    "        # Keep .mp4 that end with *_rgb_final or *_rgb_partial AND any .json (e.g., metadata)\n",
    "        if ext == '.mp4':\n",
    "            if not (file_id.endswith(\"rgb_final\") or file_id.endswith(\"rgb_partial\")):\n",
    "                return None\n",
    "        elif ext == '.json' and file_id.endswith(\"annotation\"):\n",
    "            pass\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    elif stream_type == 'audio':\n",
    "        # Only audio binaries here (.mp3/.wav). JSON is routed to 'audio_transcript' outside.\n",
    "        if ext not in ('.mp3', '.wav'):\n",
    "            return None\n",
    "\n",
    "    elif stream_type == 'distance_sensor_data':\n",
    "        if ext != '.csv':\n",
    "            return None\n",
    "\n",
    "    elif stream_type == 'smartwatch_data':\n",
    "        # Keep only synced CSVs\n",
    "        if ext != '.csv':\n",
    "            return None\n",
    "\n",
    "    elif stream_type in ('i3d_flow', 'i3d_rgb'):\n",
    "        if ext != '.npy':\n",
    "            return None\n",
    "        if stream_type == 'i3d_flow' and 'flow' not in file_id:\n",
    "            return None\n",
    "        if stream_type == 'i3d_rgb'  and 'rgb'  not in file_id:\n",
    "            return None\n",
    "\n",
    "    elif stream_type in ('resnet_ego', 'clip_ego'):\n",
    "        if ext != '.npy':\n",
    "            return None\n",
    "\n",
    "    elif stream_type == 'BBOX_MASKS':\n",
    "        # Allow common mask formats; adjust if you have a stricter spec\n",
    "        if ext not in ('.png', '.npy', '.json'):\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        # Any other stream types are not supported\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"file_id\":   file_id,\n",
    "        \"file_path\": file_path,\n",
    "    }\n",
    "\n",
    "def normalize_subject_key(subject_key: str) -> str:\n",
    "    return subject_key\n",
    "\n",
    "# --- UPDATED: only strip .txt globally; do not remove images anymore ---\n",
    "def process_files(files):\n",
    "    return [f for f in files if not f.endswith('.txt')]\n",
    "\n",
    "\n",
    "# ---------- helpers for ego â€œVerifiedâ€ selection ----------\n",
    "_VERIFIED_PREFIX_RE = re.compile(r'^(?:Verified[\\s_-]*)', re.IGNORECASE)\n",
    "\n",
    "def _strip_verified_prefix(s: str) -> str:\n",
    "    return _VERIFIED_PREFIX_RE.sub('', s)\n",
    "\n",
    "def _is_verified_name(s: str) -> bool:\n",
    "    return bool(_VERIFIED_PREFIX_RE.match(s))\n",
    "\n",
    "def process_directory(root_path):\n",
    "    subjects = []\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        parts = root.split(os.sep)\n",
    "        # Keep your depth constraint if it reflects your dataset layout\n",
    "        if len(parts) != 9:\n",
    "            continue\n",
    "\n",
    "        subj_key    = normalize_subject_key(parts[-4])\n",
    "        scen_id     = parts[-3]\n",
    "        trial_id    = parts[-2]\n",
    "        stream_name = parts[-1]\n",
    "\n",
    "        # Only accept directories that are in the canonical list\n",
    "        if subj_key.startswith('ld') or stream_name not in accepted_stream_types:\n",
    "            continue\n",
    "\n",
    "        stream_type = get_stream_type(stream_name)\n",
    "        files = process_files(files)\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        # --- subject, scenario, trial setup ---\n",
    "        subj = next((s for s in subjects if s['subject_id'] == subj_key), None)\n",
    "        if not subj:\n",
    "            subj = OrderedDict([\n",
    "                (\"subject_id\", subj_key),\n",
    "                (\"expertise_level\", \"EMT\"),\n",
    "                (\"scenarios\", [])\n",
    "            ])\n",
    "            subjects.append(subj)\n",
    "\n",
    "        scen = next((sc for sc in subj['scenarios'] if sc['scenario_id'] == scen_id), None)\n",
    "        if not scen:\n",
    "            scen = OrderedDict([(\"scenario_id\", scen_id), (\"trials\", [])])\n",
    "            subj['scenarios'].append(scen)\n",
    "\n",
    "        trial = next((t for t in scen['trials'] if t['trial_id'] == trial_id), None)\n",
    "        if not trial:\n",
    "            trial = OrderedDict([\n",
    "                (\"trial_id\", trial_id),\n",
    "                (\"streams\", OrderedDict()),\n",
    "                (\"keysteps\", OrderedDict()),\n",
    "            ])\n",
    "            scen['trials'].append(trial)\n",
    "\n",
    "        streams = trial['streams']\n",
    "\n",
    "        # --- UPDATED: split audio transcripts from audio binaries ---\n",
    "        if stream_type == 'audio':\n",
    "            for fname in sorted(files):\n",
    "                full = os.path.join(root, fname)\n",
    "                _, ext = os.path.splitext(fname)\n",
    "                ext = ext.lower()\n",
    "\n",
    "                if ext == '.json' and fname.endswith('_deidentified.json'):\n",
    "                    # Route transcript JSON into a separate stream key\n",
    "                    file_id, _ = os.path.splitext(os.path.basename(full))\n",
    "                    info = {\"file_id\": file_id, \"file_path\": full}\n",
    "                    streams.setdefault('audio_transcript', []).append(info)\n",
    "                else:\n",
    "                    info = parse_file(full, 'audio')\n",
    "                    if info:\n",
    "                        streams.setdefault('audio', []).append(info)\n",
    "\n",
    "        \n",
    "        # --- ego: prefer Verified* when duplicates exist ---\n",
    "        elif stream_type == 'ego':\n",
    "            # collect valid mp4s (by suffix rule) and jsons\n",
    "            candidate_mp4s = []\n",
    "            jsons = []\n",
    "            for fname in sorted(files):\n",
    "                full = os.path.join(root, fname)\n",
    "                info = parse_file(full, 'ego')\n",
    "                if not info:\n",
    "                    continue\n",
    "                if full.lower().endswith('.mp4'):\n",
    "                    candidate_mp4s.append(info)\n",
    "                elif full.lower().endswith('.json'):\n",
    "                    jsons.append(info)\n",
    "\n",
    "            # Deduplicate by normalized id (strip 'Verified' prefix),\n",
    "            # preferring files whose base name starts with Verified*\n",
    "            chosen_by_norm = {}\n",
    "            for info in candidate_mp4s:\n",
    "                file_id = info[\"file_id\"]\n",
    "                norm_id = _strip_verified_prefix(file_id)\n",
    "                is_verified = _is_verified_name(file_id)\n",
    "                prev = chosen_by_norm.get(norm_id)\n",
    "                if prev is None or (is_verified and not _is_verified_name(prev[\"file_id\"])):\n",
    "                    chosen_by_norm[norm_id] = info\n",
    "\n",
    "            # Save selected videos\n",
    "            if chosen_by_norm:\n",
    "                streams.setdefault('ego', []).extend(sorted(chosen_by_norm.values(), key=lambda x: x[\"file_id\"]))\n",
    "            # Save jsons as usual\n",
    "            if jsons:\n",
    "                streams.setdefault('ego', []).extend(sorted(jsons, key=lambda x: x[\"file_id\"]))\n",
    "\n",
    "        else:\n",
    "            # Standard handling for all other streams\n",
    "            for fname in sorted(files):\n",
    "                full = os.path.join(root, fname)\n",
    "                info = parse_file(full, stream_type)\n",
    "                if info:\n",
    "                    streams.setdefault(stream_type, []).append(info)\n",
    "\n",
    "    # sort for consistency\n",
    "    subjects.sort(key=lambda s: s['subject_id'])\n",
    "    for subj in subjects:\n",
    "        subj['scenarios'].sort(key=lambda sc: sc['scenario_id'])\n",
    "        for scen in subj['scenarios']:\n",
    "            scen['trials'].sort(key=lambda t: t['trial_id'])\n",
    "\n",
    "    return subjects\n",
    "\n",
    "def generate_json_structure(root_directory, version=\"v1.2025.07.07\"):\n",
    "    return OrderedDict([\n",
    "        (\"subjects\", process_directory(root_directory)),\n",
    "        (\"version\",  version)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_data = generate_json_structure(root_dir)\n",
    "\n",
    "# sort the json structure\n",
    "json_data = dict(sorted(json_data.items()))\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "print(f\"JSON structure saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict, OrderedDict\n",
    "from typing import Dict, List, Tuple, Union, Iterable\n",
    "\n",
    "TrialID = str\n",
    "\n",
    "def _iter_trials(data: Dict) -> Iterable[Tuple[TrialID, Dict]]:\n",
    "    \"\"\"\n",
    "    Yield (trial_key, trial_dict) across all subjects/scenarios/trials.\n",
    "    trial_key format: \"<subject_id>/<scenario_id>/<trial_id>\"\n",
    "    \"\"\"\n",
    "    subjects = data.get(\"subjects\", [])\n",
    "    for subj in subjects:\n",
    "        sid = subj.get(\"subject_id\", \"UNKNOWN_SUBJECT\")\n",
    "        for scen in subj.get(\"scenarios\", []):\n",
    "            scid = scen.get(\"scenario_id\", \"UNKNOWN_SCENARIO\")\n",
    "            for trial in scen.get(\"trials\", []):\n",
    "                tid = trial.get(\"trial_id\", \"UNKNOWN_TRIAL\")\n",
    "                trial_key = f\"{sid}/{scid}/{tid}\"\n",
    "                yield trial_key, trial\n",
    "\n",
    "def _count_files_in_stream(stream_items) -> int:\n",
    "    \"\"\"Count files in a stream list (list of dicts with file_id/file_path).\"\"\"\n",
    "    if not isinstance(stream_items, list):\n",
    "        return 0\n",
    "    return sum(1 for _ in stream_items)\n",
    "\n",
    "def sanity_check_dataset(\n",
    "    dataset: Union[str, Dict],\n",
    "    required_streams: Iterable[str] = (\"smartwatch_data\",),\n",
    "    also_show_empty_streams: bool = False,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Sanity check the generated dataset JSON.\n",
    "\n",
    "    Args:\n",
    "        dataset: dict already loaded OR path to a JSON file.\n",
    "        required_streams: streams that must exist (>=1 file) in every trial.\n",
    "        also_show_empty_streams: if True, list trials where a stream key exists but has 0 files.\n",
    "\n",
    "    Returns:\n",
    "        A dict with summary, per_stream stats, and missing required streams.\n",
    "    \"\"\"\n",
    "    # Load if a path is provided\n",
    "    if isinstance(dataset, str):\n",
    "        with open(dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = dataset\n",
    "\n",
    "    # Collect trials\n",
    "    trial_list: List[Tuple[TrialID, Dict]] = list(_iter_trials(data))\n",
    "    total_trials = len(trial_list)\n",
    "\n",
    "    # Per-trial stream file counts\n",
    "    per_trial_stream_counts: Dict[TrialID, Dict[str, int]] = {}\n",
    "    # Per-stream coverage\n",
    "    stream_total_files: Dict[str, int] = defaultdict(int)\n",
    "    stream_trials_having_any: Dict[str, int] = defaultdict(int)\n",
    "    stream_trials_with_zero: Dict[str, List[TrialID]] = defaultdict(list)\n",
    "\n",
    "    # Track missing required streams per trial\n",
    "    missing_required_by_stream: Dict[str, List[TrialID]] = {s: [] for s in required_streams}\n",
    "\n",
    "    for trial_key, trial in trial_list:\n",
    "        streams: Dict[str, list] = trial.get(\"streams\", {}) or {}\n",
    "        # Count files per stream for this trial\n",
    "        this_trial_counts: Dict[str, int] = {}\n",
    "        for stream_name, items in streams.items():\n",
    "            n = _count_files_in_stream(items)\n",
    "            this_trial_counts[stream_name] = n\n",
    "            stream_total_files[stream_name] += n\n",
    "            if n > 0:\n",
    "                stream_trials_having_any[stream_name] += 1\n",
    "            else:\n",
    "                stream_trials_with_zero[stream_name].append(trial_key)\n",
    "\n",
    "        # For streams that never appear as a key in this trial, they count as zero here:\n",
    "        all_stream_names_seen = set(this_trial_counts.keys())\n",
    "        # You may prefer to evaluate required streams even if they don't exist as keys.\n",
    "        for req in required_streams:\n",
    "            n = this_trial_counts.get(req, 0)\n",
    "            if n <= 0:\n",
    "                missing_required_by_stream.setdefault(req, []).append(trial_key)\n",
    "\n",
    "        per_trial_stream_counts[trial_key] = this_trial_counts\n",
    "\n",
    "    # Build per-stream report\n",
    "    per_stream_report = OrderedDict()\n",
    "    all_streams = sorted(set(stream_total_files.keys()) | set(stream_trials_with_zero.keys()))\n",
    "    for s in all_streams:\n",
    "        per_stream_report[s] = {\n",
    "            \"total_files\": stream_total_files.get(s, 0),\n",
    "            \"trials_with_at_least_one\": stream_trials_having_any.get(s, 0),\n",
    "            \"trials_missing_or_zero\": (\n",
    "                stream_trials_with_zero.get(s, [])  # keys present but zero files\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    result = {\n",
    "        \"summary\": {\n",
    "            \"total_subjects\": len(data.get(\"subjects\", [])),\n",
    "            \"total_trials\": total_trials,\n",
    "        },\n",
    "        \"per_stream\": per_stream_report,\n",
    "        \"missing_required_streams\": {\n",
    "            s: sorted(missing_required_by_stream.get(s, []))\n",
    "            for s in required_streams\n",
    "        },\n",
    "        \"per_trial_stream_counts\": per_trial_stream_counts,  # useful for deeper debugging\n",
    "    }\n",
    "\n",
    "    # Pretty print a compact summary to stdout\n",
    "    _print_sanity_summary(result, also_show_empty_streams=also_show_empty_streams)\n",
    "    return result\n",
    "\n",
    "def _print_sanity_summary(report: Dict, also_show_empty_streams: bool = False) -> None:\n",
    "    print(\"=== DATASET SANITY CHECK ===\")\n",
    "    print(f\"Total subjects: {report['summary']['total_subjects']}\")\n",
    "    print(f\"Total trials:   {report['summary']['total_trials']}\")\n",
    "    print()\n",
    "\n",
    "    print(\"=== PER-STREAM COVERAGE ===\")\n",
    "    print(f\"{'stream':25} {'files':>8} {'trials_with_any':>16}\")\n",
    "    for s, stats in report[\"per_stream\"].items():\n",
    "        print(f\"{s:25} {stats['total_files']:8d} {stats['trials_with_at_least_one']:16d}\")\n",
    "    print()\n",
    "\n",
    "    # Missing required streams\n",
    "    print(\"=== MISSING REQUIRED STREAMS ===\")\n",
    "    any_missing = False\n",
    "    for s, trials in report[\"missing_required_streams\"].items():\n",
    "        if trials:\n",
    "            any_missing = True\n",
    "            print(f\"- {s}: missing in {len(trials)} trial(s)\")\n",
    "            for t in trials:\n",
    "                print(f\"    â€¢ {t}\")\n",
    "    if not any_missing:\n",
    "        print(\"None ðŸŽ‰\")\n",
    "    print()\n",
    "\n",
    "    if also_show_empty_streams:\n",
    "        print(\"=== STREAM KEYS PRESENT BUT ZERO FILES (per stream) ===\")\n",
    "        for s, stats in report[\"per_stream\"].items():\n",
    "            zeros = stats.get(\"trials_missing_or_zero\", [])\n",
    "            if zeros:\n",
    "                print(f\"- {s}: {len(zeros)} trial(s)\")\n",
    "                for t in zeros:\n",
    "                    print(f\"    â€¢ {t}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "rep = sanity_check_dataset(json_data, required_streams=(\"smartwatch_data\",\"audio\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Key Steps using VIA Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def add_keysteps_to_json(\n",
    "    existing_json,\n",
    "    keystep_json_path,\n",
    "    subject_id=\"ng1\",\n",
    "    scenario_id=\"cardiac_arrest\",\n",
    "    trial_id=\"1\",\n",
    "    gopro_file_name=\"gopro_1.mp4\"\n",
    "):\n",
    "    # --- load annotations & mappings ---\n",
    "    with open(keystep_json_path, 'r') as f:\n",
    "        keystep_data = json.load(f)\n",
    "    with open('./class_id_mappings.json', 'r') as f:\n",
    "        class_id_mapping = json.load(f)\n",
    "\n",
    "    print(\"Adding keysteps to the JSON structure\")\n",
    "    print(f\"Subject ID: {subject_id}, Trial ID: {trial_id}, GoPro file name: {gopro_file_name}\")\n",
    "\n",
    "    # find which vidâ€key in keystep_data['file'] matches your GoPro filename\n",
    "    correct_vid_id = None\n",
    "    for vid_key, file_meta in keystep_data.get('file', {}).items():\n",
    "        print(f\"Checking file meta: {file_meta.get('fname', '')} against {gopro_file_name}\")\n",
    "        if gopro_file_name in file_meta.get('fname', ''):\n",
    "            print(\"Found matching keystep file entry:\", file_meta['fname'])\n",
    "            correct_vid_id = vid_key\n",
    "            break\n",
    "        if gopro_file_name.split('_')[0] in file_meta.get('fname', ''):\n",
    "            print(\"Found matching keystep file entry (partial match):\", file_meta['fname'])\n",
    "            correct_vid_id = vid_key\n",
    "            break\n",
    "\n",
    "    # build the keysteps list\n",
    "    keysteps = []\n",
    "    for ks_id, meta in keystep_data.get('metadata', {}).items():\n",
    "        # only include those for our video, if desired:\n",
    "        print(f\"Video ID {meta.get('vid')} VS correct_vid_id {correct_vid_id}\")\n",
    "        if correct_vid_id and meta.get('vid') != correct_vid_id:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing keystep {ks_id} for video {meta}\")\n",
    "\n",
    "        try:\n",
    "            start_t, end_t = meta['z']\n",
    "            label = meta['av']['1']\n",
    "        except Exception as e:\n",
    "            print(f\"  â†’ skipping {ks_id}: missing z or av/1\")\n",
    "            continue\n",
    "\n",
    "        if label not in class_id_mapping.get('keysteps', {}):\n",
    "            print(f\"  â†’ skipping {ks_id}: â€œ{label}â€ not a keystep\")\n",
    "            continue\n",
    "\n",
    "        if task == \"cpr_quality\" and label != \"chest_compressions\":\n",
    "            print(f\"  â†’ skipping {ks_id}: â€œ{label}â€ not a keystep for CPR quality task\")\n",
    "            continue\n",
    "            \n",
    "        keysteps.append({\n",
    "            \"keystep_id\": ks_id,\n",
    "            \"start_t\":    start_t,\n",
    "            \"end_t\":      end_t,\n",
    "            \"label\":      label,\n",
    "            \"class_id\":   class_id_mapping['keysteps'][label]\n",
    "        })\n",
    "\n",
    "    # now inject into the right trial in the new hierarchy\n",
    "    found = False\n",
    "    for subj in existing_json.get('subjects', []):\n",
    "        if subj.get('subject_id') != subject_id:\n",
    "            continue\n",
    "        for scen in subj.get('scenarios', []):\n",
    "            if scen.get('scenario_id') != scenario_id:\n",
    "                continue\n",
    "            for trial in scen.get('trials', []):\n",
    "                if trial.get('trial_id') == trial_id:\n",
    "                    trial['keysteps'] = keysteps\n",
    "                    print(f\"â†’ Inserted {len(keysteps)} keysteps into \"\n",
    "                          f\"subject={subject_id}, scenario={scen['scenario_id']}, trial={trial_id}\")\n",
    "                    found = True\n",
    "    if not found:\n",
    "        print(f\"Warning: no matching subject={subject_id} / trial={trial_id} found in existing_json\")\n",
    "\n",
    "    return existing_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(output_file, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    updated_json = data\n",
    "\n",
    "    for subject in updated_json.get('subjects', []):\n",
    "        print(\"*\" * 50)\n",
    "        # if subject['subject_id'] != 'cars_1': continue\n",
    "\n",
    "        for scenario in subject.get('scenarios', []):\n",
    "            print(f\" Scenario: {scenario['scenario_id']}\")\n",
    "            \n",
    "            for trial in scenario.get('trials', []):\n",
    "                print(f\"  Trial: {trial['trial_id']}\")\n",
    "                \n",
    "                streams = trial.get('streams', {})\n",
    "                # only care about GoPro streams\n",
    "                if 'ego' not in streams:\n",
    "                    continue\n",
    "\n",
    "                annotation_file = None\n",
    "                gopro_file_name = None\n",
    "\n",
    "                for file in streams['ego']:\n",
    "                    fp = file.get('file_path', '')\n",
    "\n",
    "                    print(f\"    File: {file['file_id']} @ {fp}\")\n",
    "\n",
    "                    # pick out the .json annotation\n",
    "                    if fp.endswith('.json'):\n",
    "                        annotation_file = fp\n",
    "                        print(f\"    Annotation: {file['file_id']} @ {fp}\")\n",
    "\n",
    "                    # pick out the matching .mp4\n",
    "                    if fp.endswith('rgb_final.mp4') or fp.endswith('rgb_partial.mp4'):\n",
    "                        gopro_file_name = file['file_id'] + '.mp4'\n",
    "                        print(f\"    GoPro video: {gopro_file_name}\")\n",
    "\n",
    "                \n",
    "\n",
    "                if annotation_file and gopro_file_name:\n",
    "                    # inject keysteps\n",
    "                    updated_json = add_keysteps_to_json(\n",
    "                        updated_json,\n",
    "                        annotation_file,\n",
    "                        subject_id=subject['subject_id'],\n",
    "                        scenario_id=scenario['scenario_id'],\n",
    "                        trial_id=trial['trial_id'],\n",
    "                        gopro_file_name=gopro_file_name\n",
    "                    )\n",
    "\n",
    "                    # remove that .json from the stream listing\n",
    "                    trial['streams']['ego'] = [\n",
    "                        f for f in trial['streams']['ego']\n",
    "                        if not f['file_path'].endswith('.json')\n",
    "                    ]\n",
    "                else:\n",
    "                    print(f\"  â†’ no annotation file or GoPro video found for {subject['subject_id']} / {scenario['scenario_id']} / {trial['trial_id']}\")\n",
    "\n",
    "    # write back out\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(updated_json, json_file, indent=4)\n",
    "\n",
    "    print(f\"\\nUpdated JSON saved to {output_file}\")\n",
    "    print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert any list to singular objects in stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'r') as json_file:\n",
    "    updated_json = json.load(json_file)\n",
    "\n",
    "    for subject in updated_json.get('subjects', []):\n",
    "        print(\"*\" * 50)\n",
    "        # if subject['subject_id'] != 'ng8': continue\n",
    "\n",
    "        for scenario in subject.get('scenarios', []):\n",
    "            print(f\" Scenario: {scenario['scenario_id']}\")\n",
    "\n",
    "            for trial in scenario.get('trials', []):\n",
    "                print(f\"  Trial: {trial['trial_id']}\")\n",
    "\n",
    "                # We need to iterate over a list of keys since we'll be modifying the dict\n",
    "                for stream_type in list(trial.get('streams', {}).keys()):\n",
    "                    files = trial['streams'][stream_type]\n",
    "                    print(f\"   {stream_type}: {files} (count={len(files) if isinstance(files, list) else 'N/A'})\")\n",
    "\n",
    "                    # If it's a list, collapse it to just the first element\n",
    "                    if isinstance(files, list) and files:\n",
    "                        trial['streams'][stream_type] = files[0]\n",
    "                        print(f\"    â†’ flattened to: {trial['streams'][stream_type]}\")\n",
    "\n",
    "    # Write the updated JSON back out\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(updated_json, json_file, indent=4)\n",
    "\n",
    "    print(f\"\\nUpdated JSON saved to {output_file}\")\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Expertise Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file\n",
    "expertise_level_file = './subject_expertise_level.json'\n",
    "# load data to a dictionary\n",
    "with open(expertise_level_file) as json_file:\n",
    "    expertise_level_data = json.load(json_file)['EXPERTISE_LEVELS']\n",
    "\n",
    "print(expertise_level_data)\n",
    "\n",
    "# iterate over the subjects and trials in main_annotation.json and update the expertise level\n",
    "with open(output_file, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    updated_json = data\n",
    "    \n",
    "    for subject in updated_json['subjects']:\n",
    "        print(\"*\" * 50)\n",
    "        subject_id = subject['subject_id']\n",
    "        expertise_level = expertise_level_data.get(subject_id, None)\n",
    "        if expertise_level:\n",
    "            subject['expertise_level'] = expertise_level\n",
    "            print(f\"Updated expertise level for subject {subject_id} to {expertise_level}\")\n",
    "        else:\n",
    "            print(f\"Expertise level not found for subject {subject_id}\")\n",
    "\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(updated_json, json_file, indent=4)\n",
    "    print(f\"JSON structure saved to {output_file}\")\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove trials that has no keysteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove trials that have no keysteps\n",
    "if task == \"cpr_quality\":  \n",
    "    with open(output_file, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        updated_json = data\n",
    "        \n",
    "        # Initialize an empty list to hold subjects with non-empty trials\n",
    "        subjects_with_trials = []\n",
    "        \n",
    "        for subject in updated_json['subjects']:\n",
    "            print(\"*\" * 50)\n",
    "            for scenario in subject.get('scenarios', []):\n",
    "            \n",
    "                # Remove trials with no keysteps\n",
    "                scenario['trials'] = [trial for trial in scenario['trials'] if trial['keysteps']]\n",
    "                \n",
    "                if scenario['trials']:\n",
    "                    subjects_with_trials.append(subject)\n",
    "                    for trial in scenario['trials']:\n",
    "                        print(f\"Trial {trial['trial_id']} has keysteps\")\n",
    "                else:\n",
    "                    print(f\"Removing subject {subject['subject_id']} as all trials are empty\")\n",
    "            \n",
    "            # Update the JSON structure to only include subjects with non-empty trials\n",
    "            updated_json['subjects'] = subjects_with_trials\n",
    "            \n",
    "        # Save the modified JSON structure back to the file\n",
    "        with open(output_file, 'w') as json_file:\n",
    "            json.dump(updated_json, json_file, indent=4)\n",
    "        print(f\"JSON structure saved to {output_file}\")\n",
    "        print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill gaps of annotations with no_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_no_action_keysteps(trial):\n",
    "    keysteps = sorted(trial.get(\"keysteps\", []), key=lambda x: x[\"start_t\"])\n",
    "    if not keysteps:\n",
    "        return\n",
    "    no_action_class_id = 15\n",
    "    filled = []\n",
    "\n",
    "    for i in range(len(keysteps) - 1):\n",
    "        curr = keysteps[i]\n",
    "        nxt  = keysteps[i + 1]\n",
    "        filled.append(curr)\n",
    "        if nxt[\"start_t\"] > curr[\"end_t\"]:\n",
    "            filled.append({\n",
    "                \"keystep_id\": f\"no_action_{i}\",\n",
    "                \"start_t\":    curr[\"end_t\"],\n",
    "                \"end_t\":      nxt[\"start_t\"],\n",
    "                \"label\":      \"no_action\",\n",
    "                \"class_id\":   no_action_class_id\n",
    "            })\n",
    "\n",
    "    filled.append(keysteps[-1])\n",
    "    trial[\"keysteps\"] = filled\n",
    "\n",
    "def process_subject_trials(subject):\n",
    "    # now loop through scenarios first, then trials\n",
    "    for scenario in subject.get(\"scenarios\", []):\n",
    "        for trial in scenario.get(\"trials\", []):\n",
    "            fill_no_action_keysteps(trial)\n",
    "    return subject\n",
    "\n",
    "def process_all_subjects(data):\n",
    "    for subject in data.get(\"subjects\", []):\n",
    "        process_subject_trials(subject)\n",
    "    return data\n",
    "\n",
    "# if task == \"segmentation\":\n",
    "#     # Load the JSON data\n",
    "#     with open(output_file, \"r\") as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     # Process all subjects â†’ scenarios â†’ trials\n",
    "#     updated = process_all_subjects(data)\n",
    "\n",
    "#     # Save the updated JSON with filled \"no_action\" keysteps\n",
    "#     with open(output_file, \"w\") as f:\n",
    "#         json.dump(updated, f, indent=4)\n",
    "\n",
    "#     print(\"Gaps filled with 'no_action' keysteps successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove overlapping segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_keystep(last_keystep, current_keystep):\n",
    "    \"\"\"\n",
    "    Adjusts the last keystep by splitting it so that it does not overlap with the current keystep.\n",
    "    \"\"\"\n",
    "    if current_keystep['start_t'] < last_keystep['end_t']:\n",
    "        # Trim the end of the last keystep so it ends exactly when the current one begins\n",
    "        last_keystep['end_t'] = current_keystep['start_t']\n",
    "    return last_keystep\n",
    "\n",
    "def remove_and_split_overlapping_keysteps(trials):\n",
    "    \"\"\"\n",
    "    For each trial in the list, sort its keysteps by start time,\n",
    "    then trim any overlap between successive keysteps.\n",
    "    \"\"\"\n",
    "    for trial in trials:\n",
    "        keysteps = sorted(trial.get(\"keysteps\", []), key=lambda x: x['start_t'])\n",
    "        non_overlapping = []\n",
    "\n",
    "        for current in keysteps:\n",
    "            if not non_overlapping:\n",
    "                non_overlapping.append(current)\n",
    "            else:\n",
    "                last = non_overlapping[-1]\n",
    "                # If they overlap, split the last one\n",
    "                if current['start_t'] < last['end_t']:\n",
    "                    last = split_keystep(last, current)\n",
    "                    non_overlapping[-1] = last\n",
    "                non_overlapping.append(current)\n",
    "\n",
    "        trial[\"keysteps\"] = non_overlapping\n",
    "\n",
    "if task == \"segmentation\":\n",
    "    # Load the JSON file\n",
    "    with open(output_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Walk through subjects â†’ scenarios â†’ trials\n",
    "    for subject in data.get(\"subjects\", []):\n",
    "        for scenario in subject.get(\"scenarios\", []):\n",
    "            remove_and_split_overlapping_keysteps(scenario.get(\"trials\", []))\n",
    "\n",
    "    # Save the updated JSON back to disk\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(\"Overlapping keysteps have been removed and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load JSON and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON data from file\n",
    "with open(output_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Total subjects:\", len(data.get('subjects', [])))\n",
    "\n",
    "trial_count = 0\n",
    "keystep_count = 0\n",
    "\n",
    "trials_without_keysteps = []\n",
    "\n",
    "for subject in data.get('subjects', []):\n",
    "    print(\"*\" * 50)\n",
    "    subject_id = subject.get('subject_id')\n",
    "    scenarios = subject.get('scenarios', [])\n",
    "    print(f\"Subject ID: {subject_id}\")\n",
    "    print(f\"Total scenarios: {len(scenarios)}\")\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        scenario_id = scenario.get('scenario_id')\n",
    "        trials = scenario.get('trials', [])\n",
    "        print(f\" Scenario ID: {scenario_id}\")\n",
    "        print(f\"  Total trials: {len(trials)}\")\n",
    "        \n",
    "        for trial in trials:\n",
    "            trial_id = trial.get('trial_id')\n",
    "            keysteps = trial.get('keysteps', [])\n",
    "            print(f\"   Trial ID: {trial_id}\")\n",
    "            print(f\"    Total keysteps: {len(keysteps)}\")\n",
    "            trial_count += 1\n",
    "            keystep_count += len(keysteps)\n",
    "\n",
    "            if len(keysteps) == 0:\n",
    "                trials_without_keysteps.append(f\"{subject_id}/{scenario_id}/{trial_id}\")\n",
    "\n",
    "            # get the streams\n",
    "            streams = trial.get('streams', {})\n",
    "            print(f\"    Total streams: {len(streams)}\")\n",
    "            for stream_type, files in streams.items():\n",
    "                if isinstance(files, list):\n",
    "                    print(f\"     {stream_type}: {len(files)} files\")\n",
    "                else:\n",
    "                    print(f\"     {stream_type}: {files}\")\n",
    "\n",
    "    print(\"*\" * 50)\n",
    "\n",
    "print(\"Total trials:\", trial_count)\n",
    "print(\"Total keysteps:\", keystep_count)\n",
    "\n",
    "print(\"Trials without keysteps:\")\n",
    "for t in trials_without_keysteps:\n",
    "    print(\" -\", t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_keystep_distribution(subjects):\n",
    "    keysteps_data = []\n",
    "\n",
    "    # Extract keystep information from all subjects, scenarios, and trials\n",
    "    for subject in subjects:\n",
    "        for scenario in subject.get('scenarios', []):\n",
    "            for trial in scenario.get('trials', []):\n",
    "                for keystep in trial.get('keysteps', []):\n",
    "                    keysteps_data.append({\n",
    "                        'label':    keystep['label'],\n",
    "                        'class_id': keystep['class_id']\n",
    "                    })\n",
    "\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(keysteps_data)\n",
    "\n",
    "    # Get the distribution of keysteps by label and by class_id\n",
    "    label_distribution = df['label'].value_counts()\n",
    "    class_distribution = df['class_id'].value_counts()\n",
    "\n",
    "    return label_distribution, class_distribution\n",
    "\n",
    "# Load the JSON file\n",
    "with open(output_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Get the keystep distribution\n",
    "label_distribution, class_distribution = get_keystep_distribution(data['subjects'])\n",
    "\n",
    "# Display the results\n",
    "print(\"Keystep Distribution by Label:\")\n",
    "print(label_distribution)\n",
    "\n",
    "print(\"\\nKeystep Distribution by Class ID:\")\n",
    "print(class_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot keysteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_keysteps_for_trial(trial, subject_id, scenario_id):\n",
    "    keysteps_data = []\n",
    "\n",
    "    # Extract keystep information from the trial\n",
    "    for ks in trial.get('keysteps', []):\n",
    "        keysteps_data.append({\n",
    "            'label':    ks['label'],\n",
    "            'start':    ks['start_t'],\n",
    "            'end':      ks['end_t'],\n",
    "            'duration': ks['end_t'] - ks['start_t']\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame for easier plotting\n",
    "    df = pd.DataFrame(keysteps_data)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for _, row in df.iterrows():\n",
    "        plt.barh(row['label'], row['duration'], left=row['start'], height=0.4)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Keysteps')\n",
    "    plt.title(f'Subject {subject_id} Scenario {scenario_id} Trial {trial[\"trial_id\"]} Keystep Timeline')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_keysteps_for_all_trials(subjects):\n",
    "    for subject in subjects:\n",
    "        subject_id = subject.get('subject_id')\n",
    "        for scenario in subject.get('scenarios', []):\n",
    "            scenario_id = scenario.get('scenario_id')\n",
    "            for trial in scenario.get('trials', []):\n",
    "                plot_keysteps_for_trial(trial, subject_id, scenario_id)\n",
    "        break  # only plot first subject's trials\n",
    "\n",
    "# with open(output_file, 'r') as f:\n",
    "#     data = json.load(f)\n",
    "# plot_keysteps_for_all_trials(data.get('subjects', []))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Production Ready Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def strip_base_path(data, base_dir):\n",
    "    \"\"\"\n",
    "    Recursively remove the leading base_dir from all file_path entries.\n",
    "    e.g. '/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/P0/...' ->\n",
    "         'P0/...'\n",
    "    \"\"\"\n",
    "    base_dir = os.path.abspath(base_dir)\n",
    "\n",
    "    def _strip_path(path):\n",
    "        try:\n",
    "            abs_path = os.path.abspath(path)\n",
    "            if abs_path.startswith(base_dir):\n",
    "                rel = os.path.relpath(abs_path, base_dir)\n",
    "                return rel.replace(\"\\\\\", \"/\")\n",
    "            return path\n",
    "        except Exception:\n",
    "            return path\n",
    "\n",
    "    for subj in data.get(\"subjects\", []):\n",
    "        for scen in subj.get(\"scenarios\", []):\n",
    "            for trial in scen.get(\"trials\", []):\n",
    "                streams = trial.get(\"streams\", {})\n",
    "                for _name, item in streams.items():\n",
    "                    if isinstance(item, dict) and \"file_path\" in item:\n",
    "                        item[\"file_path\"] = _strip_path(item[\"file_path\"])\n",
    "    return data\n",
    "\n",
    "if is_production:\n",
    "    print(\"Stripping base paths for production deployment...\")\n",
    "    # Load JSON data from file\n",
    "    with open(output_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(\"Total subjects:\", len(data.get('subjects', [])))\n",
    "\n",
    "    data = strip_base_path(data, \"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026\")\n",
    "\n",
    "    # Save the updated JSON back to file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"Updated JSON with stripped paths saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egoexoems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
