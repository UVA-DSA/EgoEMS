{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Class-wise metrics with 95% bootstrap CIs + confusion partner + confidence ===\n",
    "# - Expects columns: keystep_id (int true), pred_keystep_id (int pred), all_preds (stringified logits [[...]])\n",
    "# - Produces: /mnt/data/metrics_out/classwise_metrics_with_CIs.csv and overall_summary.csv\n",
    "# - Minimal deps: pandas, numpy, scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48cfd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1687a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- CONFIG ----\n",
    "CSV_PATH = \"./results/model_id_job_1173491_task_classification_on_20250715-163830/preds.csv\"\n",
    "modality = \"resnet_ego_imu\"  # for output filenames\n",
    "\n",
    "N_BOOT = 300          # bootstrap resamples for CIs\n",
    "ALPHA = 0.05          # 95% CI\n",
    "RANDOM_SEED = 42\n",
    "MIN_SUPPORT = 1       # filter ultra-rare classes if desired (e.g., 5 or 10)\n",
    "\n",
    "# ---- HELPERS ----\n",
    "def safe_parse_logits(x: str) -> np.ndarray:\n",
    "    \"\"\"Parse '[[...]]' or '[...]' into 1D array of logits.\"\"\"\n",
    "    arr = np.array(ast.literal_eval(x), dtype=float)\n",
    "    if arr.ndim == 2 and arr.shape[0] == 1:\n",
    "        arr = arr[0]\n",
    "    return arr\n",
    "\n",
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    z = z - np.max(z)\n",
    "    e = np.exp(z)\n",
    "    return e / e.sum()\n",
    "\n",
    "def bootstrap_classwise_metrics(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, n_classes: int,\n",
    "    n_boot: int, alpha: float, rng: np.random.Generator\n",
    "):\n",
    "    \"\"\"Dataset-level bootstrap of per-class P/R/F1.\"\"\"\n",
    "    prec_hist = {k: [] for k in range(n_classes)}\n",
    "    rec_hist  = {k: [] for k in range(n_classes)}\n",
    "    f1_hist   = {k: [] for k in range(n_classes)}\n",
    "    sup_hist  = {k: [] for k in range(n_classes)}\n",
    "    n = len(y_true)\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        yt, yp = y_true[idx], y_pred[idx]\n",
    "        P, R, F1, S = precision_recall_fscore_support(\n",
    "            yt, yp, labels=list(range(n_classes)), zero_division=0\n",
    "        )\n",
    "        for k in range(n_classes):\n",
    "            prec_hist[k].append(P[k]); rec_hist[k].append(R[k])\n",
    "            f1_hist[k].append(F1[k]);  sup_hist[k].append(S[k])\n",
    "\n",
    "    qlo, qhi = 100 * (alpha/2), 100 * (1 - alpha/2)\n",
    "    ci_prec = {k: (np.percentile(prec_hist[k], qlo), np.percentile(prec_hist[k], qhi)) for k in range(n_classes)}\n",
    "    ci_rec  = {k: (np.percentile(rec_hist[k],  qlo), np.percentile(rec_hist[k],  qhi)) for k in range(n_classes)}\n",
    "    ci_f1   = {k: (np.percentile(f1_hist[k],   qlo), np.percentile(f1_hist[k],   qhi)) for k in range(n_classes)}\n",
    "    ci_sup  = {k: (np.percentile(sup_hist[k],  qlo), np.percentile(sup_hist[k],  qhi)) for k in range(n_classes)}\n",
    "    return ci_prec, ci_rec, ci_f1, ci_sup\n",
    "\n",
    "def top_confusion_partner(cm: np.ndarray) -> Dict[int, Tuple[int, int, float]]:\n",
    "    \"\"\"\n",
    "    For each true class i, find the most frequent *wrong* predicted class j.\n",
    "    Returns: i -> (j, count, frac_of_errors); if no errors -> (-1, 0, 0.0)\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for i in range(cm.shape[0]):\n",
    "        row = cm[i].copy()\n",
    "        row[i] = 0\n",
    "        errs = row.sum()\n",
    "        if errs == 0:\n",
    "            out[i] = (-1, 0, 0.0)\n",
    "        else:\n",
    "            j = int(np.argmax(row))\n",
    "            cnt = int(row[j])\n",
    "            out[i] = (j, cnt, float(cnt / errs))\n",
    "    return out\n",
    "\n",
    "def classwise_confidence_stats(y_true: np.ndarray, y_pred: np.ndarray, conf: np.ndarray, n_classes: int):\n",
    "    \"\"\"Median max-softmax confidence for correct vs incorrect per true class.\"\"\"\n",
    "    med_correct, med_incorrect = {}, {}\n",
    "    for k in range(n_classes):\n",
    "        mk = (y_true == k)\n",
    "        if not np.any(mk):\n",
    "            med_correct[k] = np.nan; med_incorrect[k] = np.nan\n",
    "            continue\n",
    "        conf_k = conf[mk]\n",
    "        ok  = conf_k[y_pred[mk] == k]\n",
    "        bad = conf_k[y_pred[mk] != k]\n",
    "        med_correct[k]   = float(np.median(ok))  if ok.size  else np.nan\n",
    "        med_incorrect[k] = float(np.median(bad)) if bad.size else np.nan\n",
    "    return med_correct, med_incorrect\n",
    "\n",
    "# ---- LOAD ----\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Truth/pred ids\n",
    "y_true = df[\"keystep_id\"].astype(int).values\n",
    "y_pred = df[\"pred_keystep_id\"].astype(int).values\n",
    "\n",
    "# Parse logits -> probs + max confidence\n",
    "logits_list = df[\"all_preds\"].apply(safe_parse_logits).tolist()\n",
    "num_classes = len(logits_list[0])\n",
    "probs = np.vstack([softmax(z) for z in logits_list])\n",
    "pred_from_logits = np.argmax(probs, axis=1)\n",
    "max_conf = np.max(probs, axis=1)\n",
    "\n",
    "# Label map (robust to repeats)\n",
    "id_to_label = {int(k): v[\"keystep_label\"].iloc[0] for k, v in df.groupby(\"keystep_id\")}\n",
    "\n",
    "# ---- METRICS ----\n",
    "P, R, F1, S = precision_recall_fscore_support(y_true, y_pred, labels=list(range(num_classes)), zero_division=0)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "top_conf = top_confusion_partner(cm)\n",
    "med_c, med_i = classwise_confidence_stats(y_true, y_pred, max_conf, num_classes)\n",
    "\n",
    "# Bootstrap CIs\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "ci_prec, ci_rec, ci_f1, ci_sup = bootstrap_classwise_metrics(y_true, y_pred, num_classes, N_BOOT, ALPHA, rng)\n",
    "\n",
    "# ---- REPORT TABLE ----\n",
    "rows = []\n",
    "for k in range(num_classes):\n",
    "    support = int(S[k])\n",
    "    if support < MIN_SUPPORT:\n",
    "        continue\n",
    "    label = id_to_label.get(k, f\"Class_{k}\")\n",
    "    j, cnt, frac = top_conf.get(k, (-1, 0, 0.0))\n",
    "    partner_label = id_to_label.get(j, \"—\") if j >= 0 else \"—\"\n",
    "    rows.append({\n",
    "        \"class_id\": k,\n",
    "        \"label\": label,\n",
    "        \"support\": support,\n",
    "        \"precision\": P[k],\n",
    "        \"precision_CI_low\": ci_prec[k][0],\n",
    "        \"precision_CI_high\": ci_prec[k][1],\n",
    "        \"recall\": R[k],\n",
    "        \"recall_CI_low\": ci_rec[k][0],\n",
    "        \"recall_CI_high\": ci_rec[k][1],\n",
    "        \"f1\": F1[k],\n",
    "        \"f1_CI_low\": ci_f1[k][0],\n",
    "        \"f1_CI_high\": ci_f1[k][1],\n",
    "        \"top_confused_with_id\": (j if j >= 0 else np.nan),\n",
    "        \"top_confused_with_label\": partner_label,\n",
    "        \"top_confusion_fraction_of_errors\": frac,\n",
    "        \"median_conf_correct\": med_c[k],\n",
    "        \"median_conf_incorrect\": med_i[k],\n",
    "    })\n",
    "\n",
    "report_df = pd.DataFrame(rows).sort_values(by=[\"f1\", \"support\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# ---- MACRO SUMMARY ----\n",
    "overall_acc = accuracy_score(y_true, y_pred)\n",
    "macro_p = np.nanmean(P)\n",
    "macro_r = np.nanmean(R)\n",
    "macro_f1 = np.nanmean(F1)\n",
    "mismatch_rate = float(np.mean(pred_from_logits != y_pred))\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"num_samples\": len(df),\n",
    "    \"num_classes\": num_classes,\n",
    "    \"overall_accuracy\": overall_acc,\n",
    "    \"macro_precision\": macro_p,\n",
    "    \"macro_recall\": macro_r,\n",
    "    \"macro_f1\": macro_f1,\n",
    "    \"pred_vs_logits_mismatch_rate\": mismatch_rate,\n",
    "    \"bootstrap_resamples\": N_BOOT,\n",
    "    \"alpha\": ALPHA\n",
    "}])\n",
    "\n",
    "# ---- SAVE + DISPLAY ----\n",
    "out_dir = Path(\"./analysis/\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "classwise_path = out_dir / f\"{modality}_classwise_metrics_with_CIs.csv\"\n",
    "summary_path = out_dir / f\"{modality}_overall_summary.csv\"\n",
    "report_df.to_csv(classwise_path, index=False)\n",
    "summary.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(f\"- {classwise_path}\")\n",
    "print(f\"- {summary_path}\")\n",
    "\n",
    "# Nice display in Jupyter if helper is available\n",
    "try:\n",
    "    from caas_jupyter_tools import display_dataframe_to_user\n",
    "    display_dataframe_to_user(\"Class-wise metrics with 95% bootstrap CI\", report_df)\n",
    "    display_dataframe_to_user(\"Overall summary\", summary)\n",
    "except Exception:\n",
    "    display(report_df.head(20))\n",
    "    display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_top_confusion_partners(report_df: pd.DataFrame,\n",
    "                                top_n: int = 10,\n",
    "                                support_min: int = 1,\n",
    "                                title: str = None):\n",
    "    \"\"\"\n",
    "    Plot the worst `top_n` classes by F1 and show, for each, the top confusion partner\n",
    "    and the fraction of that class's errors that go to that partner.\n",
    "    \n",
    "    Expected columns in report_df:\n",
    "      - 'label' (str): class name\n",
    "      - 'f1' (float)\n",
    "      - 'support' (int)\n",
    "      - 'top_confused_with_label' (str)\n",
    "      - 'top_confusion_fraction_of_errors' (float in [0,1])\n",
    "    \"\"\"\n",
    "    # Filter to classes with enough support (optional) and valid confusion info\n",
    "    df = report_df.copy()\n",
    "    df = df[df['support'] >= support_min]\n",
    "    df = df[~df['top_confused_with_label'].isna()]  # keep rows that have a partner\n",
    "    df = df.sort_values(['f1', 'support'], ascending=[True, False]).head(top_n)\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No classes meet the filter criteria (support_min, top_n, or missing columns).\")\n",
    "\n",
    "    # Build y labels like: \"<class> → <partner>\"\n",
    "    y_labels = df.apply(\n",
    "        lambda r: f\"{str(r['label'])} → {str(r['top_confused_with_label'])}\"\n",
    "                  if isinstance(r['top_confused_with_label'], str) else str(r['label']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    frac = df['top_confusion_fraction_of_errors'].astype(float).clip(lower=0, upper=1)\n",
    "    f1_vals = df['f1'].astype(float)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, max(4, 0.5 * len(df))))\n",
    "    y_pos = np.arange(len(df))[::-1]  # reverse so worst is at top visually\n",
    "    ax.barh(y_pos, frac, alpha=0.9)   # default matplotlib color; no explicit color\n",
    "\n",
    "    # Add value annotations (as percentages) and F1 text at the end of bars\n",
    "    for i, (v, f1) in enumerate(zip(frac.values[::-1], f1_vals.values[::-1])):\n",
    "        ypos = i\n",
    "        ax.text(v + 0.01, ypos, f\"{v*100:.0f}%\", va='center')\n",
    "        ax.text(min(0.98, max(0.02, v/2)), ypos, f\"F1={f1:.2f}\", va='center', ha='center', fontsize=9, color='white')\n",
    "\n",
    "    ax.set_yticks(np.arange(len(df)))\n",
    "    ax.set_yticklabels(list(y_labels.values)[::-1])\n",
    "    ax.set_xlabel(\"Fraction of this class's errors going to top confusion partner\")\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "    ttl = title or f\"Top Confusion Partners for Bottom {len(df)} Classes (by F1)\"\n",
    "    ax.set_title(ttl)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_top_confusion_partners_clean(report_df: pd.DataFrame,\n",
    "                                      top_n: int = 10,\n",
    "                                      support_min: int = 1,\n",
    "                                      title: str = None):\n",
    "    \"\"\"\n",
    "    Plot the top confusion partners for the bottom-N classes by F1,\n",
    "    showing only the percentage of errors to the top confusion partner (no F1 text).\n",
    "    \"\"\"\n",
    "    df = report_df.copy()\n",
    "    df = df[df['support'] >= support_min]\n",
    "    df = df[~df['top_confused_with_label'].isna()]\n",
    "    df = df.sort_values(['f1', 'support'], ascending=[True, False]).head(top_n)\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid classes to plot after filtering.\")\n",
    "\n",
    "    # Build y-axis labels: \"true → partner\"\n",
    "    y_labels = df.apply(\n",
    "        lambda r: f\"{str(r['label'])} → {str(r['top_confused_with_label'])}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    frac = df['top_confusion_fraction_of_errors'].astype(float).clip(0, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, max(4, 0.5 * len(df))))\n",
    "    y_pos = np.arange(len(df))[::-1]\n",
    "\n",
    "    # Bars\n",
    "    ax.barh(y_pos, frac, alpha=0.9)\n",
    "\n",
    "    # Annotate bars with percentages only\n",
    "    for i, v in enumerate(frac.values[::-1]):\n",
    "        ypos = i\n",
    "        ax.text(v + 0.01, ypos, f\"{v*100:.0f}%\", va='center')\n",
    "\n",
    "    ax.set_yticks(np.arange(len(df)))\n",
    "    ax.set_yticklabels(list(y_labels.values)[::-1])\n",
    "    ax.set_xlabel(\"Fraction of this class's errors going to top confusion partner\")\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "    ttl = title or f\"Top Confusion Partners for Bottom {len(df)} Classes (by F1)\"\n",
    "    ax.set_title(ttl)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# Example usage:\n",
    "# report_df = pd.read_csv(\"/mnt/data/metrics_out/classwise_metrics_with_CIs.csv\")\n",
    "# plot_top_confusion_partners(report_df, top_n=10, support_min=5)\n",
    "# plt.show()\n",
    "ego_report_df = pd.read_csv(\"./analysis/ego_classwise_metrics_with_CIs.csv\")\n",
    "\n",
    "plot_top_confusion_partners_clean(ego_report_df, top_n=10, support_min=5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd134738",
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_imu_report_df = pd.read_csv(\"./analysis/resnet_ego_imu_classwise_metrics_with_CIs.csv\")\n",
    "\n",
    "plot_top_confusion_partners_clean(ego_imu_report_df, top_n=10, support_min=5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_partner_reduction_stacked(\n",
    "    report_video: pd.DataFrame,\n",
    "    report_imu: pd.DataFrame,\n",
    "    top_n: int = 10,\n",
    "    support_min: int = 1,\n",
    "    min_errors: int = 3,          # require ≥ this many errors in BOTH; set 0 to keep all\n",
    "    key_on: str = \"class_id\",\n",
    "    rank_by: str = \"video\",       # \"video\" | \"max\" | \"delta\"\n",
    "    title: str = None,\n",
    "    zero_eps: float = 0.004       # tiny sliver so 0% bars are still visible\n",
    "):\n",
    "    \"\"\"Bars: Video-only (TOP), Video+IMU (BOTTOM). Δ=(IMU−Video) shown to the right of IMU bar.\"\"\"\n",
    "    # --- join & prep ---\n",
    "    cols = [key_on, \"label\", \"support\",\n",
    "            \"top_confused_with_label\", \"top_confusion_fraction_of_errors\",\n",
    "            \"f1\", \"recall\"]\n",
    "\n",
    "    A = report_video[cols].copy().rename(columns={\n",
    "        \"support\":\"support_video\",\n",
    "        \"top_confused_with_label\":\"partner_video\",\n",
    "        \"top_confusion_fraction_of_errors\":\"frac_video\",\n",
    "        \"f1\":\"f1_video\",\n",
    "        \"recall\":\"recall_video\",\n",
    "    })\n",
    "    B = report_imu[cols].copy().rename(columns={\n",
    "        \"support\":\"support_imu\",\n",
    "        \"top_confused_with_label\":\"partner_imu\",\n",
    "        \"top_confusion_fraction_of_errors\":\"frac_imu\",\n",
    "        \"f1\":\"f1_imu\",\n",
    "        \"recall\":\"recall_imu\",\n",
    "    })\n",
    "\n",
    "    M = pd.merge(A, B, on=[key_on, \"label\"], how=\"inner\")\n",
    "    # error counts\n",
    "    M[\"errors_video\"] = (M[\"support_video\"] * (1 - M[\"recall_video\"])).round().astype(\"Int64\")\n",
    "    M[\"errors_imu\"]   = (M[\"support_imu\"]   * (1 - M[\"recall_imu\"])).round().astype(\"Int64\")\n",
    "\n",
    "    # fractions\n",
    "    M[\"frac_video\"] = pd.to_numeric(M[\"frac_video\"], errors=\"coerce\").clip(0, 1)\n",
    "    M[\"frac_imu\"]   = pd.to_numeric(M[\"frac_imu\"],   errors=\"coerce\").clip(0, 1)\n",
    "\n",
    "    # filters\n",
    "    M = M[(M[\"support_video\"] >= support_min) & (M[\"support_imu\"] >= support_min)].copy()\n",
    "    if min_errors > 0:\n",
    "        M = M[(M[\"errors_video\"] >= min_errors) & (M[\"errors_imu\"] >= min_errors)].copy()\n",
    "        if M.empty:\n",
    "            raise ValueError(\"No classes after min_errors filter; lower min_errors or set to 0.\")\n",
    "\n",
    "    # Δ (negative = improvement with IMU)\n",
    "    M[\"delta\"] = (M[\"frac_imu\"] - M[\"frac_video\"]).astype(float)\n",
    "\n",
    "    # ranking\n",
    "    if rank_by == \"video\":\n",
    "        M = M.sort_values([\"frac_video\", \"f1_video\"], ascending=[False, True])\n",
    "    elif rank_by == \"max\":\n",
    "        M[\"frac_max\"] = M[[\"frac_video\",\"frac_imu\"]].max(axis=1)\n",
    "        M = M.sort_values([\"frac_max\"], ascending=False)\n",
    "    elif rank_by == \"delta\":\n",
    "        M = M.sort_values([\"delta\"], ascending=True)  # most negative (best reduction) first\n",
    "    else:\n",
    "        raise ValueError(\"rank_by must be one of {'video','max','delta'}\")\n",
    "\n",
    "    M = M.head(top_n).reset_index(drop=True)\n",
    "\n",
    "    # plotting arrays\n",
    "    n = len(M)\n",
    "    y = np.arange(n)*0.7          # we'll invert y-axis so 0 is visually at top\n",
    "    h = 0.14                  # small vertical offset so bars sit close\n",
    "\n",
    "    fv_true = M[\"frac_video\"].to_numpy(float)\n",
    "    fi_true = M[\"frac_imu\"].to_numpy(float)\n",
    "\n",
    "    # draw 0% bars as tiny slivers so both bars are always visible\n",
    "    fv_plot = np.where(np.isnan(fv_true) | (fv_true == 0.0), zero_eps, fv_true)\n",
    "    fi_plot = np.where(np.isnan(fi_true) | (fi_true == 0.0), zero_eps, fi_true)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, max(4.6, 0.55*n)))\n",
    "\n",
    "    # Video-only (TOP bar), Video+IMU (BOTTOM bar)\n",
    "    ax.barh(y - h, fv_plot, height=0.25, label=\"Video-only\")\n",
    "    ax.barh(y + h, fi_plot, height=0.25, label=\"Video + IMU\")\n",
    "\n",
    "    # annotate % on bar ends\n",
    "    for i in range(n):\n",
    "        ax.text(fv_plot[i] + 0.01, y[i] - h, f\"{fv_true[i]*100:.0f}%\", va=\"center\")\n",
    "        ax.text(fi_plot[i] + 0.01, y[i] + h, f\"{fi_true[i]*100:.0f}%\", va=\"center\")\n",
    "        # Δ to the RIGHT of the IMU bar\n",
    "        ax.text(min(1.0, fi_plot[i] + 0.1), y[i] + h, f\"Δ={(fi_true[i]-fv_true[i])*100:+.0f}%\",\n",
    "                va=\"center\", ha=\"left\", fontsize=9)\n",
    "\n",
    "    # labels & cosmetics\n",
    "    y_labels = M.apply(lambda r: f\"GT: {r['label']}\\nvideo→{r['partner_video']} | vid+IMU→{r['partner_imu']}\", axis=1)\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    ax.invert_yaxis()  # put first row at top\n",
    "    ax.set_xlabel(\"Fraction of this class's errors going to top confusion partner\")\n",
    "    ax.set_xlim(0, 1.09)  # little room for Δ text\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.35)\n",
    "    ax.set_title(title or \"Top Confusion Partner Concentration: Video (top) vs Video+IMU (bottom)\")\n",
    "    plt.tight_layout()\n",
    "    return fig, ax, M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_partner_reduction_stacked(ego_report_df, ego_imu_report_df, top_n=5, support_min=5, rank_by=\"delta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92040ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_worse_with_imu(\n",
    "    report_video: pd.DataFrame,\n",
    "    report_imu: pd.DataFrame,\n",
    "    top_k: int = 3,\n",
    "    support_min: int = 1,\n",
    "    min_errors: int = 3,          # require ≥ this many errors in BOTH to avoid 100% single-error artifacts\n",
    "    key_on: str = \"class_id\",\n",
    "    title: str = None,\n",
    "    zero_eps: float = 0.004,      # tiny sliver so 0% bars still visible\n",
    "    selected_keystep_ids: list = None  # NEW: list of specific keystep IDs to plot\n",
    "):\n",
    "    \"\"\"\n",
    "    Show the classes where adding IMU made confusion *more concentrated*:\n",
    "      Δ = (IMU − Video) on 'top_confusion_fraction_of_errors' > 0.\n",
    "    Plots the worst `top_k` classes with tight paired bars and Δ next to the IMU bar.\n",
    "\n",
    "    Expects each report to have columns:\n",
    "      [key_on, 'label', 'support', 'recall', 'top_confused_with_label', 'top_confusion_fraction_of_errors']\n",
    "    \n",
    "    Args:\n",
    "        selected_keystep_ids: Optional list of specific keystep IDs to plot. \n",
    "                             If provided, only these classes will be plotted (ignoring top_k).\n",
    "                             If None, plots top_k classes by delta.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- prepare & join ---\n",
    "    cols = [key_on, \"label\", \"support\", \"recall\",\n",
    "            \"top_confused_with_label\", \"top_confusion_fraction_of_errors\"]\n",
    "\n",
    "    A = report_video[cols].copy().rename(columns={\n",
    "        \"support\":\"support_video\",\n",
    "        \"recall\":\"recall_video\",\n",
    "        \"top_confused_with_label\":\"partner_video\",\n",
    "        \"top_confusion_fraction_of_errors\":\"frac_video\",\n",
    "    })\n",
    "    B = report_imu[cols].copy().rename(columns={\n",
    "        \"support\":\"support_imu\",\n",
    "        \"recall\":\"recall_imu\",\n",
    "        \"top_confused_with_label\":\"partner_imu\",\n",
    "        \"top_confusion_fraction_of_errors\":\"frac_imu\",\n",
    "    })\n",
    "\n",
    "    M = pd.merge(A, B, on=[key_on, \"label\"], how=\"inner\")\n",
    "\n",
    "    # convert labels to title case for better display and replace '_' with ' '\n",
    "    M[\"label\"] = M[\"label\"].str.replace('_', ' ').str.title()\n",
    "    M[\"partner_video\"] = M[\"partner_video\"].str.replace('_', ' ').str.title()\n",
    "    M[\"partner_imu\"] = M[\"partner_imu\"].str.replace('_', ' ').str.title()\n",
    "\n",
    "    # Compute error counts for filtering & annotation\n",
    "    M[\"errors_video\"] = (M[\"support_video\"] * (1 - M[\"recall_video\"])).round().astype(\"Int64\")\n",
    "    M[\"errors_imu\"]   = (M[\"support_imu\"]   * (1 - M[\"recall_imu\"])).round().astype(\"Int64\")\n",
    "\n",
    "    # Clean/clip fractions\n",
    "    M[\"frac_video\"] = pd.to_numeric(M[\"frac_video\"], errors=\"coerce\").clip(0, 1)\n",
    "    M[\"frac_imu\"]   = pd.to_numeric(M[\"frac_imu\"],   errors=\"coerce\").clip(0, 1)\n",
    "\n",
    "    # Basic filters\n",
    "    M = M[(M[\"support_video\"] >= support_min) & (M[\"support_imu\"] >= support_min)].copy()\n",
    "    if min_errors > 0:\n",
    "        M = M[(M[\"errors_video\"] >= min_errors) & (M[\"errors_imu\"] >= min_errors)].copy()\n",
    "    if M.empty:\n",
    "        raise ValueError(\"No classes after filters; relax min_errors/support_min.\")\n",
    "\n",
    "    # Δ > 0 means *worse with IMU* (more concentrated confusion)\n",
    "    M[\"delta\"] = (M[\"frac_imu\"] - M[\"frac_video\"]).astype(float)\n",
    "\n",
    "    # Filter to only positive deltas\n",
    "    M = M[M[\"delta\"] > 0].copy()\n",
    "    \n",
    "    if M.empty:\n",
    "        raise ValueError(\"No classes where IMU increased confusion concentration (Δ<=0 everywhere).\")\n",
    "    \n",
    "    # NEW: Select classes based on selected_keystep_ids or top_k\n",
    "    if selected_keystep_ids is not None:\n",
    "        # Filter to only the selected keystep IDs\n",
    "        W = M[M[key_on].isin(selected_keystep_ids)].copy()\n",
    "        if W.empty:\n",
    "            raise ValueError(f\"None of the selected keystep IDs {selected_keystep_ids} found in filtered data with Δ>0.\")\n",
    "        # Sort by delta descending for consistent ordering\n",
    "        W = W.sort_values(\"delta\", ascending=False).reset_index(drop=True)\n",
    "    else:\n",
    "        # Use top_k as before\n",
    "        W = M.sort_values(\"delta\", ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "    # --- plotting ---\n",
    "    n = len(W)\n",
    "    y = np.arange(n) * 0.5    # tighter spacing between class rows\n",
    "    h = 0.08                   # tight pairing separation\n",
    "\n",
    "    fv_true = W[\"frac_video\"].to_numpy(float)\n",
    "    fi_true = W[\"frac_imu\"].to_numpy(float)\n",
    "\n",
    "    # make zero bars visible as slivers\n",
    "    fv = np.where((~np.isfinite(fv_true)) | (fv_true == 0.0), zero_eps, fv_true)\n",
    "    fi = np.where((~np.isfinite(fi_true)) | (fi_true == 0.0), zero_eps, fi_true)\n",
    "\n",
    "    # Set seaborn style\n",
    "    # sns.set_style(\"whitegrid\")\n",
    "    # sns.set_context(\"paper\", font_scale=1.2)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(9, max(3.5, 0.5*n)))\n",
    "    sns.reset_orig()  # reset to original matplotlib params after seaborn set_context\n",
    "\n",
    "    # Get seaborn color palette\n",
    "    colors = sns.color_palette(\"tab10\", 2)\n",
    "    \n",
    "    # Video (top bar) and IMU (bottom bar) with seaborn colors\n",
    "    ax.barh(y - h, fv, height=0.16, label=\"Video-only\", color=colors[0])\n",
    "    ax.barh(y + h, fi, height=0.16, label=\"Video + IMU\", color=colors[1])\n",
    "\n",
    "    # annotate % and Δ to the right of bars, and predicted class ON the bars\n",
    "    for i in range(n):\n",
    "        # Video bar: percentage and predicted class\n",
    "        ax.text(fv[i] + 0.01, y[i] - h, f\"{fv_true[i]*100:.0f}%\", va=\"center\")\n",
    "        ax.text(fv[i]/2, y[i] - h, f\"{W.iloc[i]['partner_video']}\", \n",
    "                va=\"center\", ha=\"center\", fontsize=11, fontweight=\"bold\", color=\"white\")\n",
    "        \n",
    "        # IMU bar: percentage, predicted class, and delta\n",
    "        ax.text(fi[i] + 0.01, y[i] + h, f\"{fi_true[i]*100:.0f}%\", va=\"center\")\n",
    "        ax.text(fi[i]/2, y[i] + h, f\"{W.iloc[i]['partner_imu']}\", \n",
    "                va=\"center\", ha=\"center\", fontsize=11, fontweight=\"bold\", color=\"white\")\n",
    "        ax.text(min(1.0, fi[i] + 0.06), y[i] + h, f\"Δ={(fi_true[i]-fv_true[i])*100:+.0f}%\",\n",
    "                va=\"center\", ha=\"left\", fontsize=11, color='red', fontweight=\"bold\")\n",
    "\n",
    "    # y labels: show only GT class\n",
    "    y_labels = W[\"label\"]\n",
    "    # split long labels into two lines if needed\n",
    "    y_labels = y_labels.apply(lambda x: x if len(x) <= 7 else '\\n'.join(x.split(' ')[:len(x.split(' '))//2]) + '\\n' + ' '.join(x.split(' ')[len(x.split(' '))//2:]))\n",
    "\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels(y_labels, fontsize=11)\n",
    "    ax.invert_yaxis()\n",
    "    # set y-axis title\n",
    "    ax.set_ylabel(\"Ground Truth Class\", fontsize=14)\n",
    "\n",
    "    ax.set_xlabel(\"Confusion Concentration (Fraction of Errors)\", fontsize=14)\n",
    "    ax.set_xlim(0, 1.06)\n",
    "    ax.legend(loc=\"upper right\", frameon=True)\n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.35)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save figure\n",
    "    plt.savefig(\"./analysis/worse_with_imu_confusion_concentration.png\", dpi=300)\n",
    "    return fig, ax, W\n",
    "\n",
    "# Example usage:\n",
    "# Plot top 2 (default behavior)\n",
    "# plot_worse_with_imu(ego_report_df, ego_imu_report_df, top_k=2, support_min=3)\n",
    "\n",
    "# Plot specific keystep IDs\n",
    "# plot_worse_with_imu(ego_report_df, ego_imu_report_df, \n",
    "#                     selected_keystep_ids=[5, 12, 18], \n",
    "#                     support_min=3)\n",
    "plot_worse_with_imu(ego_report_df, ego_imu_report_df, top_k=2, support_min=3, selected_keystep_ids=[15,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe3b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_top_worse_classes_with_examples(\n",
    "    report_video: pd.DataFrame,\n",
    "    report_imu: pd.DataFrame,\n",
    "    preds_video: pd.DataFrame,\n",
    "    preds_imu: pd.DataFrame,\n",
    "    top_k: int = 3,\n",
    "    support_min: int = 1,\n",
    "    min_errors: int = 3,             # require ≥ this many errors in BOTH models\n",
    "    key_on: str = \"class_id\",\n",
    "    # prediction CSV column names (override if yours differ)\n",
    "    true_id_col: str = \"keystep_id\",\n",
    "    pred_id_col: str = \"pred_keystep_id\",\n",
    "    start_frame_col: str = \"start_frame\",\n",
    "    end_frame_col: str = \"end_frame\",\n",
    "    trial_col: str = \"trial_id\",\n",
    "    subject_col: str = \"subject_id\",\n",
    "    max_examples_per_class: int = 15\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      worst_classes: DataFrame with the top_k classes where IMU increased confusion concentration\n",
    "                     columns: [class_id,label,partner_video,frac_video,partner_imu,frac_imu,delta,errors_video,errors_imu]\n",
    "      examples:      DataFrame with rows from preds_imu for those classes, restricted to\n",
    "                     instances where IMU predicted the (IMU) top confusion partner.\n",
    "                     columns: [class_id,label,partner_imu,delta,keystep_id,start_frame,end_frame,trial_id,subject_id]\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- build comparison table (similar to earlier helpers) ----\n",
    "    cols = [key_on, \"label\", \"support\", \"recall\",\n",
    "            \"top_confused_with_label\", \"top_confusion_fraction_of_errors\"]\n",
    "\n",
    "    A = report_video[cols].copy().rename(columns={\n",
    "        \"support\":\"support_video\",\n",
    "        \"recall\":\"recall_video\",\n",
    "        \"top_confused_with_label\":\"partner_video\",\n",
    "        \"top_confusion_fraction_of_errors\":\"frac_video\",\n",
    "    })\n",
    "    B = report_imu[cols].copy().rename(columns={\n",
    "        \"support\":\"support_imu\",\n",
    "        \"recall\":\"recall_imu\",\n",
    "        \"top_confused_with_label\":\"partner_imu\",\n",
    "        \"top_confusion_fraction_of_errors\":\"frac_imu\",\n",
    "    })\n",
    "\n",
    "    M = pd.merge(A, B, on=[key_on, \"label\"], how=\"inner\")\n",
    "    # numeric clean-up\n",
    "    M[\"frac_video\"] = pd.to_numeric(M[\"frac_video\"], errors=\"coerce\").clip(0, 1)\n",
    "    M[\"frac_imu\"]   = pd.to_numeric(M[\"frac_imu\"],   errors=\"coerce\").clip(0, 1)\n",
    "    # error counts for filtering\n",
    "    M[\"errors_video\"] = (M[\"support_video\"] * (1 - M[\"recall_video\"])).round().astype(\"Int64\")\n",
    "    M[\"errors_imu\"]   = (M[\"support_imu\"]   * (1 - M[\"recall_imu\"])).round().astype(\"Int64\")\n",
    "\n",
    "    # basic filters\n",
    "    M = M[(M[\"support_video\"] >= support_min) & (M[\"support_imu\"] >= support_min)].copy()\n",
    "    if min_errors > 0:\n",
    "        M = M[(M[\"errors_video\"] >= min_errors) & (M[\"errors_imu\"] >= min_errors)].copy()\n",
    "\n",
    "    if M.empty:\n",
    "        raise ValueError(\"No shared classes after filters; relax support_min/min_errors.\")\n",
    "\n",
    "    # Δ > 0  => IMU increases confusion concentration (worse)\n",
    "    M[\"delta\"] = (M[\"frac_imu\"] - M[\"frac_video\"]).astype(float)\n",
    "    worst_classes = M[M[\"delta\"] > 0].sort_values(\"delta\", ascending=False).head(top_k).copy()\n",
    "\n",
    "    if worst_classes.empty:\n",
    "        raise ValueError(\"No classes where IMU increased confusion concentration (Δ <= 0).\")\n",
    "\n",
    "    # ---- map partner labels to IDs so we can pull examples from preds_imu ----\n",
    "    # build label->id mapping from either report (prefer IMU report)\n",
    "    label_to_id = dict(zip(report_imu[\"label\"], report_imu[key_on]))\n",
    "    # fallback if missing\n",
    "    label_to_id.update({row[\"label\"]: row[key_on] for _, row in report_video.iterrows() if row[\"label\"] not in label_to_id})\n",
    "\n",
    "    rows = []\n",
    "    for _, r in worst_classes.iterrows():\n",
    "        cls_id   = int(r[key_on])\n",
    "        cls_lab  = r[\"label\"]\n",
    "        partner_lab = r[\"partner_imu\"]\n",
    "        partner_id  = label_to_id.get(partner_lab, None)\n",
    "\n",
    "        # filter IMU predictions: true class == this class AND predicted == partner\n",
    "        df = preds_imu[preds_imu[true_id_col] == cls_id].copy()\n",
    "        if partner_id is not None and pred_id_col in df.columns:\n",
    "            df = df[df[pred_id_col] == partner_id]\n",
    "\n",
    "        # If still empty (e.g., partner_id not found), keep *all mispredictions* for the class\n",
    "        if df.empty and pred_id_col in preds_imu.columns:\n",
    "            df = preds_imu[(preds_imu[true_id_col] == cls_id) &\n",
    "                           (preds_imu[pred_id_col] != cls_id)].copy()\n",
    "\n",
    "        # select just the fields you want + context\n",
    "        keep_cols = [true_id_col, start_frame_col, end_frame_col, trial_col, subject_col]\n",
    "        missing = [c for c in keep_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            # degrade gracefully if any column names differ\n",
    "            present = [c for c in keep_cols if c in df.columns]\n",
    "            df = df[present].copy()\n",
    "        else:\n",
    "            df = df[keep_cols].copy()\n",
    "\n",
    "        # attach context for ease of triage\n",
    "        df.insert(0, \"partner_imu\", partner_lab)\n",
    "        df.insert(0, \"label\", cls_lab)\n",
    "        df.insert(0, key_on, cls_id)\n",
    "        df.insert(0, \"delta\", r[\"delta\"])\n",
    "\n",
    "        # cap the number of examples per class\n",
    "        if len(df) > max_examples_per_class:\n",
    "            df = df.head(max_examples_per_class)\n",
    "\n",
    "        rows.append(df)\n",
    "\n",
    "    examples = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "    # rename columns in output to exactly what you asked for\n",
    "    rename_map = {\n",
    "        true_id_col: \"keystep_id\",\n",
    "        start_frame_col: \"start_frame\",\n",
    "        end_frame_col: \"end_frame\",\n",
    "        trial_col: \"trial_id\",\n",
    "        subject_col: \"subject_id\",\n",
    "    }\n",
    "    examples = examples.rename(columns={k: v for k, v in rename_map.items() if k in examples.columns})\n",
    "\n",
    "    # reorder for readability if all requested columns are present\n",
    "    desired = [\"delta\", key_on, \"label\", \"partner_imu\", \"keystep_id\",\n",
    "               \"start_frame\", \"end_frame\", \"trial_id\", \"subject_id\"]\n",
    "    examples = examples[[c for c in desired if c in examples.columns]]\n",
    "\n",
    "    return worst_classes[[\n",
    "        key_on, \"label\", \"partner_video\", \"frac_video\",\n",
    "        \"partner_imu\", \"frac_imu\", \"delta\", \"errors_video\", \"errors_imu\"\n",
    "    ]], examples\n",
    "\n",
    "worst_classes, examples = get_top_worse_classes_with_examples(\n",
    "    ego_report_df,\n",
    "    ego_imu_report_df,\n",
    "    preds_video=pd.read_csv(\"./results/model_id_job_1173414_task_classification_on_20250715-145545/preds.csv\"),\n",
    "preds_imu=pd.read_csv(\"./results/model_id_job_1173491_task_classification_on_20250715-163830/preds.csv\"),\n",
    "    top_k=30,\n",
    "    support_min=1,\n",
    "    min_errors=1\n",
    ")\n",
    "\n",
    "print(\"Worst classes where IMU increased confusion concentration:\")\n",
    "display(worst_classes)\n",
    "print(\"\\nExamples from IMU model where it predicted the top confusion partner:\")\n",
    "display(examples)\n",
    "\n",
    "# save examples to CSV\n",
    "examples.to_csv(\"./analysis/worse_with_imu_examples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- helpers (include once, outside the loop) ---\n",
    "def _ensure_frame(df: pd.DataFrame, frame_col: str = \"frame\") -> pd.DataFrame:\n",
    "    if frame_col not in df.columns:\n",
    "        df = df.reset_index(drop=True)\n",
    "        df[frame_col] = np.arange(len(df), dtype=int)\n",
    "    return df\n",
    "\n",
    "def _acc_energy(df: pd.DataFrame, x=\"x_01\", y=\"y_01\", z=\"z_01\"):\n",
    "    if all(c in df.columns for c in (x, y, z)):\n",
    "        return np.sqrt(df[x]**2 + df[y]**2 + df[z]**2)\n",
    "    # fallback: try any accel-looking numeric columns\n",
    "    cand = [c for c in df.columns if c.lower() not in {\"timestamp\", \"frame\"}]\n",
    "    cand = [c for c in cand if df[c].dtype.kind in \"fc\"]\n",
    "    if len(cand) >= 3:\n",
    "        arr = df[cand[:3]].to_numpy(dtype=float)\n",
    "        return np.sqrt((arr**2).sum(axis=1))\n",
    "    return None\n",
    "\n",
    "def _zscore_rolling(x: pd.Series, win: int = 5) -> pd.Series:\n",
    "    x = (x - x.mean()) / (x.std() + 1e-8)\n",
    "    return x.rolling(win, center=True, min_periods=1).median()\n",
    "\n",
    "\n",
    "def analyze_imu_energy(imu_data: pd.DataFrame,\n",
    "                       imu_segment: pd.DataFrame,\n",
    "                       row: dict, loc: str = None):\n",
    "    \"\"\"\n",
    "    Analyze IMU accelerometer energy in a labeled segment vs full file.\n",
    "    Prints diagnostics and creates a plot with context.\n",
    "\n",
    "    Args:\n",
    "      imu_data:    Full IMU DataFrame with at least 'x_01','y_01','z_01' columns.\n",
    "      imu_segment: Segment DataFrame (subset of imu_data) for the labeled window.\n",
    "      row:         Dictionary with metadata about the segment (e.g., start_frame, end_frame).\n",
    "    \"\"\"\n",
    "    # ------------- inside your loop, REPLACE the plotting block with this -------------\n",
    "    # Make sure both full IMU file and this segment have 'frame'\n",
    "    imu_data = _ensure_frame(imu_data)\n",
    "    imu_segment = _ensure_frame(imu_segment)\n",
    "\n",
    "    # Compute accelerometer magnitude (energy proxy) for full file and the segment\n",
    "    E_full = _acc_energy(imu_data)\n",
    "    E_seg  = _acc_energy(imu_segment)\n",
    "\n",
    "    if E_full is None or E_seg is None:\n",
    "        print(\"⚠️ Could not compute accel energy (missing x_01/y_01/z_01).\")\n",
    "    else:\n",
    "        # Smooth & z-score\n",
    "        Ef = _zscore_rolling(pd.Series(E_full, index=imu_data.index))\n",
    "\n",
    "        # Diagnostics on the labeled segment (use segment indices)\n",
    "        Es = _zscore_rolling(pd.Series(E_seg,  index=imu_segment.index))\n",
    "        E = Es.to_numpy()\n",
    "        frames_seg = imu_segment[\"frame\"].to_numpy()\n",
    "\n",
    "        med = np.median(E); iqr = np.percentile(E, 75) - np.percentile(E, 25)\n",
    "        thr = med + 0.5 * (iqr if iqr > 1e-8 else 1.0)\n",
    "        frac_high = float((E > thr).mean()) if len(E) else np.nan\n",
    "        peak_rel = np.nan; peak_frame = None; peak_val = None\n",
    "        if len(E) > 0:\n",
    "            pk = int(np.argmax(E))\n",
    "            peak_rel = (frames_seg[pk] - frames_seg[0]) / max(1, (frames_seg[-1] - frames_seg[0]))\n",
    "            peak_frame = frames_seg[pk]\n",
    "            peak_val = Es.iloc[pk]\n",
    "\n",
    "        motion_level = \"LOW\" if frac_high < 0.25 else (\"MED\" if frac_high < 0.6 else \"HIGH\")\n",
    "        phase_hint   = \"EARLY\" if (peak_rel is not None and peak_rel < 0.33) else \\\n",
    "                    (\"MID\"   if (peak_rel is not None and peak_rel < 0.66) else \"LATE\")\n",
    "\n",
    "        print(f\"IMU energy → frac_high={frac_high:.2f}, peak_rel={peak_rel:.2f} (0=start,1=end)  \"\n",
    "            f\"→ Motion:{motion_level}, Peak:{phase_hint}\")\n",
    "\n",
    "        # ---- Single context plot with everything annotated ----\n",
    "        N = 30  # context frames on each side\n",
    "        s0, s1 = int(row.get('start_frame')), int(row.get('end_frame'))\n",
    "        f0 = max(0, s0 - N)\n",
    "        f1 = s1 + N\n",
    "\n",
    "        ctx = imu_data[(imu_data[\"frame\"] >= f0) & (imu_data[\"frame\"] <= f1)].copy()\n",
    "        Ec  = _zscore_rolling(pd.Series(_acc_energy(ctx), index=ctx.index))\n",
    "\n",
    "        # Set seaborn style\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.reset_orig()  # reset to original matplotlib params after seaborn set_context\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(9, 3))\n",
    "\n",
    "\n",
    "        # Get seaborn color palette\n",
    "        colors = sns.color_palette(\"tab10\")\n",
    "\n",
    "        # 1) plot context energy with seaborn color\n",
    "        ax.plot(ctx[\"frame\"], Ec, label=\"Accel energy (z, smoothed)\", color=colors[0], linewidth=1.5)\n",
    "\n",
    "        # 2) shade labeled window\n",
    "        ax.axvspan(s0, s1, color=colors[2], alpha=0.15, label=\"Labeled window\")\n",
    "\n",
    "        # 3) context threshold (robust)\n",
    "        ctx_med = np.nanmedian(Ec)\n",
    "        ctx_iqr = np.nanpercentile(Ec, 75) - np.nanpercentile(Ec, 25)\n",
    "        ctx_thr = ctx_med + 0.5 * (ctx_iqr if ctx_iqr > 1e-8 else 1.0)\n",
    "        # ax.axhline(ctx_thr, ls=\"--\", alpha=0.35, label=\"Context thr\")\n",
    "\n",
    "        # 4) segment start/end markers\n",
    "        ax.axvline(s0, color='k', lw=0.8, alpha=0.35)\n",
    "        ax.axvline(s1, color='k', lw=0.8, alpha=0.35)\n",
    "\n",
    "        # 5) segment-only threshold (horizontal)\n",
    "        # ax.axhline(thr, ls=\":\", alpha=0.6, label=\"Segment thr\")\n",
    "\n",
    "        # 6) peak marker if available\n",
    "        # if peak_frame is not None and peak_val is not None:\n",
    "        #     ax.plot([peak_frame], [peak_val+0.2], marker='o', ms=6)\n",
    "        #     ax.annotate(\"peak\", (peak_frame, peak_val+0.2),\n",
    "        #                 xytext=(peak_frame, peak_val + 0.6),\n",
    "        #                 arrowprops=dict(arrowstyle=\"->\", lw=0.8), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "        # 7) compact on-plot textbox with diagnostics\n",
    "        keystep_name = row['label'].replace('_', ' ').title()\n",
    "        partner_label = row.get('partner_imu', '?').replace('_', ' ').title()\n",
    "\n",
    "        #\n",
    "        txt = (f\"KS {row.get('keystep_id')} • {row.get('subject_id','?')}-trial:{row.get('trial_id','?')}\\n\"\n",
    "            f\"Frames {s0}-{s1} | Motion:{motion_level} | Peak:{phase_hint}\\n\"\n",
    "            f\"frac_high={frac_high:.2f} | peak_rel={peak_rel:.2f}\")\n",
    "        \n",
    "        if loc == \"upper left\":\n",
    "            ax.text(0.02, 0.95, txt, transform=ax.transAxes, va=\"top\", ha=\"left\",\n",
    "                bbox=dict(boxstyle=\"round,pad=0.35\", fc=\"white\", alpha=0.35, ec=\"0.2\"), fontsize=10)\n",
    "        if loc == \"middle\":\n",
    "            ax.text(0.33, 0.95, txt, transform=ax.transAxes, va=\"top\", ha=\"left\",\n",
    "                bbox=dict(boxstyle=\"round,pad=0.35\", fc=\"white\", alpha=0.35, ec=\"0.2\"), fontsize=10)\n",
    "\n",
    "        if loc == \"upper right\":\n",
    "            ax.text(0.95, 0.95, txt, transform=ax.transAxes, va=\"top\", ha=\"right\",\n",
    "                bbox=dict(boxstyle=\"round,pad=0.35\", fc=\"white\", alpha=0.35, ec=\"0.2\"), fontsize=10)\n",
    "\n",
    "        # ax.set_title(f\"Smartwatch IMU Energy • GT: {keystep_name} → PRED: {partner_label}\")\n",
    "        ax.set_title(f\"Smartwatch IMU Energy • GT: {keystep_name}\")\n",
    "        ax.set_xlabel(\"Frame\", fontsize=14)\n",
    "        ax.set_ylabel(\"Z-scored energy\", fontsize=14)\n",
    "        ax.legend(loc=\"lower left\", fontsize=11, ncol=2, frameon=True)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.25)\n",
    "        \n",
    "        # sns.despine(left=True, bottom=True)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # ---- save figure ----\n",
    "        os.makedirs(\"analysis\", exist_ok=True)\n",
    "        subj = str(row.get('subject_id','na')).replace('/', '-')\n",
    "        tri  = str(row.get('trial_id','na')).replace('/', '-')\n",
    "        ksid = str(row.get('keystep_id','na'))\n",
    "        fname = f\"{subj}_{tri}_ks{ksid}_{s0}-{s1}.png\"\n",
    "        out_path = os.path.join(\"analysis\", fname)\n",
    "        plt.show()\n",
    "        fig.savefig(out_path, dpi=150)\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved figure → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check examples to diagnose\n",
    "for _, row in examples.iterrows():\n",
    "    print(f\"Class ID: {row['class_id']}, Label: {row['label']}, Partner: {row['partner_imu']}, \"\n",
    "          f\"Keystep ID: {row['keystep_id']}, Trial: {row.get('trial_id', 'N/A')}, \"\n",
    "          f\"Subject: {row.get('subject_id', 'N/A')}, Frames: {row.get('start_frame', 'N/A')}-{row.get('end_frame', 'N/A')}\")\n",
    "    subject_id = row.get('subject_id', 'N/A')\n",
    "    trial_id = row.get('trial_id', 'N/A')\n",
    "\n",
    "    keystep_id = row['keystep_id']\n",
    "\n",
    "    print(f\"Loading IMU data for keystep ID {keystep_id} from subject {subject_id}, trial {trial_id}...\")\n",
    "\n",
    "    # if keystep_id == 63:\n",
    "    #     trial_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/{subject_id}/stroke/{trial_id}/\"\n",
    "    #     imu_path = trial_path + f\"smartwatch_data/{subject_id.replace('_','')}_stroke_{trial_id}_synchronized_smartwatch_01.csv\"\n",
    "\n",
    "    # if keystep_id == 25:\n",
    "    #     trial_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/{subject_id}/chest_pain/{trial_id}/\"\n",
    "    #     imu_path = trial_path + f\"smartwatch_data/{subject_id.replace('_','')}_chestpain_{trial_id}_sync_smartwatch.csv\"\n",
    "\n",
    "    if keystep_id == 26:\n",
    "        trial_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/{subject_id}/chest_pain/{trial_id}/\"\n",
    "        imu_path = trial_path + f\"smartwatch_data/{subject_id.replace('_','')}_chestpain_{trial_id}_sync_smartwatch.csv\"\n",
    "\n",
    "    # if keystep_id == 0: # approach patient\n",
    "    #     trial_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/{subject_id}/stroke/{trial_id}/\"\n",
    "    #     imu_path = trial_path + f\"smartwatch_data/{subject_id.replace('_','')}_stroke_{trial_id}_synchronized_smartwatch_02.csv\"\n",
    "\n",
    "    # if keystep_id == 0: # approach patient\n",
    "    #     trial_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/{subject_id}/stroke/{trial_id}/\"\n",
    "    #     imu_path = trial_path + f\"smartwatch_data/{subject_id.replace('_','')}_stroke_{trial_id}_sync_smartwatch.csv\"\n",
    "\n",
    "\n",
    "    # if keystep_id == 15: # no action\n",
    "    #     trial_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/{subject_id}/cardiac_arrest/{trial_id}/\"\n",
    "    #     imu_path = trial_path + f\"smartwatch_data/{subject_id.replace('_','')}_cardiacarrest_{trial_id}_sync_smartwatch.csv\"\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping keystep ID {keystep_id} for IMU analysis.\")\n",
    "        continue\n",
    "\n",
    "    # load IMU data\n",
    "    try:\n",
    "        imu_data = pd.read_csv(imu_path)\n",
    "        print(f\"Loaded IMU data from: {imu_path}\")\n",
    "        # get the relevant frames\n",
    "        start_frame = row.get('start_frame', None)\n",
    "        end_frame = row.get('end_frame', None)\n",
    "        if start_frame is not None and end_frame is not None:\n",
    "            # USE INDEX IF frame column does not exist\n",
    "            if 'frame' not in imu_data.columns:\n",
    "                imu_data = imu_data.reset_index().rename(columns={'index': 'frame'})\n",
    "            imu_segment = imu_data[(imu_data['frame'] >= start_frame) & (imu_data['frame'] <= end_frame)]\n",
    "            print(f\"IMU data segment for frames {start_frame}-{end_frame}:\")\n",
    "            # print(imu_segment)\n",
    "\n",
    "            # # basic statistics\n",
    "            # print(\"IMU segment statistics:\")\n",
    "\n",
    "            # # plot IMU data segment\n",
    "            # plt.figure(figsize=(10, 6))\n",
    "            # for col in imu_segment.columns:\n",
    "            #     if col != 'frame' and col != 'timestamp':\n",
    "            #         plt.plot(imu_segment['frame'], imu_segment[col], label=col)\n",
    "            # plt.title(f\"IMU Data Segment for Keystep ID {row['keystep_id']}\")\n",
    "            # plt.xlabel(\"Frame\")\n",
    "            # plt.ylabel(\"Sensor Values\")\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "\n",
    "            analyze_imu_energy(imu_data, imu_segment, row)\n",
    "        else:\n",
    "            print(\"Start or end frame missing; cannot extract segment.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load IMU data from: {imu_path}. Error: {e}\")\n",
    "    print(\"-----\")\n",
    "\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keystep 25 imu data segment\n",
    "subject_id = \"cars_2\"\n",
    "scenario_id = \"stroke\"\n",
    "trial_id = 0\n",
    "imu_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/{subject_id}/{scenario_id}/{trial_id}/smartwatch_data/{subject_id.replace('_','')}_{scenario_id}_{trial_id}_synchronized_smartwatch_01.csv\"\n",
    "\n",
    "ks_25_start = int(375.3253*30)  # convert to frames\n",
    "ks_25_end   = int(377.77643*30)\n",
    "\n",
    "imu_data_25 = pd.read_csv(imu_path)\n",
    "imu_data_25 = _ensure_frame(imu_data_25)\n",
    "imu_segment_25 = imu_data_25[(imu_data_25['frame'] >= ks_25_start) & (imu_data_25['frame'] <= ks_25_end)]\n",
    "\n",
    "analyze_imu_energy(imu_data_25, imu_segment_25, {\n",
    "    'keystep_id': 25,\n",
    "    'label': 'place_v3_lead',\n",
    "    'start_frame': ks_25_start,\n",
    "    'end_frame': ks_25_end,\n",
    "    'subject_id': subject_id,\n",
    "    'trial_id': trial_id,\n",
    "}, loc=\"upper left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "subject_id = \"ms1\"\n",
    "scenario_id = \"chest_pain\"\n",
    "trial_id = 6\n",
    "imu_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/ms1/chest_pain/6/smartwatch_data/ms1_chestpain_6_sync_smartwatch.csv\"\n",
    "\n",
    "# \"file_path\": \"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/cars_1/chest_pain/0/smartwatch_data/cars1_chestpain_0_sync_smartwatch.csv\"\n",
    "    # \"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/cars_1/chestpain/0/smartwatch_data/cars1_chestpain_0_sync_smartwatch.csv\"\n",
    "\n",
    "# /standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/cars1/chestpain/0/smartwatch_data/cars1_chestpain_0_sync_smartwatch.csv\n",
    "ks_26_start =  int(305.531*30)  \n",
    "ks_26_end   =  int(313.20165*30)\n",
    "\n",
    "imu_data_26 = pd.read_csv(imu_path)\n",
    "imu_data_26 = _ensure_frame(imu_data_26)\n",
    "imu_segment_26 = imu_data_26[(imu_data_26['frame'] >= ks_26_start) & (imu_data_26['frame'] <= ks_26_end)]   \n",
    "analyze_imu_energy(imu_data_26, imu_segment_26, {\n",
    "    'keystep_id': 26,\n",
    "    'label': 'place_v4_lead',\n",
    "    'start_frame': ks_26_start,\n",
    "    'end_frame': ks_26_end,\n",
    "    'subject_id': subject_id,\n",
    "    'trial_id': trial_id,\n",
    "}, loc=\"upper left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# {\n",
    "#     \"keystep_id\": \"2_2uGfna2P\",\n",
    "#     \"start_t\": 314.784,\n",
    "#     \"end_t\": 322.11136,\n",
    "#     \"label\": \"place_v3_lead\",\n",
    "#     \"class_id\": 25\n",
    "# },\n",
    "ks_25_start =  int( 314.784*30)  \n",
    "ks_25_end   =  int(322.11136*30)\n",
    "\n",
    "imu_data_25 = pd.read_csv(imu_path)\n",
    "imu_data_25 = _ensure_frame(imu_data_25)\n",
    "imu_segment_25 = imu_data_25[(imu_data_25['frame'] >= ks_25_start) & (imu_data_25['frame'] <= ks_25_end)]\n",
    "analyze_imu_energy(imu_data_25, imu_segment_25, {\n",
    "    'keystep_id': 25,\n",
    "    'label': 'place_v3_lead',\n",
    "    'start_frame': ks_25_start,\n",
    "    'end_frame': ks_25_end,\n",
    "    'subject_id': subject_id,\n",
    "    'trial_id': trial_id,\n",
    "}, loc=\"upper left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ks_15_start = int(26.251*30)\n",
    "# ks_15_end   = int(28.68495*30)\n",
    "# subject_id = \"cars_1\"\n",
    "# scenario_id = \"chest_pain\"\n",
    "# ks_15_label = \"no_action\"\n",
    "# trial_id = 0\n",
    "# imu_path = \"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/cars_1/chest_pain/0/smartwatch_data/cars1_chestpain_0_sync_smartwatch.csv\"\n",
    "# imu_data_15 = pd.read_csv(imu_path)\n",
    "# imu_data_15 = _ensure_frame(imu_data_15)\n",
    "# imu_segment_15 = imu_data_15[(imu_data_15['frame'] >= ks_15_start) & (imu_data_15['frame'] <= ks_15_end)]\n",
    "# analyze_imu_energy(imu_data_15, imu_segment_15, {\n",
    "#     'keystep_id': 15,\n",
    "#     'label': ks_15_label,\n",
    "#     'start_frame': ks_15_start,\n",
    "#     'end_frame': ks_15_end,\n",
    "#     'subject_id': subject_id,\n",
    "#     'trial_id': trial_id,\n",
    "# }, loc=\"upper right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cabea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------- helpers (define once) -----------------\n",
    "def _ensure_frame(df: pd.DataFrame, frame_col: str = \"frame\") -> pd.DataFrame:\n",
    "    \"\"\"Add a monotonically increasing 'frame' column if it's missing.\"\"\"\n",
    "    if frame_col not in df.columns:\n",
    "        df = df.reset_index(drop=True)\n",
    "        df[frame_col] = np.arange(len(df), dtype=int)\n",
    "    return df\n",
    "\n",
    "def _acc_energy(df: pd.DataFrame, x=\"x_01\", y=\"y_01\", z=\"z_01\") -> pd.Series:\n",
    "    \"\"\"Return √(x²+y²+z²); fallback to first three numeric, excluding timestamp/frame.\"\"\"\n",
    "    if all(c in df.columns for c in (x, y, z)):\n",
    "        return np.sqrt(df[x]**2 + df[y]**2 + df[z]**2)\n",
    "    cand = [c for c in df.columns if c.lower() not in {\"timestamp\", \"frame\"} and df[c].dtype.kind in \"fc\"]\n",
    "    if len(cand) >= 3:\n",
    "        arr = df[cand[:3]].to_numpy(dtype=float)\n",
    "        return np.sqrt((arr**2).sum(axis=1))\n",
    "    return None\n",
    "\n",
    "def _zscore_rolling(x: pd.Series, win: int = 5) -> pd.Series:\n",
    "    x = (x - x.mean()) / (x.std() + 1e-8)\n",
    "    return x.rolling(win, center=True, min_periods=1).median()\n",
    "\n",
    "def _motion_bin(frac_high: float) -> str:\n",
    "    return \"LOW\" if frac_high < 0.25 else (\"MED\" if frac_high < 0.6 else \"HIGH\")\n",
    "\n",
    "def _phase_bin(peak_rel: float ) -> str:\n",
    "    if peak_rel is None or np.isnan(peak_rel): \n",
    "        return \"N/A\"\n",
    "    return \"EARLY\" if peak_rel < 0.33 else (\"MID\" if peak_rel < 0.66 else \"LATE\")\n",
    "\n",
    "# ----------------- main plotting utility -----------------\n",
    "def plot_trial_confused_segments(\n",
    "    imu_path: str,\n",
    "    subject_id: str,\n",
    "    trial_id: str,\n",
    "    keystep_segments: list[dict],\n",
    "    context_pad: int = 30,         # frames before/after the window\n",
    "    dpi: int = 150,\n",
    "    out_dir: str = \"analysis\",\n",
    "    out_name: str = f\"{subject_id}_{trial_id}_segments.png\",   # default built from subject/trial\n",
    "):\n",
    "    \"\"\"\n",
    "    Make a single figure with one stacked subplot per segment, each showing:\n",
    "      - context energy (z-scored, smoothed),\n",
    "      - shaded labeled window,\n",
    "      - context threshold (median + 0.5*IQR over context),\n",
    "      - segment threshold (median + 0.5*IQR over segment),\n",
    "      - start/end vertical lines,\n",
    "      - peak marker + on-plot diagnostics textbox.\n",
    "\n",
    "    Saves to analysis/<subject>_<trial>_segments.png (or out_name if provided).\n",
    "    Returns fig, axes for optional further tweaking.\n",
    "    \"\"\"\n",
    "    # Load IMU\n",
    "    imu_data = pd.read_csv(imu_path)\n",
    "    print(f\"Loaded IMU data from: {imu_path}\")\n",
    "    imu_data = _ensure_frame(imu_data)\n",
    "\n",
    "    # Precompute full energy (to avoid recalculating)\n",
    "    E_full = _acc_energy(imu_data)\n",
    "    if E_full is None:\n",
    "        raise ValueError(\"Could not compute accel energy (missing x_01/y_01/z_01 and no numeric fallback).\")\n",
    "    Ef = _zscore_rolling(pd.Series(E_full, index=imu_data.index))\n",
    "\n",
    "    n = len(keystep_segments)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"keystep_segments is empty.\")\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n, ncols=1,\n",
    "        figsize=(11, max(3.5, 2.7 * n)),\n",
    "        sharex=False, sharey=False\n",
    "    )\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seg in zip(axes, keystep_segments):\n",
    "        ks = int(seg[\"keystep_id\"])\n",
    "        s0 = int(seg[\"start_frame\"])\n",
    "        s1 = int(seg[\"end_frame\"])\n",
    "\n",
    "        # Context slice\n",
    "        f0 = max(0, s0 - context_pad)\n",
    "        f1 = s1 + context_pad\n",
    "        ctx = imu_data[(imu_data[\"frame\"] >= f0) & (imu_data[\"frame\"] <= f1)].copy()\n",
    "        Ec  = _zscore_rolling(pd.Series(_acc_energy(ctx), index=ctx.index))\n",
    "\n",
    "        # Segment slice (within context for consistency)\n",
    "        seg_mask = (ctx[\"frame\"] >= s0) & (ctx[\"frame\"] <= s1)\n",
    "        seg_ctx = ctx[seg_mask].copy()\n",
    "        Es = _zscore_rolling(pd.Series(_acc_energy(seg_ctx), index=seg_ctx.index))\n",
    "\n",
    "        # Thresholds\n",
    "        # context threshold\n",
    "        ctx_med = np.nanmedian(Ec)\n",
    "        ctx_iqr = np.nanpercentile(Ec, 75) - np.nanpercentile(Ec, 25)\n",
    "        ctx_thr = ctx_med + 0.5 * (ctx_iqr if ctx_iqr > 1e-8 else 1.0)\n",
    "        # segment threshold\n",
    "        E = Es.to_numpy(dtype=float)\n",
    "        seg_med = np.nanmedian(E)\n",
    "        seg_iqr = np.nanpercentile(E, 75) - np.nanpercentile(E, 25)\n",
    "        seg_thr = seg_med + 0.5 * (seg_iqr if seg_iqr > 1e-8 else 1.0)\n",
    "\n",
    "        # Diagnostics\n",
    "        frac_high = float((E > seg_thr).mean()) if len(E) else np.nan\n",
    "        peak_rel = np.nan; peak_frame = None; peak_val = None\n",
    "        if len(E) > 0:\n",
    "            # detect peak on RAW magnitude within segment (more intuitive), but annotate at z-score height\n",
    "            E_raw = _acc_energy(seg_ctx)\n",
    "            pk = int(np.nanargmax(E_raw))\n",
    "            frames_seg = seg_ctx[\"frame\"].to_numpy()\n",
    "            peak_frame = frames_seg[pk]\n",
    "            peak_rel   = (peak_frame - frames_seg[0]) / max(1, (frames_seg[-1] - frames_seg[0]))\n",
    "            peak_val   = Es.iloc[pk]  # z-score value at peak frame for vertical position\n",
    "\n",
    "        motion_level = _motion_bin(frac_high)\n",
    "        phase_hint   = _phase_bin(peak_rel)\n",
    "\n",
    "        # --------- plot on one axes ---------\n",
    "        ax.plot(ctx[\"frame\"], Ec, label=\"Accel energy (z, smoothed)\")\n",
    "        ax.axvspan(s0, s1, color=\"orange\", alpha=0.15, label=\"Labeled window\")\n",
    "        ax.axhline(ctx_thr, ls=\"--\", alpha=0.35, label=\"Context thr\")\n",
    "        ax.axhline(seg_thr, ls=\":\",  alpha=0.6,  label=\"Segment thr\")\n",
    "        ax.axvline(s0, color='k', lw=0.8, alpha=0.35)\n",
    "        ax.axvline(s1, color='k', lw=0.8, alpha=0.35)\n",
    "\n",
    "        # Peak marker & annotation\n",
    "        # if (peak_frame is not None) and (peak_val is not None) and np.isfinite(peak_val):\n",
    "        #     ax.plot([peak_frame], [peak_val], marker='o', ms=5)\n",
    "        #     ax.annotate(\n",
    "        #         \"peak\",\n",
    "        #         (peak_frame, peak_val),\n",
    "        #         xytext=(peak_frame, peak_val + 0.35),\n",
    "        #         arrowprops=dict(arrowstyle=\"->\", lw=0.8),\n",
    "        #         ha=\"center\", va=\"bottom\"\n",
    "        #     )\n",
    "\n",
    "        # Diagnostics textbox\n",
    "        txt = (f\"KS {ks} • Frames {s0}-{s1} • Pad±{context_pad}\\n\"\n",
    "               f\"Motion:{motion_level} | Peak:{phase_hint}\\n\"\n",
    "               f\"frac_high={frac_high:.2f} | peak_rel={peak_rel:.2f}\")\n",
    "        ax.text(\n",
    "            0.01, 0.98, txt, transform=ax.transAxes, va=\"top\", ha=\"left\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.35\", fc=\"white\", alpha=0.85, ec=\"0.5\"),\n",
    "            fontsize=9\n",
    "        )\n",
    "\n",
    "        ax.set_ylabel(\"Z-scored energy\")\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.25)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Frame\")\n",
    "\n",
    "    # Put a single legend on the first axes only (cleaner)\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    axes[0].legend(handles, labels, loc=\"upper right\", fontsize=8, ncol=2)\n",
    "\n",
    "    title = f\"IMU Context Energy • {subject_id}/{trial_id} • {len(keystep_segments)} segments\"\n",
    "    fig.suptitle(title, y=0.995, fontsize=12)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    if out_name is None:\n",
    "        out_name = f\"{subject_id}_{trial_id}_segments.png\"\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    fig.savefig(out_path, dpi=dpi)\n",
    "    print(f\"Saved figure → {out_path}\")\n",
    "    return fig, axes\n",
    "\n",
    "# ----------------- example usage -----------------\n",
    "subject_id = \"ms1\"\n",
    "trial_id   = \"7\"\n",
    "trial_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/{subject_id}/chest_pain/{trial_id}/\"\n",
    "imu_path   = trial_path + f\"smartwatch_data/{subject_id.replace('_','')}_chestpain_{trial_id}_sync_smartwatch.csv\"\n",
    "\n",
    "# FIXED: dict keys must be strings\n",
    "keystep_segments = [\n",
    "    {\"keystep_id\": 25, \"start_frame\": 4471, \"end_frame\": 4691},\n",
    "    {\"keystep_id\": 26, \"start_frame\": 4100, \"end_frame\": 4218},\n",
    "]\n",
    "\n",
    "# run\n",
    "plot_trial_confused_segments(\n",
    "    imu_path=imu_path,\n",
    "    subject_id=subject_id,\n",
    "    trial_id=trial_id,\n",
    "    keystep_segments=keystep_segments,\n",
    "    context_pad=30,   # adjust if you want more/less context\n",
    "    dpi=150\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016413d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.9\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- helpers (raw, no post-processing) ----------\n",
    "def _ensure_frame(df: pd.DataFrame, frame_col: str = \"frame\") -> pd.DataFrame:\n",
    "    if frame_col not in df.columns:\n",
    "        df = df.reset_index(drop=True)\n",
    "        df[frame_col] = np.arange(len(df), dtype=int)\n",
    "    return df\n",
    "\n",
    "def _acc_magnitude(df: pd.DataFrame, x=\"sw_value_X_Axis\", y=\"sw_value_Y_Axis\", z=\"sw_value_Z_Axis\") -> np.ndarray:\n",
    "    if not all(c in df.columns for c in (x, y, z)):\n",
    "        raise ValueError(\"Expected accelerometer columns: sw_value_X_Axis, sw_value_Y_Axis, sw_value_Z_Axis.\")\n",
    "    arr = df[[x, y, z]].to_numpy(dtype=float)\n",
    "    return np.sqrt((arr ** 2).sum(axis=1))\n",
    "\n",
    "def _overlap_at_lag(a: np.ndarray, b: np.ndarray, lag: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n, m = len(a), len(b)\n",
    "    if lag >= 0:\n",
    "        L = min(n, m - lag)\n",
    "        if L <= 1: return np.array([]), np.array([])\n",
    "        return a[:L], b[lag:lag+L]\n",
    "    else:\n",
    "        shift = -lag\n",
    "        L = min(n - shift, m)\n",
    "        if L <= 1: return np.array([]), np.array([])\n",
    "        return a[shift:shift+L], b[:L]\n",
    "\n",
    "def _max_norm_xcorr(a: np.ndarray, b: np.ndarray, max_lag: int) -> Tuple[float, int]:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    best_corr, best_lag = -np.inf, 0\n",
    "    for lag in range(-max_lag, max_lag + 1):\n",
    "        aa, bb = _overlap_at_lag(a, b, lag)\n",
    "        if aa.size <= 1: continue\n",
    "        aa0 = aa - aa.mean()\n",
    "        bb0 = bb - bb.mean()\n",
    "        denom = (np.linalg.norm(aa0) * np.linalg.norm(bb0)) + 1e-8\n",
    "        corr = float(np.dot(aa0, bb0)) / denom\n",
    "        if corr > best_corr:\n",
    "            best_corr, best_lag = corr, lag\n",
    "    if not np.isfinite(best_corr):  # very short signals\n",
    "        best_corr, best_lag = 0.0, 0\n",
    "    return best_corr, best_lag\n",
    "\n",
    "def _cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    num = float(np.dot(a, b))\n",
    "    den = float(np.linalg.norm(a) * np.linalg.norm(b)) + 1e-8\n",
    "    return num / den\n",
    "\n",
    "def _dtw_distance_raw(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    n, m = len(a), len(b)\n",
    "    D = np.full((n + 1, m + 1), np.inf, dtype=float)\n",
    "    D[0, 0] = 0.0\n",
    "    for i in range(1, n + 1):\n",
    "        ai = a[i - 1]\n",
    "        for j in range(1, m + 1):\n",
    "            bj = b[j - 1]\n",
    "            cost = (ai - bj) ** 2\n",
    "            D[i, j] = cost + min(D[i - 1, j], D[i, j - 1], D[i - 1, j - 1])\n",
    "    return float(np.sqrt(D[n, m]) / (n + m))\n",
    "\n",
    "def _cosine_centered(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a0 = a - a.mean()\n",
    "    b0 = b - b.mean()\n",
    "    num = float(np.dot(a0, b0))\n",
    "    den = float(np.linalg.norm(a0) * np.linalg.norm(b0)) + 1e-8\n",
    "    return num / den  # == Pearson\n",
    "\n",
    "def _cosine_delta(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    # first differences (shortens by 1 sample)\n",
    "    if len(a) < 2 or len(b) < 2:\n",
    "        return np.nan\n",
    "    da = np.diff(a).astype(float)\n",
    "    db = np.diff(b).astype(float)\n",
    "    num = float(np.dot(da, db))\n",
    "    den = float(np.linalg.norm(da) * np.linalg.norm(db)) + 1e-8\n",
    "    return num / den\n",
    "\n",
    "\n",
    "# ---------- main (raw plotting + raw similarities) ----------\n",
    "def plot_trial_segment_similarity_raw(\n",
    "    imu_path: str,\n",
    "    subject_id: str,\n",
    "    trial_id: str,\n",
    "    keystep_segments: List[Dict[str, int]],\n",
    "    x_pad: int = 30,      # frames before/after for context plotting only\n",
    "    dpi: int = 150,\n",
    "    out_dir: str = \"analysis\",\n",
    "    out_name: str = None,\n",
    "    print_tables: bool = False,    # set True if you still want the CSV/tables\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot raw accelerometer magnitude for each segment in its context (±x_pad frames),\n",
    "    vertically stacked. No smoothing/z-scoring/resampling.\n",
    "\n",
    "    Only the FIRST subplot (reference) shows keystep identity (no similarity stats).\n",
    "    Subsequent subplots show Cosine and DTW vs the FIRST segment.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    imu = pd.read_csv(imu_path)\n",
    "    imu = _ensure_frame(imu)\n",
    "\n",
    "    # gather raw magnitudes and contexts\n",
    "    segments = []  # {'ks','ks_name','s0','s1','ctx_frames','mag_ctx','mag_seg'}\n",
    "    for seg in keystep_segments:\n",
    "        ks = int(seg[\"keystep_id\"])\n",
    "        s0 = int(seg[\"start_frame\"]); s1 = int(seg[\"end_frame\"])\n",
    "        if s1 < s0:\n",
    "            raise ValueError(f\"Bad frames for KS {ks}: end_frame < start_frame\")\n",
    "        f0 = max(0, s0 - x_pad); f1 = s1 + x_pad\n",
    "        ctx = imu[(imu[\"frame\"] >= f0) & (imu[\"frame\"] <= f1)].copy()\n",
    "        seg_df = imu[(imu[\"frame\"] >= s0) & (imu[\"frame\"] <= s1)].copy()\n",
    "        if seg_df.empty:\n",
    "            raise ValueError(f\"Empty segment for KS {ks}: frames {s0}-{s1}\")\n",
    "\n",
    "        mag_ctx = _acc_magnitude(ctx)        # raw\n",
    "        mag_seg = _acc_magnitude(seg_df)     # raw\n",
    "        segments.append({\n",
    "            \"ks\": ks,\n",
    "            \"ks_name\": seg.get(\"keystep_name\", f\"KS{ks}\"),\n",
    "            \"s0\": s0, \"s1\": s1,\n",
    "            \"ctx_frames\": ctx[\"frame\"].to_numpy(),\n",
    "            \"mag_ctx\": mag_ctx,\n",
    "            \"mag_seg\": mag_seg\n",
    "        })\n",
    "\n",
    "    n = len(segments)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"No segments provided.\")\n",
    "\n",
    "    # reference (first segment)\n",
    "    ref = segments[0][\"mag_seg\"]\n",
    "    ref_name = segments[0][\"ks_name\"]\n",
    "\n",
    "    # Optional: compute pairwise (only if printing/saving tables)\n",
    "    if print_tables:\n",
    "        labels = [f\"{d['ks_name']}:{d['s0']}-{d['s1']}\" for d in segments]\n",
    "        COS = np.zeros((n, n), dtype=float)\n",
    "        DTW = np.zeros((n, n), dtype=float)\n",
    "        for i in range(n):\n",
    "            ai = segments[i][\"mag_seg\"]\n",
    "            for j in range(n):\n",
    "                bj = segments[j][\"mag_seg\"]\n",
    "                # Cosine on overlapping region at best lag\n",
    "                max_lag = max(1, int(0.15 * max(len(ai), len(bj))))\n",
    "                _, lag = _max_norm_xcorr(ai, bj, max_lag=max_lag)\n",
    "                ao, bo = _overlap_at_lag(ai, bj, lag)\n",
    "                COS[i, j] = _cosine_similarity(ao, bo) if len(ao) > 1 else np.nan\n",
    "                DTW[i, j] = _dtw_distance_raw(ai, bj)\n",
    "        df_cos = pd.DataFrame(COS, index=labels, columns=labels)\n",
    "        df_dtw = pd.DataFrame(DTW, index=labels, columns=labels)\n",
    "        print(\"\\nPairwise COSINE (raw, overlap at best lag):\")\n",
    "        print(df_cos.round(3))\n",
    "        print(\"\\nPairwise DTW distance (raw; lower=more similar):\")\n",
    "        print(df_dtw.round(3))\n",
    "        # save CSVs\n",
    "        base = f\"{subject_id}_{trial_id}\"\n",
    "        df_cos.to_csv(os.path.join(out_dir, f\"{base}_sim_cosine_raw.csv\"))\n",
    "        df_dtw.to_csv(os.path.join(out_dir, f\"{base}_sim_dtw_raw.csv\"))\n",
    "\n",
    "    # figure\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(11, max(3.0, 2.5 * n)), sharex=False, sharey=False)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, (ax, seg) in enumerate(zip(axes, segments)):\n",
    "        ks_name = seg[\"ks_name\"]; s0 = seg[\"s0\"]; s1 = seg[\"s1\"]\n",
    "        frames = seg[\"ctx_frames\"]; mag_ctx = seg[\"mag_ctx\"]; mag_seg = seg[\"mag_seg\"]\n",
    "\n",
    "        # plot raw magnitude in context\n",
    "        ax.plot(frames, mag_ctx, label=\"Accel magnitude (raw)\")\n",
    "        ax.axvspan(s0, s1, color=\"orange\", alpha=0.15, label=\"Labeled window\")\n",
    "        ax.axvline(s0, color='k', lw=0.8, alpha=0.35)\n",
    "        ax.axvline(s1, color='k', lw=0.8, alpha=0.35)\n",
    "\n",
    "        if idx == 0:\n",
    "            # Reference only: keystep identity, no similarity numbers\n",
    "            txt = f\"{ks_name}  •  Frames {s0}-{s1}  (reference)\"\n",
    "        else:\n",
    "            # Similarity vs reference: Cosine + DTW only\n",
    "            max_lag = max(1, int(0.15 * max(len(mag_seg), len(ref))))\n",
    "            _, lag = _max_norm_xcorr(mag_seg, ref, max_lag=max_lag)\n",
    "            ao, bo = _overlap_at_lag(mag_seg, ref, lag)\n",
    "            cos = _cosine_similarity(ao, bo) if len(ao) > 1 else np.nan\n",
    "            dtw = _dtw_distance_raw(mag_seg, ref)\n",
    "\n",
    "            \n",
    "            txt = (f\"{ks_name}  •  Frames {s0}-{s1}\\n\"\n",
    "                   f\"vs {ref_name}  →  cos={cos:.2f}  |  dtw={dtw:.3f}\")\n",
    "\n",
    "        ax.text(0.01, 0.98, txt, transform=ax.transAxes, va=\"top\", ha=\"left\",\n",
    "                bbox=dict(boxstyle=\"round,pad=0.35\", fc=\"white\", alpha=0.85, ec=\"0.5\"),\n",
    "                fontsize=9)\n",
    "\n",
    "        ax.set_ylabel(\"Accel mag (raw)\")\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.25)\n",
    "\n",
    "    axes[-1].set_xlabel(\"Frame\")\n",
    "    handles, labels_ = axes[0].get_legend_handles_labels()\n",
    "    axes[0].legend(handles, labels_, loc=\"upper right\", fontsize=8, ncol=1)\n",
    "\n",
    "    title = f\"Raw Segment Similarity • {subject_id}/{trial_id} • {len(segments)} segments\"\n",
    "    fig.suptitle(title, y=0.995, fontsize=12)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    if out_name is None:\n",
    "        out_name = f\"{subject_id}_{trial_id}_segments_similarity_raw.png\"\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    fig.savefig(out_path, dpi=dpi)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved figure → {out_path}\")\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "\n",
    "subject_id = \"ms1\"\n",
    "trial_id   = \"7\"\n",
    "trial_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/{subject_id}/chest_pain/{trial_id}/\"\n",
    "imu_path   = trial_path + f\"smartwatch_data/{subject_id.replace('_','')}_chestpain_{trial_id}_sync_smartwatch.csv\"\n",
    "\n",
    "keystep_segments = [\n",
    "    {\"keystep_id\": 25, \"keystep_name\":\"place_v3_lead\", \"start_frame\": 4471, \"end_frame\": 4691},\n",
    "    {\"keystep_id\": 26, \"keystep_name\":\"place_v4_lead\", \"start_frame\": 4100, \"end_frame\": 4218},\n",
    "    # add more if needed…\n",
    "]\n",
    "\n",
    "plot_trial_segment_similarity_raw(\n",
    "    imu_path=imu_path,\n",
    "    subject_id=subject_id,\n",
    "    trial_id=trial_id,\n",
    "    keystep_segments=keystep_segments,\n",
    "    x_pad=30,   # context for plotting only\n",
    "    dpi=150,\n",
    "    out_dir=\"analysis\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_keystep_segments(csv_path: str, target_keysteps: List[int] = [25, 26]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the predictions CSV and extract segments belonging to specific keystep IDs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str\n",
    "        Path to your predictions CSV file.\n",
    "    target_keysteps : list[int]\n",
    "        List of keystep IDs to filter (default: [25, 26])\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Filtered DataFrame containing only the desired keysteps, with parsed predictions.\n",
    "    \"\"\"\n",
    "    # --- Load and basic cleanup ---\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if \"keystep_id\" not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'keystep_id' column.\")\n",
    "\n",
    "    # --- Filter by keystep IDs ---\n",
    "    df_filtered = df[df[\"keystep_id\"].isin(target_keysteps)].copy()\n",
    "\n",
    "    # --- Parse all_preds safely ---\n",
    "    def parse_preds(x):\n",
    "        try:\n",
    "            arr = ast.literal_eval(x)\n",
    "            return arr[0] if isinstance(arr, list) and len(arr) > 0 else []\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    df_filtered[\"parsed_preds\"] = df_filtered[\"all_preds\"].apply(parse_preds)\n",
    "\n",
    "    # --- Useful metadata fields ---\n",
    "    cols_keep = [\n",
    "        \"keystep_label\", \"keystep_id\", \"start_frame\", \"end_frame\",\n",
    "        \"subject_id\", \"trial_id\", \"pred_keystep_id\", \"parsed_preds\"\n",
    "    ]\n",
    "    existing_cols = [c for c in cols_keep if c in df_filtered.columns]\n",
    "    df_filtered = df_filtered[existing_cols]\n",
    "\n",
    "    # --- Convert frames to int for consistency ---\n",
    "    df_filtered[\"start_frame\"] = df_filtered[\"start_frame\"].astype(int)\n",
    "    df_filtered[\"end_frame\"]   = df_filtered[\"end_frame\"].astype(int)\n",
    "\n",
    "    print(f\"Extracted {len(df_filtered)} segments for keysteps {target_keysteps}.\")\n",
    "    print(df_filtered[[\"subject_id\", \"trial_id\", \"keystep_id\", \"start_frame\", \"end_frame\"]].head())\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def find_imu_data(subject_id, trial_id):\n",
    "    # Placeholder function to find IMU segments\n",
    "    # In practice, this would involve loading the IMU data and extracting the relevant segment\n",
    "\n",
    "    imu_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/{subject_id}/chest_pain/{trial_id}/smartwatch_data/\"\n",
    "    # check if imu_path exists\n",
    "    if not os.path.exists(imu_path):\n",
    "        imu_path = f\"/standard/UVA-DSA/NIST EMS Project Data/EgoEMS_AAAI2026/{subject_id}/stroke/{trial_id}/smartwatch_data/\"\n",
    "        if not os.path.exists(imu_path):\n",
    "            raise FileNotFoundError(f\"IMU data path not found for subject {subject_id}, trial {trial_id}.\")\n",
    "    # find the csv file in imu_path\n",
    "    imu_files = [f for f in os.listdir(imu_path) if f.endswith(\".csv\")]\n",
    "    if not imu_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {imu_path}.\")\n",
    "    \n",
    "\n",
    "    imu_csv = os.path.join(imu_path, imu_files[0])\n",
    "    imu_data = pd.read_csv(imu_csv)\n",
    "\n",
    "    # ensure frame column\n",
    "    imu_data = _ensure_frame(imu_data)\n",
    "    \n",
    "\n",
    "    return imu_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_csv_path = \"./results/model_id_job_1173491_task_classification_on_20250715-163830/preds.csv\"\n",
    "segments_25_26 = extract_keystep_segments(imu_csv_path, target_keysteps=[25, 26])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "ks_25_segments = []\n",
    "ks_26_segments = []\n",
    "\n",
    "# Example: iterate or export to inspect\n",
    "for _, row in segments_25_26.iterrows():\n",
    "    print(f\"KS{row.keystep_id} | {row.subject_id}/{row.trial_id} | \"\n",
    "          f\"Frames {row.start_frame}-{row.end_frame} | Pred→{row.pred_keystep_id}\")\n",
    "    \n",
    "    # find the imu data for this subject/trial\n",
    "    imu_data = find_imu_data(row.subject_id, row.trial_id)\n",
    "\n",
    "    imu_segment = imu_data[(imu_data['frame'] >= row.start_frame) & (imu_data['frame'] <= row.end_frame)]\n",
    "\n",
    "    if row.keystep_id == 25:\n",
    "        ks_25_segments.append(imu_segment)\n",
    "    elif row.keystep_id == 26:\n",
    "        ks_26_segments.append(imu_segment)\n",
    "\n",
    "#     print(len(imu_segment), \"frames in segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0604c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ---------- signal helpers ----------\n",
    "def _get_xyz_cols(df: pd.DataFrame) -> Tuple[str, str, str]:\n",
    "    print(df.columns)\n",
    "    # support both schemas\n",
    "    if all(c in df.columns for c in (\"x_01\",\"y_01\",\"z_01\")):\n",
    "        return \"x_01\",\"y_01\",\"z_01\"\n",
    "    if all(c in df.columns for c in (\"x_02\",\"y_02\",\"z_02\")):\n",
    "        return \"x_02\",\"y_02\",\"z_02\"\n",
    "    if all(c in df.columns for c in (\"x_03\",\"y_03\",\"z_03\")):\n",
    "        return \"x_03\",\"y_03\",\"z_03\"\n",
    "    if all(c in df.columns for c in (\"sw_value_X_Axis\",\"sw_value_Y_Axis\",\"sw_value_Z_Axis\")):\n",
    "        return \"sw_value_X_Axis\",\"sw_value_Y_Axis\",\"sw_value_Z_Axis\"\n",
    "    raise ValueError(\"IMU DataFrame missing expected accel columns.\")\n",
    "\n",
    "def _acc_mag(df: pd.DataFrame) -> np.ndarray:\n",
    "    x,y,z = _get_xyz_cols(df)\n",
    "    arr = df[[x,y,z]].to_numpy(dtype=float)\n",
    "    return np.sqrt((arr**2).sum(axis=1))\n",
    "\n",
    "def _overlap_at_lag(a: np.ndarray, b: np.ndarray, lag: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    a = np.asarray(a, float).ravel(); b = np.asarray(b, float).ravel()\n",
    "    n, m = len(a), len(b)\n",
    "    if lag >= 0:\n",
    "        L = min(n, m-lag)\n",
    "        if L <= 1: return np.array([]), np.array([])\n",
    "        return a[:L], b[lag:lag+L]\n",
    "    else:\n",
    "        sh = -lag\n",
    "        L = min(n-sh, m)\n",
    "        if L <= 1: return np.array([]), np.array([])\n",
    "        return a[sh:sh+L], b[:L]\n",
    "\n",
    "def _max_norm_xcorr(a: np.ndarray, b: np.ndarray, max_lag: int) -> Tuple[float,int]:\n",
    "    a = np.asarray(a, float).ravel(); b = np.asarray(b, float).ravel()\n",
    "    best_corr, best_lag = -np.inf, 0\n",
    "    for lag in range(-max_lag, max_lag+1):\n",
    "        aa, bb = _overlap_at_lag(a, b, lag)\n",
    "        if aa.size <= 1: continue\n",
    "        aa0 = aa - aa.mean(); bb0 = bb - bb.mean()\n",
    "        denom = (np.linalg.norm(aa0)*np.linalg.norm(bb0)) + 1e-8\n",
    "        corr = float(np.dot(aa0, bb0)) / denom\n",
    "        if corr > best_corr:\n",
    "            best_corr, best_lag = corr, lag\n",
    "    if not np.isfinite(best_corr): best_corr, best_lag = 0.0, 0\n",
    "    return best_corr, best_lag\n",
    "\n",
    "def _cosine_centered(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a0 = a - a.mean(); b0 = b - b.mean()\n",
    "    denom = (np.linalg.norm(a0)*np.linalg.norm(b0)) + 1e-8\n",
    "    return float(np.dot(a0,b0)) / denom\n",
    "\n",
    "def _dtw_raw(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    n, m = len(a), len(b)\n",
    "    D = np.full((n+1, m+1), np.inf); D[0,0] = 0.0\n",
    "    for i in range(1, n+1):\n",
    "        ai = a[i-1]\n",
    "        for j in range(1, m+1):\n",
    "            bj = b[j-1]\n",
    "            cost = (ai - bj)**2\n",
    "            D[i,j] = cost + min(D[i-1,j], D[i,j-1], D[i-1,j-1])\n",
    "    return float(np.sqrt(D[n,m])/(n+m))\n",
    "\n",
    "def _cliffs_delta(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    # rank-based effect size in [-1,1]\n",
    "    x = np.asarray(x); y = np.asarray(y)\n",
    "    x_sorted = np.sort(x); y_sorted = np.sort(y)\n",
    "    i = j = more = less = 0\n",
    "    nx, ny = len(x_sorted), len(y_sorted)\n",
    "    while i < nx and j < ny:\n",
    "        if x_sorted[i] > y_sorted[j]:\n",
    "            more += nx - i; j += 1\n",
    "        elif x_sorted[i] < y_sorted[j]:\n",
    "            less += ny - j; i += 1\n",
    "        else:\n",
    "            # handle ties by advancing both\n",
    "            ii, jj = i, j\n",
    "            while ii < nx and x_sorted[ii] == x_sorted[i]: ii += 1\n",
    "            while jj < ny and y_sorted[jj] == y_sorted[j]: jj += 1\n",
    "            ties_x = ii - i; ties_y = jj - j\n",
    "            i, j = ii, jj\n",
    "    return (more - less) / (nx*ny + 1e-8)\n",
    "\n",
    "# ---------- feature extraction for each segment ----------\n",
    "def _segment_features(seg_df: pd.DataFrame) -> dict:\n",
    "    mag = _acc_mag(seg_df)\n",
    "    if mag.size == 0:\n",
    "        return {\"mean\":np.nan,\"std\":np.nan,\"energy\":np.nan,\"peak_rel\":np.nan,\"frac_high\":np.nan}\n",
    "    med = np.median(mag); iqr = np.percentile(mag,75)-np.percentile(mag,25)\n",
    "    thr = med + 0.5*(iqr if iqr>1e-8 else 1.0)\n",
    "    peak_rel = float(np.argmax(mag) / max(1, (len(mag)-1)))\n",
    "    return {\n",
    "        \"mean\": float(mag.mean()),\n",
    "        \"std\": float(mag.std(ddof=0)),\n",
    "        \"energy\": float(np.sum(mag**2)),\n",
    "        \"peak_rel\": peak_rel,\n",
    "        \"frac_high\": float((mag > thr).mean())\n",
    "    }\n",
    "\n",
    "# ---------- compute distributions & similarities ----------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ... keep your helpers _get_xyz_cols, _acc_mag, _overlap_at_lag, _max_norm_xcorr,\n",
    "#     _cosine_centered, _dtw_raw, _cliffs_delta, _segment_features as-is ...\n",
    "\n",
    "def analyze_ks25_ks26(\n",
    "    ks25: List[pd.DataFrame],\n",
    "    ks26: List[pd.DataFrame],\n",
    "    print_examples: int = 5,\n",
    "    out_dir: str = \"analysis\"\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ---------- Per-segment features ----------\n",
    "    rows = []\n",
    "    for df in ks25:\n",
    "        f = _segment_features(df); f[\"keystep\"] = 25; rows.append(f)\n",
    "    for df in ks26:\n",
    "        f = _segment_features(df); f[\"keystep\"] = 26; rows.append(f)\n",
    "    feats = pd.DataFrame(rows)\n",
    "\n",
    "    def med_iqr_vals(x: np.ndarray) -> Tuple[float, float, float]:\n",
    "        x = x[~np.isnan(x)]\n",
    "        if x.size == 0:\n",
    "            return np.nan, np.nan, np.nan\n",
    "        return float(np.nanmedian(x)), float(np.nanpercentile(x,25)), float(np.nanpercentile(x,75))\n",
    "\n",
    "    # ---------- Build magnitude lists ----------\n",
    "    mags25 = [ _acc_mag(df) for df in ks25 ]\n",
    "    mags26 = [ _acc_mag(df) for df in ks26 ]\n",
    "\n",
    "    # ---------- Pairwise similarities ----------\n",
    "    def pairwise_stats(A: List[np.ndarray], B: List[np.ndarray]):\n",
    "        cos_c = []; dtw = []; pairs = []\n",
    "        for i, a in enumerate(A):\n",
    "            for j, b in enumerate(B):\n",
    "                max_lag = max(1, int(0.15*max(len(a), len(b))))\n",
    "                _, lag = _max_norm_xcorr(a, b, max_lag)\n",
    "                ao, bo = _overlap_at_lag(a, b, lag)\n",
    "                cos_c.append(_cosine_centered(ao, bo) if len(ao) > 1 else np.nan)\n",
    "                dtw.append(_dtw_raw(a, b))\n",
    "                pairs.append((i, j))\n",
    "        return np.array(cos_c), np.array(dtw), pairs\n",
    "\n",
    "    cos25_25, dtw25_25, _ = pairwise_stats(mags25, mags25)  # includes diagonal\n",
    "    cos26_26, dtw26_26, _ = pairwise_stats(mags26, mags26)\n",
    "    # remove diagonals (i==j)\n",
    "    def rm_diag(vals: np.ndarray, n: int) -> np.ndarray:\n",
    "        if n <= 1: return np.array([])\n",
    "        mask = np.ones(n*n, dtype=bool)\n",
    "        mask[np.arange(0, n*n, n+1)] = False\n",
    "        return vals[mask]\n",
    "    if len(mags25) > 1:\n",
    "        cos25_25 = rm_diag(cos25_25, len(mags25))\n",
    "        dtw25_25 = rm_diag(dtw25_25, len(mags25))\n",
    "    else:\n",
    "        cos25_25 = np.array([]); dtw25_25 = np.array([])\n",
    "    if len(mags26) > 1:\n",
    "        cos26_26 = rm_diag(cos26_26, len(mags26))\n",
    "        dtw26_26 = rm_diag(dtw26_26, len(mags26))\n",
    "    else:\n",
    "        cos26_26 = np.array([]); dtw26_26 = np.array([])\n",
    "\n",
    "    cos25_26, dtw25_26, pairs_25_26 = pairwise_stats(mags25, mags26)\n",
    "\n",
    "    # ---------- Summaries to DataFrame ----------\n",
    "    rows_summary = []\n",
    "    for name, cos_vals, dtw_vals in [\n",
    "        (\"Within KS25\", cos25_25, dtw25_25),\n",
    "        (\"Within KS26\", cos26_26, dtw26_26),\n",
    "        (\"Cross KS25↔KS26\", cos25_26, dtw25_26),\n",
    "    ]:\n",
    "        cos_med, cos_q25, cos_q75 = med_iqr_vals(cos_vals)\n",
    "        dtw_med, dtw_q25, dtw_q75 = med_iqr_vals(dtw_vals)\n",
    "        rows_summary.append({\n",
    "            \"group\": name,\n",
    "            \"n_pairs_cos\": int(np.sum(~np.isnan(cos_vals))),\n",
    "            \"centered_cos_median\": cos_med,\n",
    "            \"centered_cos_q25\": cos_q25,\n",
    "            \"centered_cos_q75\": cos_q75,\n",
    "            \"n_pairs_dtw\": int(np.sum(~np.isnan(dtw_vals))),\n",
    "            \"dtw_median\": dtw_med,\n",
    "            \"dtw_q25\": dtw_q25,\n",
    "            \"dtw_q75\": dtw_q75,\n",
    "        })\n",
    "    sim_summary_df = pd.DataFrame(rows_summary)\n",
    "    sim_summary_path = os.path.join(out_dir, \"ks25_ks26_similarity_summary.csv\")\n",
    "    sim_summary_df.to_csv(sim_summary_path, index=False)\n",
    "\n",
    "    # ---------- Effect sizes (Cliff’s δ: cross vs within) ----------\n",
    "    eff_rows = []\n",
    "    if cos25_25.size:\n",
    "        eff_rows.append({\n",
    "            \"metric\": \"centered_cosine\",\n",
    "            \"reference\": \"Within KS25\",\n",
    "            \"delta_cross_vs_within\": _cliffs_delta(cos25_26[~np.isnan(cos25_26)], cos25_25[~np.isnan(cos25_25)])\n",
    "        })\n",
    "    if cos26_26.size:\n",
    "        eff_rows.append({\n",
    "            \"metric\": \"centered_cosine\",\n",
    "            \"reference\": \"Within KS26\",\n",
    "            \"delta_cross_vs_within\": _cliffs_delta(cos25_26[~np.isnan(cos25_26)], cos26_26[~np.isnan(cos26_26)])\n",
    "        })\n",
    "    # For DTW (lower=more similar): flip sign so positive means cross is \"more similar\" than within\n",
    "    if dtw25_25.size:\n",
    "        eff_rows.append({\n",
    "            \"metric\": \"dtw\",\n",
    "            \"reference\": \"Within KS25\",\n",
    "            \"delta_cross_vs_within\": _cliffs_delta(-dtw25_26[~np.isnan(dtw25_26)], -dtw25_25[~np.isnan(dtw25_25)])\n",
    "        })\n",
    "    if dtw26_26.size:\n",
    "        eff_rows.append({\n",
    "            \"metric\": \"dtw\",\n",
    "            \"reference\": \"Within KS26\",\n",
    "            \"delta_cross_vs_within\": _cliffs_delta(-dtw25_26[~np.isnan(dtw25_26)], -dtw26_26[~np.isnan(dtw26_26)])\n",
    "        })\n",
    "    effect_sizes_df = pd.DataFrame(eff_rows)\n",
    "    effect_sizes_path = os.path.join(out_dir, \"ks25_ks26_effect_sizes.csv\")\n",
    "    effect_sizes_df.to_csv(effect_sizes_path, index=False)\n",
    "\n",
    "    # ---------- Top cross-class pairs ----------\n",
    "    k = min(print_examples, len(dtw25_26))\n",
    "    top_pairs_dtw_df = pd.DataFrame(columns=[\"ks25_index\",\"ks26_index\",\"dtw\"])\n",
    "    top_pairs_cos_df = pd.DataFrame(columns=[\"ks25_index\",\"ks26_index\",\"centered_cosine\"])\n",
    "    if k > 0:\n",
    "        idx_sorted_dtw = np.argsort(dtw25_26)[:k]\n",
    "        top_pairs_dtw_df = pd.DataFrame(\n",
    "            [{\"ks25_index\": int(pairs_25_26[idx][0]),\n",
    "              \"ks26_index\": int(pairs_25_26[idx][1]),\n",
    "              \"dtw\": float(dtw25_26[idx])} for idx in idx_sorted_dtw]\n",
    "        )\n",
    "        top_pairs_dtw_df.to_csv(os.path.join(out_dir, \"ks25_ks26_top_pairs_dtw.csv\"), index=False)\n",
    "\n",
    "        idx_sorted_cos = np.argsort(-cos25_26)[:k]\n",
    "        top_pairs_cos_df = pd.DataFrame(\n",
    "            [{\"ks25_index\": int(pairs_25_26[idx][0]),\n",
    "              \"ks26_index\": int(pairs_25_26[idx][1]),\n",
    "              \"centered_cosine\": float(cos25_26[idx])} for idx in idx_sorted_cos]\n",
    "        )\n",
    "        top_pairs_cos_df.to_csv(os.path.join(out_dir, \"ks25_ks26_top_pairs_cos.csv\"), index=False)\n",
    "\n",
    "    # Also keep your original per-segment features CSV write if desired elsewhere\n",
    "    return feats, sim_summary_df, effect_sizes_df, top_pairs_dtw_df, top_pairs_cos_df\n",
    "\n",
    "\n",
    "feats_df, sim_df, eff_df, top_dtw_df, top_cos_df = analyze_ks25_ks26(\n",
    "    ks_25_segments, ks_26_segments, print_examples=5, out_dir=\"analysis\"\n",
    ")\n",
    "\n",
    "feats_df.to_csv(\"analysis/ks25_ks26_segment_features.csv\", index=False)\n",
    "print(\"Wrote:\",\n",
    "      \"analysis/ks25_ks26_segment_features.csv,\",\n",
    "      \"analysis/ks25_ks26_similarity_summary.csv,\",\n",
    "      \"analysis/ks25_ks26_effect_sizes.csv,\",\n",
    "      \"analysis/ks25_ks26_top_pairs_dtw.csv,\",\n",
    "      \"analysis/ks25_ks26_top_pairs_cos.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egoexoems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
