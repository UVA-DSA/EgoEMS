{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size:  5686\n",
      "val dataset size:  1892\n",
      "test dataset size:  2477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/cjh9fw/conda/egoexoems/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/cjh9fw/conda/egoexoems/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_shape torch.Size([1, 30, 1024])\n",
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import *\n",
    "from scripts.config import DefaultArgsNamespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from datautils.ems import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Accurate seek is not implemented for pyav backend\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args = DefaultArgsNamespace()\n",
    "\n",
    "train_loader, val_loader, test_loader = eee_get_dataloaders(args)\n",
    "\n",
    "# # get input feature  size\n",
    "# dataiter = next(iter(train_loader))\n",
    "# batch =  dataiter\n",
    "\n",
    "\n",
    "# # Access the parsed arguments\n",
    "model, optimizer, criterion, device = init_model(args)# verbose_mode = args.verbose\n",
    "model = model.to(device)\n",
    "scheduler = StepLR(optimizer, step_size=args.learning_params[\"lr_drop\"], gamma=0.1)  # adjust parameters as needed\n",
    "\n",
    "dummy_input = torch.randn(1, 30, 1024) # batch, num_frames, channels, height, width\n",
    "\n",
    "dummy_input = dummy_input.to(args.device)\n",
    "\n",
    "dummy_output = model(dummy_input)   \n",
    "\n",
    "print(dummy_output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, modality, backbone):\n",
    "    # check the shape of the input tensor\n",
    "    output = None\n",
    "    if('video' in modality):\n",
    "        output = None\n",
    "        x = x['frames']\n",
    "        # extract resnet50 features\n",
    "        x = x.to(device)\n",
    "        x = backbone(x)\n",
    "        output = x\n",
    "    \n",
    "\n",
    "    elif ( 'flow' in modality and  'rgb' in modality):\n",
    "\n",
    "        # I3D features are already extracted\n",
    "        flow = x['flow'].float()\n",
    "        rgb = x['rgb'].float()\n",
    "        output = torch.cat((flow, rgb), dim=-1).float()\n",
    "\n",
    "    elif ('rgb' in modality):\n",
    "        # I3D features are already extracted\n",
    "        output = x['rgb'].float()\n",
    "\n",
    "    elif ('flow' in modality):\n",
    "        # I3D features are already extracted\n",
    "        output = x['flow'].float()\n",
    "\n",
    "    elif ('audio' in modality):\n",
    "        # Audio features are already extracted\n",
    "        output = x['audio'].float()\n",
    "\n",
    "    feature_size = output.shape[-1]\n",
    "    return output, feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality:  ['video', 'audio']\n",
      "Item index: 101, Clip index: 6\n",
      "Feature size:  2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/cjh9fw/conda/egoexoems/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/cjh9fw/conda/egoexoems/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item index: 587, Clip index: 2\n",
      "torch.Size([1, 30, 2048])\n",
      "x_shape torch.Size([1, 30, 2048])\n",
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "print(\"Modality: \", args.dataloader_params['modality'])\n",
    "\n",
    "# Find feature dimension\n",
    "input, feature_size = preprocess(next(iter(train_loader)), args.dataloader_params['modality'], model.extract_resnet)\n",
    "\n",
    "print(\"Feature size: \", feature_size)\n",
    "\n",
    "args.transformer_params['input_dim'] = feature_size\n",
    "\n",
    "model, optimizer, criterion, device = init_model(args)# verbose_mode = args.verbose\n",
    "model = model.to(device)\n",
    "scheduler = StepLR(optimizer, step_size=args.learning_params[\"lr_drop\"], gamma=0.1)  # adjust parameters as needed\n",
    "\n",
    "\n",
    "# # print one batch\n",
    "for i, batch in enumerate(train_loader):\n",
    "    input,feature_size = preprocess(batch, args.dataloader_params['modality'], model.extract_resnet)\n",
    "    print(input.shape)\n",
    "    output = model(input)\n",
    "    print(output.shape)\n",
    "    break\n",
    "\n",
    "# # Train the model\n",
    "# for epoch in range(args.learning_params[\"epochs\"]):\n",
    "#     train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, wandb_logger)\n",
    "#     val_loss = validate(model, val_loader, criterion, device, wandb_logger)\n",
    "#     scheduler.step()\n",
    "#     print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egoexoems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
