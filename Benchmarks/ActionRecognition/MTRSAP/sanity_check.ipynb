{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size:  781\n",
      "val dataset size:  226\n",
      "test dataset size:  425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/cjh9fw/conda/egoexoems/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/cjh9fw/conda/egoexoems/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import *\n",
    "from scripts.config import DefaultArgsNamespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from datautils.ems import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Accurate seek is not implemented for pyav backend\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args = DefaultArgsNamespace()\n",
    "\n",
    "train_loader, val_loader, test_loader = eee_get_dataloaders(args)\n",
    "\n",
    "# # get input feature  size\n",
    "# dataiter = next(iter(train_loader))\n",
    "# batch =  dataiter\n",
    "\n",
    "\n",
    "# # Access the parsed arguments\n",
    "model, optimizer, criterion, device = init_model(args)# verbose_mode = args.verbose\n",
    "model = model.to(device)\n",
    "scheduler = StepLR(optimizer, step_size=args.learning_params[\"lr_drop\"], gamma=0.1)  # adjust parameters as needed\n",
    "\n",
    "dummy_input = torch.randn(1, 30, 1024) # batch, num_frames, channels, height, width\n",
    "\n",
    "dummy_input = dummy_input.to(args.device)\n",
    "\n",
    "dummy_output = model(dummy_input)   \n",
    "\n",
    "print(dummy_output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "keysteps = args.dataloader_params['keysteps']\n",
    "out_classes = len(keysteps)\n",
    "print(out_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality:  ['audio']\n",
      "Feature size:  1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "modality = args.dataloader_params['modality']\n",
    "print(\"Modality: \", modality)\n",
    "\n",
    "# Find feature dimension\n",
    "input, feature_size, label = preprocess(next(iter(train_loader)), args.dataloader_params['modality'], model, device=device)\n",
    "print(\"Feature size: \", feature_size)\n",
    "\n",
    "# print(input[:,0,-3:])\n",
    "\n",
    "args.transformer_params['input_dim'] = feature_size\n",
    "args.transformer_params['output_dim'] = out_classes\n",
    "\n",
    "model, optimizer, criterion, device = init_model(args)# verbose_mode = args.verbose\n",
    "model = model.to(device)\n",
    "scheduler = StepLR(optimizer, step_size=args.learning_params[\"lr_drop\"], gamma=0.1)  # adjust parameters as needed\n",
    "\n",
    "wandb_logger = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"EgoExoEMS\",\n",
    "    group=\"Keystep Recognition\",\n",
    "    mode=\"disabled\",\n",
    "    name=\"Testing on EgoExoEMS with I3D RGB Features - ICRA Model\",\n",
    "    notes=\"initial attempt ICRA model with I3D RGB features\",\n",
    "    config={\n",
    "    \"args\": args,\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input: torch.Size([1, 1794, 1024])\n",
      "Model output: torch.Size([1, 31])\n",
      "Label: tensor([9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# # # print one batch\n",
    "for i, batch in enumerate(train_loader):\n",
    "    input,feature_size, label = preprocess(batch, args.dataloader_params['modality'], model, device=device)\n",
    "    print(\"Model input:\",input.shape)\n",
    "    output = model(input)\n",
    "    print(\"Model output:\",output.shape)\n",
    "    print(\"Label:\",label)\n",
    "    break\n",
    "\n",
    "# results_dir = './results'\n",
    "# # # # Train the model\n",
    "# for epoch in range(args.learning_params[\"epochs\"]):\n",
    "#     train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, wandb_logger, modality=modality)\n",
    "#     val_loss = validate(model, val_loader, criterion, device, wandb_logger, modality=modality)\n",
    "#     scheduler.step()\n",
    "#     print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "#     results = test_model(model, test_loader, criterion, device, wandb_logger, modality=modality, epoch=epoch, results_dir=results_dir)\n",
    "#     print(results)\n",
    "#     break\n",
    "\n",
    "# dummy_input = torch.randn(1, 3, 1024) # batch, num_frames, channels, height, width\n",
    "# dummy_input = dummy_input.to(args.device)\n",
    "# pred = model(dummy_input)\n",
    "# print(pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, feature_dim, out_channels, kernel_size=3):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=feature_dim, out_channels=512, kernel_size=kernel_size, stride=1, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # No change in temporal dimension\n",
    "            nn.Conv1d(in_channels=512, out_channels=256, kernel_size=kernel_size, stride=1, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # No change in temporal dimension\n",
    "            nn.Conv1d(in_channels=256, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)  # No change in temporal dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, feature_dim] -> need to permute for Conv1d\n",
    "        x = x.permute(0, 2, 1)  # [batch, feature_dim, seq_len]\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        # x = torch.mean(x, dim=-1)  # Global average pooling\n",
    "        return x\n",
    "\n",
    "feature_size = 1024\n",
    "hidden_dim = 256\n",
    "cnn_encoder = CNN_Encoder(feature_size, hidden_dim)\n",
    "\n",
    "# Test the model\n",
    "dummy_input = torch.randn(1, 30, 1024)  # batch, seq_len, feature_dim\n",
    "\n",
    "pred = cnn_encoder(dummy_input)\n",
    "\n",
    "print(pred.shape)  # [batch, 1]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egoexoems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
