{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjh9fw/conda/envs/egoems/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** ========== **********\n",
      "Loading dataloader for Segmentation task\n",
      "Splitting data to windows\n",
      "Class stats: {'chest_compressions': 1053, 'approach_patient': 12, 'review_medications': 18, 'inquire_substance_use': 9, 'check_symptom_duration': 12, 'document_lkw_time': 8, 'load_patient_to_stretcher': 75, 'secure_patient_on_stretcher': 32, 'notify_hospital_of_stroke_alert': 8, 'transport': 116, 'check_responsiveness': 28, 'check_vision_deficits': 6, 'assess_neglect_signs': 1, 'check_blood_pressure': 66, 'inquire_medication_anticoagulants': 4, 'handoff_patient_to_hospital': 27, 'check_breathing': 24, 'request_assistance': 5, 'request_aed': 6, 'attach_defib_pads': 103, 'no_action': 35, 'check_pulse': 24, 'turn_on_aed': 34, 'clear_for_analysis': 83, 'clear_for_shock': 40, 'administer_shock_aed': 34, 'open_airway': 2, 'ventilate_patient': 140, 'assess_patient': 53, 'explain_procedure': 8, 'place_right_arm_lead': 9, 'place_left_arm_lead': 11, 'place_right_leg_lead': 8, 'place_left_leg_lead': 9, 'place_v2_lead': 6, 'place_v1_lead': 8, 'place_v3_lead': 3, 'place_v6_lead': 9, 'place_v5_lead': 7, 'obtain_ecg_recording': 44, 'request_patient_to_not_move': 5, 'place_bvm': 54, 'check_a&o': 3, 'document_hpi_and_pmh': 8, 'check_skin_condition': 1, 'check_perrl': 4, 'face_droop_check': 2, 'arm_drift_check': 22, 'speech_abnormality_check': 5, 'inquire_hpi_and_pmh': 19, 'interpret_and_report': 15, 'place_v4_lead': 9, 'check_oxygen_saturation': 3, 'ask_patient_age_sex': 4, 'prepare_glucometer_and_strip': 25, 'read_and_record_glucose_level': 12, 'check_heart_rate': 1, 'check_respiratory_rate': 4, 'notify_hospital': 17, 'check_grip_strength': 3, 'connect_leads_to_ecg': 7}\n",
      "Total windowed clips: 2511\n",
      "Splitting data to windows\n",
      "Class stats: {'chest_compressions': 283, 'check_pulse': 9, 'request_aed': 2, 'attach_defib_pads': 32, 'clear_for_analysis': 31, 'clear_for_shock': 10, 'administer_shock_aed': 11, 'place_bvm': 13, 'ventilate_patient': 41, 'check_responsiveness': 9, 'turn_on_aed': 7, 'no_action': 9, 'check_breathing': 8, 'check_oxygen_saturation': 1, 'prepare_glucometer_and_strip': 10, 'load_patient_to_stretcher': 5, 'secure_patient_on_stretcher': 2, 'transport': 4, 'notify_hospital': 8, 'assess_patient': 16, 'explain_procedure': 1, 'place_left_arm_lead': 1, 'place_right_arm_lead': 1, 'place_left_leg_lead': 2, 'place_v1_lead': 4, 'place_v4_lead': 1, 'place_v3_lead': 2, 'place_v5_lead': 1, 'request_patient_to_not_move': 1, 'obtain_ecg_recording': 9, 'interpret_and_report': 5}\n",
      "Total windowed clips: 564\n",
      "Splitting data to windows\n",
      "Class stats: {'chest_compressions': 635, 'check_blood_pressure': 165, 'inquire_hpi_and_pmh': 38, 'check_oxygen_saturation': 17, 'explain_procedure': 37, 'place_v1_lead': 22, 'place_v2_lead': 20, 'place_v4_lead': 17, 'place_v3_lead': 18, 'place_v6_lead': 20, 'place_left_arm_lead': 19, 'place_right_leg_lead': 17, 'place_left_leg_lead': 17, 'request_patient_to_not_move': 12, 'shave_patient': 5, 'interpret_and_report': 37, 'obtain_ecg_recording': 88, 'secure_patient_on_stretcher': 138, 'check_heart_rate': 6, 'handoff_patient_to_hospital': 282, 'check_a&o': 2, 'face_droop_check': 17, 'inquire_substance_use': 8, 'ask_patient_age_sex': 27, 'check_perrl': 11, 'check_vision_deficits': 15, 'check_grip_strength': 20, 'evaluate_aphasia': 9, 'prepare_glucometer_and_strip': 51, 'read_and_record_glucose_level': 13, 'transport': 420, 'load_patient_to_stretcher': 131, 'notify_hospital': 71, 'suction_airway': 17, 'approach_patient': 22, 'check_symptom_duration': 18, 'check_pulse': 48, 'speech_abnormality_check': 8, 'arm_drift_check': 11, 'document_lkw_time': 28, 'review_medications': 25, 'assess_neglect_signs': 1, 'request_assistance': 6, 'notify_hospital_of_stroke_alert': 108, 'check_respiratory_rate': 19, 'place_right_arm_lead': 22, 'check_responsiveness': 32, 'check_breathing': 12, 'document_hpi_and_pmh': 27, 'place_v5_lead': 11, 'turn_on_ecg': 2, 'assess_patient': 104, 'inquire_medication_anticoagulants': 4, 'inset_NPA': 9, 'check_skin_condition': 1, 'turn_on_aed': 19, 'attach_defib_pads': 49, 'clear_for_analysis': 47, 'clear_for_shock': 26, 'administer_shock_aed': 17, 'ventilate_patient': 72, 'connect_leads_to_ecg': 8, 'place_bvm': 31, 'open_airway': 2, 'no_action': 22, 'request_aed': 3, 'assess_balance_and_coordination': 3}\n",
      "Total windowed clips: 3332\n",
      "Train class stats:  {'chest_compressions': 1053, 'approach_patient': 12, 'review_medications': 18, 'inquire_substance_use': 9, 'check_symptom_duration': 12, 'document_lkw_time': 8, 'load_patient_to_stretcher': 75, 'secure_patient_on_stretcher': 32, 'notify_hospital_of_stroke_alert': 8, 'transport': 116, 'check_responsiveness': 28, 'check_vision_deficits': 6, 'assess_neglect_signs': 1, 'check_blood_pressure': 66, 'inquire_medication_anticoagulants': 4, 'handoff_patient_to_hospital': 27, 'check_breathing': 24, 'request_assistance': 5, 'request_aed': 6, 'attach_defib_pads': 103, 'no_action': 35, 'check_pulse': 24, 'turn_on_aed': 34, 'clear_for_analysis': 83, 'clear_for_shock': 40, 'administer_shock_aed': 34, 'open_airway': 2, 'ventilate_patient': 140, 'assess_patient': 53, 'explain_procedure': 8, 'place_right_arm_lead': 9, 'place_left_arm_lead': 11, 'place_right_leg_lead': 8, 'place_left_leg_lead': 9, 'place_v2_lead': 6, 'place_v1_lead': 8, 'place_v3_lead': 3, 'place_v6_lead': 9, 'place_v5_lead': 7, 'obtain_ecg_recording': 44, 'request_patient_to_not_move': 5, 'place_bvm': 54, 'check_a&o': 3, 'document_hpi_and_pmh': 8, 'check_skin_condition': 1, 'check_perrl': 4, 'face_droop_check': 2, 'arm_drift_check': 22, 'speech_abnormality_check': 5, 'inquire_hpi_and_pmh': 19, 'interpret_and_report': 15, 'place_v4_lead': 9, 'check_oxygen_saturation': 3, 'ask_patient_age_sex': 4, 'prepare_glucometer_and_strip': 25, 'read_and_record_glucose_level': 12, 'check_heart_rate': 1, 'check_respiratory_rate': 4, 'notify_hospital': 17, 'check_grip_strength': 3, 'connect_leads_to_ecg': 7}\n",
      "Train Number of classes:  61\n",
      "val class stats:  {'chest_compressions': 283, 'check_pulse': 9, 'request_aed': 2, 'attach_defib_pads': 32, 'clear_for_analysis': 31, 'clear_for_shock': 10, 'administer_shock_aed': 11, 'place_bvm': 13, 'ventilate_patient': 41, 'check_responsiveness': 9, 'turn_on_aed': 7, 'no_action': 9, 'check_breathing': 8, 'check_oxygen_saturation': 1, 'prepare_glucometer_and_strip': 10, 'load_patient_to_stretcher': 5, 'secure_patient_on_stretcher': 2, 'transport': 4, 'notify_hospital': 8, 'assess_patient': 16, 'explain_procedure': 1, 'place_left_arm_lead': 1, 'place_right_arm_lead': 1, 'place_left_leg_lead': 2, 'place_v1_lead': 4, 'place_v4_lead': 1, 'place_v3_lead': 2, 'place_v5_lead': 1, 'request_patient_to_not_move': 1, 'obtain_ecg_recording': 9, 'interpret_and_report': 5}\n",
      "Val Number of classes:  31\n",
      "test class stats:  {'chest_compressions': 635, 'check_blood_pressure': 165, 'inquire_hpi_and_pmh': 38, 'check_oxygen_saturation': 17, 'explain_procedure': 37, 'place_v1_lead': 22, 'place_v2_lead': 20, 'place_v4_lead': 17, 'place_v3_lead': 18, 'place_v6_lead': 20, 'place_left_arm_lead': 19, 'place_right_leg_lead': 17, 'place_left_leg_lead': 17, 'request_patient_to_not_move': 12, 'shave_patient': 5, 'interpret_and_report': 37, 'obtain_ecg_recording': 88, 'secure_patient_on_stretcher': 138, 'check_heart_rate': 6, 'handoff_patient_to_hospital': 282, 'check_a&o': 2, 'face_droop_check': 17, 'inquire_substance_use': 8, 'ask_patient_age_sex': 27, 'check_perrl': 11, 'check_vision_deficits': 15, 'check_grip_strength': 20, 'evaluate_aphasia': 9, 'prepare_glucometer_and_strip': 51, 'read_and_record_glucose_level': 13, 'transport': 420, 'load_patient_to_stretcher': 131, 'notify_hospital': 71, 'suction_airway': 17, 'approach_patient': 22, 'check_symptom_duration': 18, 'check_pulse': 48, 'speech_abnormality_check': 8, 'arm_drift_check': 11, 'document_lkw_time': 28, 'review_medications': 25, 'assess_neglect_signs': 1, 'request_assistance': 6, 'notify_hospital_of_stroke_alert': 108, 'check_respiratory_rate': 19, 'place_right_arm_lead': 22, 'check_responsiveness': 32, 'check_breathing': 12, 'document_hpi_and_pmh': 27, 'place_v5_lead': 11, 'turn_on_ecg': 2, 'assess_patient': 104, 'inquire_medication_anticoagulants': 4, 'inset_NPA': 9, 'check_skin_condition': 1, 'turn_on_aed': 19, 'attach_defib_pads': 49, 'clear_for_analysis': 47, 'clear_for_shock': 26, 'administer_shock_aed': 17, 'ventilate_patient': 72, 'connect_leads_to_ecg': 8, 'place_bvm': 31, 'open_airway': 2, 'no_action': 22, 'request_aed': 3, 'assess_balance_and_coordination': 3}\n",
      "Test Number of classes:  67\n",
      "train dataset size:  2511\n",
      "val dataset size:  564\n",
      "test dataset size:  3332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjh9fw/conda/envs/egoems/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cjh9fw/conda/envs/egoems/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/cjh9fw/conda/envs/egoems/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training class counts:  {'chest_compressions': 1053, 'approach_patient': 12, 'review_medications': 18, 'inquire_substance_use': 9, 'check_symptom_duration': 12, 'document_lkw_time': 8, 'load_patient_to_stretcher': 75, 'secure_patient_on_stretcher': 32, 'notify_hospital_of_stroke_alert': 8, 'transport': 116, 'check_responsiveness': 28, 'check_vision_deficits': 6, 'assess_neglect_signs': 1, 'check_blood_pressure': 66, 'inquire_medication_anticoagulants': 4, 'handoff_patient_to_hospital': 27, 'check_breathing': 24, 'request_assistance': 5, 'request_aed': 6, 'attach_defib_pads': 103, 'no_action': 35, 'check_pulse': 24, 'turn_on_aed': 34, 'clear_for_analysis': 83, 'clear_for_shock': 40, 'administer_shock_aed': 34, 'open_airway': 2, 'ventilate_patient': 140, 'assess_patient': 53, 'explain_procedure': 8, 'place_right_arm_lead': 9, 'place_left_arm_lead': 11, 'place_right_leg_lead': 8, 'place_left_leg_lead': 9, 'place_v2_lead': 6, 'place_v1_lead': 8, 'place_v3_lead': 3, 'place_v6_lead': 9, 'place_v5_lead': 7, 'obtain_ecg_recording': 44, 'request_patient_to_not_move': 5, 'place_bvm': 54, 'check_a&o': 3, 'document_hpi_and_pmh': 8, 'check_skin_condition': 1, 'check_perrl': 4, 'face_droop_check': 2, 'arm_drift_check': 22, 'speech_abnormality_check': 5, 'inquire_hpi_and_pmh': 19, 'interpret_and_report': 15, 'place_v4_lead': 9, 'check_oxygen_saturation': 3, 'ask_patient_age_sex': 4, 'prepare_glucometer_and_strip': 25, 'read_and_record_glucose_level': 12, 'check_heart_rate': 1, 'check_respiratory_rate': 4, 'notify_hospital': 17, 'check_grip_strength': 3, 'connect_leads_to_ecg': 7}\n",
      "Validation class counts:  {'chest_compressions': 283, 'check_pulse': 9, 'request_aed': 2, 'attach_defib_pads': 32, 'clear_for_analysis': 31, 'clear_for_shock': 10, 'administer_shock_aed': 11, 'place_bvm': 13, 'ventilate_patient': 41, 'check_responsiveness': 9, 'turn_on_aed': 7, 'no_action': 9, 'check_breathing': 8, 'check_oxygen_saturation': 1, 'prepare_glucometer_and_strip': 10, 'load_patient_to_stretcher': 5, 'secure_patient_on_stretcher': 2, 'transport': 4, 'notify_hospital': 8, 'assess_patient': 16, 'explain_procedure': 1, 'place_left_arm_lead': 1, 'place_right_arm_lead': 1, 'place_left_leg_lead': 2, 'place_v1_lead': 4, 'place_v4_lead': 1, 'place_v3_lead': 2, 'place_v5_lead': 1, 'request_patient_to_not_move': 1, 'obtain_ecg_recording': 9, 'interpret_and_report': 5}\n",
      "Class counts:  tensor([1.2000e+01, 2.8000e+01, 2.4000e+01, 2.4000e+01, 1.0530e+03, 6.0000e+00,\n",
      "        5.0000e+00, 3.4000e+01, 1.0300e+02, 8.3000e+01, 4.0000e+01, 3.4000e+01,\n",
      "        2.0000e+00, 5.4000e+01, 1.4000e+02, 3.5000e+01, 5.3000e+01, 8.0000e+00,\n",
      "        1.0000e+00, 1.1000e+01, 9.0000e+00, 9.0000e+00, 8.0000e+00, 8.0000e+00,\n",
      "        6.0000e+00, 3.0000e+00, 9.0000e+00, 7.0000e+00, 9.0000e+00, 4.0000e+00,\n",
      "        5.0000e+00, 1.0000e+00, 7.0000e+00, 4.4000e+01, 1.5000e+01, 1.1600e+02,\n",
      "        3.0000e+00, 1.2000e+01, 1.8000e+01, 4.0000e+00, 1.9000e+01, 9.0000e+00,\n",
      "        8.0000e+00, 6.6000e+01, 1.0000e+00, 3.0000e+00, 4.0000e+00, 2.0000e+00,\n",
      "        2.2000e+01, 5.0000e+00, 1.0000e+00, 8.0000e+00, 6.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 2.5000e+01, 1.2000e+01, 1.0000e+00, 1.0000e+00, 7.5000e+01,\n",
      "        3.2000e+01, 2.7000e+01, 4.0000e+00, 1.0000e+00, 3.0000e+00, 1.7000e+01,\n",
      "        8.0000e+00]) 67\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 120, 16])\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import *\n",
    "from scripts.config import DefaultArgsNamespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from datautils.ems import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Accurate seek is not implemented for pyav backend\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args = DefaultArgsNamespace()\n",
    "\n",
    "train_loader, val_loader, test_loader, train_class_stats, val_class_stats = eee_get_dataloaders(args)\n",
    "\n",
    "# # get input feature  size\n",
    "# dataiter = next(iter(train_loader))\n",
    "# batch =  dataiter\n",
    "args.dataloader_params[\"train_class_stats\"] = train_class_stats\n",
    "args.dataloader_params[\"val_class_stats\"] = val_class_stats\n",
    "\n",
    "\n",
    "# # Access the parsed arguments\n",
    "model, optimizer, criterion, device = init_model(args)# verbose_mode = args.verbose\n",
    "model = model.to(device)\n",
    "scheduler = StepLR(optimizer, step_size=args.learning_params[\"lr_drop\"], gamma=0.1)  # adjust parameters as needed\n",
    "\n",
    "dummy_input = torch.randn(1, 120, 1024) # batch, num_frames, channels, height, width\n",
    "\n",
    "dummy_input = dummy_input.to(args.device)\n",
    "\n",
    "dummy_output = model(dummy_input)   \n",
    "\n",
    "print(dummy_output.shape)\n",
    "# repeat the output to match sequence length\n",
    "dummy_output = dummy_output.repeat(1, 120, 1)\n",
    "\n",
    "print(dummy_output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cjh9fw/conda/envs/egoems/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "keysteps = args.dataloader_params['keysteps']\n",
    "out_classes = len(keysteps)\n",
    "print(out_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality:  ['audio']\n",
      "Input shape: torch.Size([1, 155648, 2])\n",
      "Converted mono shape: torch.Size([1, 155648])\n",
      "Processed input_values shape: torch.Size([1, 155648])\n",
      "Extracted Wav2Vec2 features: torch.Size([1, 486, 768])\n",
      "Wav2Vec feature shape:  torch.Size([1, 486, 768])\n",
      "Feature size:  768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training class counts:  {'chest_compressions': 441, 'approach_patient': 0, 'check_responsiveness': 0, 'check_pulse': 0, 'check_breathing': 0, 'request_aed': 0, 'request_assistance': 0, 'turn_on_aed': 0, 'attach_defib_pads': 0, 'clear_for_analysis': 0, 'clear_for_shock': 0, 'administer_shock_aed': 0, 'open_airway': 0, 'place_bvm': 0, 'ventilate_patient': 0, 'no_action': 0, 'assess_patient': 0, 'explain_procedure': 0, 'shave_patient': 0, 'place_left_arm_lead': 0, 'place_right_arm_lead': 0, 'place_left_leg_lead': 0, 'place_right_leg_lead': 0, 'place_v1_lead': 0, 'place_v2_lead': 0, 'place_v3_lead': 0, 'place_v4_lead': 0, 'place_v5_lead': 0, 'place_v6_lead': 0, 'ask_patient_age_sex': 0, 'request_patient_to_not_move': 0, 'turn_on_ecg': 0, 'connect_leads_to_ecg': 0, 'obtain_ecg_recording': 0, 'interpret_and_report': 0}\n",
      "Validation class counts:  {'chest_compressions': 283}\n",
      "Class counts:  tensor([  1.,   1.,   1.,   1., 441.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "          1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
      "          1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.]) 35\n"
     ]
    }
   ],
   "source": [
    "modality = args.dataloader_params['modality']\n",
    "print(\"Modality: \", modality)\n",
    "\n",
    "# Find feature dimension\n",
    "input, feature_size, label = preprocess(next(iter(train_loader)), args.dataloader_params['modality'], model, device=device)\n",
    "print(\"Feature size: \", feature_size)\n",
    "\n",
    "# print(input[:,0,-3:])\n",
    "\n",
    "args.transformer_params['input_dim'] = feature_size\n",
    "args.transformer_params['output_dim'] = out_classes\n",
    "\n",
    "model, optimizer, criterion, device = init_model(args)# verbose_mode = args.verbose\n",
    "model = model.to(device)\n",
    "scheduler = StepLR(optimizer, step_size=args.learning_params[\"lr_drop\"], gamma=0.1)  # adjust parameters as needed\n",
    "\n",
    "wandb_logger = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"EgoExoEMS\",\n",
    "    group=\"Keystep Recognition\",\n",
    "    mode=\"disabled\",\n",
    "    name=\"Testing on EgoExoEMS with I3D RGB Features - ICRA Model\",\n",
    "    notes=\"initial attempt ICRA model with I3D RGB features\",\n",
    "    config={\n",
    "    \"args\": args,\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 0])\n",
      "Input shape: torch.Size([1, 192512, 2])\n",
      "Converted mono shape: torch.Size([1, 192512])\n",
      "Processed input_values shape: torch.Size([1, 192512])\n",
      "Extracted Wav2Vec2 features: torch.Size([1, 601, 768])\n",
      "Wav2Vec feature shape:  torch.Size([1, 601, 768])\n",
      "Model input: torch.Size([1, 601, 768])\n",
      "Model prediction: tensor([30], device='cuda:0')\n",
      "Ground truth: 4\n",
      "Loss: 3.492825984954834\n"
     ]
    }
   ],
   "source": [
    "# # # print one batch\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(batch['smartwatch'].shape)\n",
    "    input,feature_size, label = preprocess(batch, args.dataloader_params['modality'], model, device=device, task='segmentation')\n",
    "    print(\"Model input:\",input.shape)\n",
    "    output = model(input)\n",
    "    \n",
    "    # print(\"Model output:\",output.shape)\n",
    "    # print(\"Label:\",label.shape)\n",
    "    # Get the majority label along the sequence length dimension\n",
    "    \n",
    "    # print(\"Majority label:\", majority_label.shape)\n",
    "    \n",
    "    print(\"Model prediction:\",output.argmax(dim=1))\n",
    "    print(\"Ground truth:\",label.item())   \n",
    "    # Adjust output shape if needed to match the label shape before computing loss\n",
    "    # Assuming you want to compute the loss between majority_label and pred, no need to repeat output now\n",
    "    loss = criterion(output, label)  # Use majority label for loss computation\n",
    "    \n",
    "    print(\"Loss:\",loss.item())\n",
    "    break\n",
    "\n",
    "# results_dir = './results'\n",
    "# # # # Train the model\n",
    "# for epoch in range(args.learning_params[\"epochs\"]):\n",
    "#     train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, wandb_logger, modality=modality)\n",
    "#     val_loss = validate(model, val_loader, criterion, device, wandb_logger, modality=modality)\n",
    "#     scheduler.step()\n",
    "#     print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "#     results = test_model(model, test_loader, criterion, device, wandb_logger, modality=modality, epoch=epoch, results_dir=results_dir)\n",
    "#     print(results)\n",
    "#     break\n",
    "\n",
    "# dummy_input = torch.randn(1, 3, 1024) # batch, num_frames, channels, height, width\n",
    "# dummy_input = dummy_input.to(args.device)\n",
    "# pred = model(dummy_input)\n",
    "# print(pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, feature_dim, out_channels, kernel_size=3):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=feature_dim, out_channels=512, kernel_size=kernel_size, stride=1, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # No change in temporal dimension\n",
    "            nn.Conv1d(in_channels=512, out_channels=256, kernel_size=kernel_size, stride=1, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # No change in temporal dimension\n",
    "            nn.Conv1d(in_channels=256, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=kernel_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)  # No change in temporal dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, feature_dim] -> need to permute for Conv1d\n",
    "        x = x.permute(0, 2, 1)  # [batch, feature_dim, seq_len]\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        # x = torch.mean(x, dim=-1)  # Global average pooling\n",
    "        return x\n",
    "\n",
    "feature_size = 1024\n",
    "hidden_dim = 256\n",
    "cnn_encoder = CNN_Encoder(feature_size, hidden_dim)\n",
    "\n",
    "# Test the model\n",
    "dummy_input = torch.randn(1, 30, 1024)  # batch, seq_len, feature_dim\n",
    "\n",
    "pred = cnn_encoder(dummy_input)\n",
    "\n",
    "print(pred.shape)  # [batch, 1]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egoems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
