{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjh9fw/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/sfs/qumulo/qhome/cjh9fw/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/cjh9fw/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cjh9fw/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16])\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import *\n",
    "from scripts.config import DefaultArgsNamespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from datautils.ems import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import wandb\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args = DefaultArgsNamespace()\n",
    "\n",
    "# Access the parsed arguments\n",
    "model, optimizer, criterion, device = init_model(args)# verbose_mode = args.verbose\n",
    "scheduler = StepLR(optimizer, step_size=args.learning_params[\"lr_drop\"], gamma=0.1)  # adjust parameters as needed\n",
    "\n",
    "dummy_input = torch.randn(5, 30, 3, 224, 224) # batch, num_frames, channels, height, width\n",
    "\n",
    "dummy_input = dummy_input.to(args.device)\n",
    "\n",
    "dummy_output = model(dummy_input)   \n",
    "\n",
    "print(dummy_output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size:  10\n",
      "val dataset size:  3\n",
      "test dataset size:  4\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = eee_get_dataloaders(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjh9fw/.local/lib/python3.8/site-packages/torchvision/io/video_reader.py:245: UserWarning: Accurate seek is not implemented for pyav backend\n",
      "  warnings.warn(\"Accurate seek is not implemented for pyav backend\")\n",
      "/home/cjh9fw/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 274, 3, 224, 224]) torch.Size([1, 422912, 2]) torch.Size([1, 264, 1024]) torch.Size([1, 264, 1024]) ['administer_shock_aed'] tensor([7]) tensor([2382]) tensor([2646]) tensor([79.4036], dtype=torch.float64) tensor([88.1953], dtype=torch.float64) ['ng1'] ['1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print one batch\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(batch['frames'].shape, batch['audio'].shape, batch['flow'].shape, batch['rgb'].shape, batch['keystep_label'], batch['keystep_id'], batch['start_frame'], batch['end_frame'],batch['start_t'], batch['end_t'],  batch['subject_id'], batch['trial_id'])\n",
    "\n",
    "    break\n",
    "\n",
    "# # Train the model\n",
    "# for epoch in range(args.learning_params[\"epochs\"]):\n",
    "#     train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, wandb_logger)\n",
    "#     val_loss = validate(model, val_loader, criterion, device, wandb_logger)\n",
    "#     scheduler.step()\n",
    "#     print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
