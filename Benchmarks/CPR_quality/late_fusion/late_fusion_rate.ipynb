{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/gpfs/tardis/home/cjh9fw/Desktop/2024/repos/EgoExoEMS/Benchmarks/CPR_quality/late_fusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing from timm.models.layers is deprecated, please import via timm.layers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to load grounding dino directly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Models initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744982254.148104  670823 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1744982254.225665  671054 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 550.54.14), renderer: NVIDIA A40/PCIe/SSE2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "from utils.utils import *\n",
    "from scripts.config import DefaultArgsNamespace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from datautils.ems import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from datetime import datetime\n",
    "\n",
    "from ego_rate_estimator.cpr_rate_detection_video import ego_rate_detect, init_models\n",
    "from smartwatch_rate_estimator.cpr_rate_detection_smartwatch import smartwatch_rate_detect, get_gt_cpr_rate\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Accurate seek is not implemented for pyav backend\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality:  ['video', 'smartwatch', 'depth_sensor']\n",
      "Num of classes:  35\n",
      "Window:  150\n",
      "Task:  cpr_quality\n",
      "********** ========== **********\n",
      "Loading dataloader for CPR quality task\n",
      "Train class stats:  {'chest_compressions': 215}\n",
      "Train Number of classes:  1\n",
      "val class stats:  {'chest_compressions': 49}\n",
      "Val Number of classes:  1\n",
      "train dataset size:  215\n",
      "val dataset size:  49\n",
      "test dataset size:  121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1744982254.259969  671004 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744982254.269718  671025 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# get cmd line args\n",
    "args = DefaultArgsNamespace()\n",
    "\n",
    "keysteps = args.dataloader_params['keysteps']\n",
    "out_classes = len(keysteps)\n",
    "\n",
    "modality = args.dataloader_params['modality']\n",
    "print(\"Modality: \", modality)\n",
    "print(\"Num of classes: \", out_classes)\n",
    "\n",
    "window = args.dataloader_params['observation_window']\n",
    "print(\"Window: \", window)\n",
    "\n",
    "task = args.dataloader_params['task']\n",
    "print(\"Task: \", task)\n",
    "\n",
    "\n",
    "# train_loader, val_loader, test_loader = get_dataloaders(args)\n",
    "train_loader, val_loader, test_loader, train_class_stats, val_class_stats = eee_get_dataloaders(args)\n",
    "args.dataloader_params['train_class_stats'] = train_class_stats\n",
    "args.dataloader_params['val_class_stats'] = val_class_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for rate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_rate(frames,smartwatch,gt,window_size,video_id):\n",
    "    # iterate through the frames using a tumbling window with the given window size\n",
    "    # and calculate the detection rate\n",
    "    # for each window\n",
    "\n",
    "    for i in range(0, len(frames), window_size):\n",
    "        print(\"---------------------\")\n",
    "        print(\"Window: \", i)\n",
    "\n",
    "        # get the current window\n",
    "        window = frames[i:i+window_size]\n",
    "        # get the current smartwatch data\n",
    "        smartwatch_window = smartwatch[i:i+window_size]\n",
    "        # get the current gt data\n",
    "        gt_window = gt[i:i+window_size]\n",
    "\n",
    "        print(\"Window: \", window.shape)\n",
    "        print(\"Smartwatch: \", smartwatch_window.shape)\n",
    "        print(\"GT: \", gt_window.shape)\n",
    "\n",
    "        # call video rate detection function\n",
    "        ego_cpr_rate = ego_rate_detect(window,video_id)\n",
    "        \n",
    "        # call smartwatch rate detection function\n",
    "        smartwatch_cpr_rate = smartwatch_rate_detect(smartwatch_window)\n",
    "        print(\"Smartwatch CPR Rate: \", smartwatch_cpr_rate)\n",
    "\n",
    "        gt_cpr_rate = get_gt_cpr_rate(gt_window)\n",
    "        print(\"GT CPR Rate: \", gt_cpr_rate)\n",
    "\n",
    "        # weighted fusion of smartwatch and video rate to achieve best performance\n",
    "        # final_predicted_cpr_rate = 0.5 * smartwatch_cpr_rate + 0.5 * ego_cpr_rate\n",
    "\n",
    "    return None,None,None,None\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************\n",
      "Batch:  0\n",
      "Special case for CPR quality task\n",
      "Subject:  ['ng4']\n",
      "Trial ['1']\n",
      "Input frames:  torch.Size([1, 207, 3, 224, 224])\n",
      "---------------------\n",
      "Window:  0\n",
      "Window:  torch.Size([150, 3, 224, 224])\n",
      "Smartwatch:  torch.Size([150, 3])\n",
      "GT:  torch.Size([150, 1])\n",
      "Starting ego rate detection...\n",
      "Number of RGB images:  (150, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "None of the inputs have requires_grad=True. Gradients will be None\n",
      "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "W0000 00:00:1744907991.850649  184908 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching RGB frames:  150\n",
      "Number of wrist keypoints:  150\n",
      "number of filtered wrist keypoints:  113\n",
      "number of filtered rgb images in the window:  113\n",
      "Predicted CPR rate per window: 6.5 cycles\n",
      "Predicted CPR rate (BPM): 78.0\n",
      "Smartwatch CPR Rate:  192.0\n",
      "GT CPR Rate:  144.0\n",
      "---------------------\n",
      "Window:  150\n",
      "Window:  torch.Size([57, 3, 224, 224])\n",
      "Smartwatch:  torch.Size([57, 3])\n",
      "GT:  torch.Size([57, 1])\n",
      "Starting ego rate detection...\n",
      "Number of RGB images:  (57, 224, 224, 3)\n",
      "Number of matching RGB frames:  57\n",
      "Number of wrist keypoints:  57\n",
      "number of filtered wrist keypoints:  24\n",
      "number of filtered rgb images in the window:  24\n",
      "Predicted CPR rate per window: 3.0 cycles\n",
      "Predicted CPR rate (BPM): 36.0\n",
      "Smartwatch CPR Rate:  142.10526315789474\n",
      "GT CPR Rate:  126.31578947368422\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    print(\"****************\")\n",
    "    print(\"Batch: \", i)\n",
    "\n",
    "    input,feature_size, gt_sensor_data = preprocess(batch, modality, None, device, task=task)\n",
    "    print(\"Subject: \", batch['subject_id'])\n",
    "    print(\"Trial\", batch['trial_id'])\n",
    "    print(\"Input frames: \", input['frames'].shape)\n",
    "\n",
    "    video_id = f\"{batch['subject_id']}_{batch['trial_id']}_ego\"\n",
    "    fused_rate, sw_rate, video_rate, gt_rates = detect_rate(input['frames'][0], input['smartwatch'][0], gt_sensor_data[0], window, video_id)\n",
    "    print(\"****************\")\n",
    "\n",
    "    break\n",
    "    \n",
    "    # # sample image\n",
    "    # image = input['frames'][0][0].permute(1,2,0).cpu().numpy()\n",
    "    # # convert to PIL image\n",
    "    # image = Image.fromarray(image)\n",
    "    # # show pil image\n",
    "    # plt.imshow(image)\n",
    "    \n",
    "    # print(\"Input smartwatch: \", input['smartwatch'].shape)\n",
    "    # print(\"gt_sensor_data shape: \", gt_sensor_data.shape)\n",
    "    \n",
    "    # # plot the smartwatch data (3-axis accelerometer)\n",
    "    # smartwatch_data = input['smartwatch'][0].cpu().numpy()\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # plt.plot(smartwatch_data[:, 0], label='X-axis')\n",
    "    # plt.plot(smartwatch_data[:, 1], label='Y-axis')\n",
    "    # plt.plot(smartwatch_data[:, 2], label='Z-axis')\n",
    "    # plt.title('Smartwatch Data')\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Acceleration')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    \n",
    "    # # plot the gt_sensor_data (1-axis distance)\n",
    "    # gt_sensor_data = gt_sensor_data[0].cpu().numpy()\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # plt.plot(gt_sensor_data, label='Distance')\n",
    "    # plt.title('Ground Truth Sensor Data')\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Distance')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    # print(\"****************\")\n",
    "    \n",
    "    # break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
